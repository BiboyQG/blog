+++
title = 'Meeting Discussion (5)'
date = 2024-05-01T16:48:28-05:00
draft = false
categories = ['meeting-discussions']
tags = ['meeting-discussions', 'research', 'Quantization']
+++

#### 1. Table of Contents

* Accuracy graph under diffrerent quantization metrics: ✅
* Max value within the layers: ✅

#### 2. Accuracy graph under diffrerent quantization metrics:

![weigt](https://s2.loli.net/2024/05/02/a2ZCioXbnQyfdt9.png)

![activation](https://s2.loli.net/2024/05/02/eWJlbMt64ihZQkf.png)

As we can observe from both graphs, activation is clearly influced more by quantization.

#### 3. Max value within the layers

![weight_amax](https://s2.loli.net/2024/05/02/psoeGLZWcfiYBRd.png)

![activation_amax](https://s2.loli.net/2024/05/02/Q3KCObkmnH4ysiP.png)

In the first graph, we can see that the max value within the weigh ranges from 0.1 to 2.94, while in the second graph, we can find an interesting max value pattern, with its value ranging from 8 to 53.74, which also explains why activation is influenced more by quantization.

#### 4. What's next?

- Add a customized quantizer for the sparse conv3d layer.
- Add a customized quantizer for operation in [SmoothQuant](https://arxiv.org/abs/2211.10438).
