+++
title = 'Meeting Discussion (6)'
date = 2024-05-02T17:04:02-05:00
draft = false
categories = ['meeting-discussions']
tags = ['meeting-discussions', 'research', 'Quantization']
+++

#### 1. Table of Contents

- In-depth Memory Usage Visualization
- Ideas about how to implement quantization of sparse conv3d

- Ideas about how to implement SmoothQuant operation on conv2d

#### 2. Large Chunk GPU Memory Usage Overview

- Data loader
- Backbone 3d
- Backbone 3d -> Backbone 2d
- Backbone 2d
- Head

![img](https://s2.loli.net/2024/05/03/QDslj3v27dH86KO.png)

Below are structure for each major chunk:

- Data Loader

![img](https://s2.loli.net/2024/05/03/EB86rHDU1nvJ4w5.png)

- Backbone 3d

![img](https://s2.loli.net/2024/05/03/Zk54O2FQuMePmAV.png)

- 3d feature to 2d feature

![img](https://s2.loli.net/2024/05/03/AKPCTygsZBjGHFd.png)

- Backbone 2d

![img](https://s2.loli.net/2024/05/03/YCN6cUx5dhazMFL.png)

- Head

![img](https://s2.loli.net/2024/05/03/yghIoLpfwFdGDzc.png)

#### 3. How to implement quantization of sparse conv3d?

- Take a look at Nvidiaâ€™s implementation of Conv3d quantized layer

![image-20240502170058646](https://s2.loli.net/2024/05/03/o2akKjtTCpIOd1l.png)

The process should be similar if we see:

![img](https://s2.loli.net/2024/05/03/MuzSNR9AVHPdpDy.png)

#### 4. How to implement SmoothQuant operation on conv2d?

- Get activation scale

We will implement the process if getting activation scale similar to the following process (from SmoothQuant):

![img](https://s2.loli.net/2024/05/03/93iH8EL1u7hcyef.png)

- Migrate difficulty

![img](https://s2.loli.net/2024/05/03/p5hLFvOm96X4QRG.png)

#### 5. What's next?

- Implement SmoothQuant operation first, and then
- Implement quantization of sparse conv3d.
