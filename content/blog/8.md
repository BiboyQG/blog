+++
title = 'Meeting Discussion (8)'
date = 2024-05-14T14:20:16-05:00
draft = false
categories = ['meeting-discussions']
tags = ['meeting-discussions', 'research', 'Quantization']
+++

#### 1. Table of Contents

- Original implementation of SmoothQuant and why it’s not correct: ✅
- The correct way of implementation IMO: ✅

#### 2. Original way of getting absMax values

```python
def register_collect_smoothquant_hook(model, data_loader, num_batch=200):
    model.eval()
    act_scales = {}
    weight_scales = {}

    def forward_hook(module, input, name):
        hidden_dim_act = input[0].shape[1]
        tensor_act = input[0].view(-1, hidden_dim_act).abs().detach()
        comming_max_act = torch.max(tensor_act, dim=0)[0].float().cpu()
        if name not in act_scales:
            act_scales[name] = comming_max_act
        else:
            act_scales[name] = torch.max(act_scales[name], comming_max_act)
```

- Input shape: [4, 256, 182, 182]
- hidden_dim_act = 256
- tensor_act: [4\*182\*182, 256]
- torch.max(tensor_act, dim=0): [1, 256]
- torch.max(tensor_act, dim=0)[0]: [256]
- **Divide input by the scaling factor $ s $ computed with these max values $\neq$ SmoothQuant**

#### 3. The correct way of implementation

![image-20240514141635648](https://s2.loli.net/2024/05/15/lh3KTwrQLfCM75S.png)

- Convolution operation can be further divided into [im2col+sgemm](https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA%3D%3D&mid=2247488158&idx=1&sn=3722bc7433811d494e179cb828dade32&chksm=cf108a9bf867038d4e7e451212429925a48dcbd47d9811315c132271f104540fe503555e7611&token=1276531538&lang=zh_CN&scene=21#wechat_redirect).
- Therefore, the above two processes should be similar.

#### 4. What's next?

- Implement im2col+sgemm on Pytorch level.
- Implement quantization between im2col and sgemm.
- Implement SmoothQuant operation (migrate difficulty) before applying quantization.
