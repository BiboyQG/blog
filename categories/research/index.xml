<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Research on Banghao&#39;s Blog</title>
    <link>https://biboyqg.github.io/blog/categories/research/</link>
    <description>Recent content in Research on Banghao&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Jun 2024 19:24:12 +0800</lastBuildDate>
    <atom:link href="https://biboyqg.github.io/blog/categories/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLMarking: Adaptive Automatic Short-Answer Grading Using Large Language Models</title>
      <link>https://biboyqg.github.io/blog/blog/llmarking/</link>
      <pubDate>Wed, 12 Jun 2024 19:24:12 +0800</pubDate>
      <guid>https://biboyqg.github.io/blog/blog/llmarking/</guid>
      <description>This is the official repo for Automatic Short Answer Grading (ASAG) project, named LLMarking, from Xi&amp;rsquo;an Jiaotong Liverpool University (XJTLU).
Using vLLM as the Large Language Model (LLM) inference framework and FastAPI as the HTTP service framework, this project can achieve high throughput of both LLM tokens delivered and request handling.
Feature This project aims to achieve high concurrency automatic short answer grading (ASAG) system and implement the construction of service.</description>
    </item>
    <item>
      <title>Quantization on CenterPoint</title>
      <link>https://biboyqg.github.io/blog/blog/quantization-on-centerpoint/</link>
      <pubDate>Mon, 01 Apr 2024 16:32:18 -0500</pubDate>
      <guid>https://biboyqg.github.io/blog/blog/quantization-on-centerpoint/</guid>
      <description>Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:
class Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get(&amp;#39;model_wrapper_cfg&amp;#39;), self.model) # get model name from the model class if hasattr(self.model, &amp;#39;module&amp;#39;): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils.</description>
    </item>
  </channel>
</rss>
