<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Meeting-Discussions on Banghao&#39;s Blog</title>
    <link>https://banghao.live/tags/meeting-discussions/</link>
    <description>Recent content in Meeting-Discussions on Banghao&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 14:01:08 -0500</lastBuildDate>
    <atom:link href="https://banghao.live/tags/meeting-discussions/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Meeting Discussion (7)</title>
      <link>https://banghao.live/blog/7/</link>
      <pubDate>Fri, 10 May 2024 14:01:08 -0500</pubDate>
      <guid>https://banghao.live/blog/7/</guid>
      <description>1. Table of Contents Implementation of SmoothQuant on Conv2d: ✅
Validation of the above implementation: ✅ (for $ \alpha = 0.5 $)
2. Implementation of SmoothQuant operation on Conv2d Get activation scale Get weight scale Compute smoothing factor $ s $ based on above two scales Apply scaling: $\text{input} \mathrel{{/}{=}} s$ $\text{weight} \mathrel{{*}{=}} s$ 2.1 Get activation &amp;amp; weight scale Take a look at the shape of activation, output, and weight in Conv2d: Take one layer as an example: Input shape: torch.</description>
    </item>
    <item>
      <title>Meeting Discussion (6)</title>
      <link>https://banghao.live/blog/6/</link>
      <pubDate>Thu, 02 May 2024 17:04:02 -0500</pubDate>
      <guid>https://banghao.live/blog/6/</guid>
      <description>1. Table of Contents In-depth Memory Usage Visualization
Ideas about how to implement quantization of sparse conv3d
Ideas about how to implement SmoothQuant operation on conv2d
2. Large Chunk GPU Memory Usage Overview Data loader Backbone 3d Backbone 3d -&amp;gt; Backbone 2d Backbone 2d Head Below are structure for each major chunk:
Data Loader Backbone 3d 3d feature to 2d feature Backbone 2d Head 3. How to implement quantization of sparse conv3d?</description>
    </item>
    <item>
      <title>Meeting Discussion (5)</title>
      <link>https://banghao.live/blog/5/</link>
      <pubDate>Wed, 01 May 2024 16:48:28 -0500</pubDate>
      <guid>https://banghao.live/blog/5/</guid>
      <description>1. Accuracy graph under diffrerent quantization metrics: As we can observe from both graphs, activation is clearly influced more by quantization.
2. Max value within the layers In the first graph, we can see that the max value within the weigh ranges from 0.1 to 2.94, while in the second graph, we can find an interesting max value pattern, with its value ranging from 8 to 53.74, which also explains why activation is influenced more by quantization.</description>
    </item>
    <item>
      <title>Meeting Discussion (4)</title>
      <link>https://banghao.live/blog/4/</link>
      <pubDate>Fri, 26 Apr 2024 15:32:56 -0500</pubDate>
      <guid>https://banghao.live/blog/4/</guid>
      <description>1. The strcuture of the CenterPoint-Vexel model CenterPoint( (vfe): MeanVFE() (backbone_3d): VoxelResBackBone8x( (conv_input): SparseSequential( (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm) (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True) (2): ReLU() ) (conv1): SparseSequential( (0): SparseBasicBlock( (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm) (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True) (relu): ReLU() (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.</description>
    </item>
  </channel>
</rss>
