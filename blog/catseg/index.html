<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>CATseg: A complete walk through of the model architecture | Banghao&#39;s Blog</title>
<meta name="keywords" content="Open Vocabulary Segmentation, CLIP, Swin Transformer">
<meta name="description" content="1. Model Architecture setup and evaluation data flow(for ade150k) CATSeg setup:
backbone: D2SwinTransformer -&gt; Swintransformer -&gt; BasicLayer(2) -&gt; SwinTransformerBlock -&gt; WindowAttention
sem_seg_head: CATSegHead.from_config -&gt; CATSegPredictor -&gt;
Load CLIP model -&gt; Load text templates -&gt; class_embeddings(self.class_texts, prompt_templates, clip_model) -&gt; for each class:
bpe encode classname in different templates and save results in variable texts (80(number of templates), 77(number of sentence length)). CLIP encode texts : texts go through token_embedding(nn.Embedding) (80,77,768(hidden_dim)) texts go through a 12 layers of ResidualAttentionBlock (80,77,768) take features of texts from the eot_token (80,768) do the above for all classes (150(number of test classes),80,768)">
<meta name="author" content="Banghao Chi">
<link rel="canonical" href="https://banghao.live/blog/catseg/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://banghao.live/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://banghao.live/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://banghao.live/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://banghao.live/apple-touch-icon.png">
<link rel="mask-icon" href="https://banghao.live/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://banghao.live/blog/catseg/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="CATseg: A complete walk through of the model architecture" />
<meta property="og:description" content="1. Model Architecture setup and evaluation data flow(for ade150k) CATSeg setup:
backbone: D2SwinTransformer -&gt; Swintransformer -&gt; BasicLayer(2) -&gt; SwinTransformerBlock -&gt; WindowAttention
sem_seg_head: CATSegHead.from_config -&gt; CATSegPredictor -&gt;
Load CLIP model -&gt; Load text templates -&gt; class_embeddings(self.class_texts, prompt_templates, clip_model) -&gt; for each class:
bpe encode classname in different templates and save results in variable texts (80(number of templates), 77(number of sentence length)). CLIP encode texts : texts go through token_embedding(nn.Embedding) (80,77,768(hidden_dim)) texts go through a 12 layers of ResidualAttentionBlock (80,77,768) take features of texts from the eot_token (80,768) do the above for all classes (150(number of test classes),80,768)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://banghao.live/blog/catseg/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-03-15T05:21:09-05:00" />
<meta property="article:modified_time" content="2024-03-15T05:21:09-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="CATseg: A complete walk through of the model architecture"/>
<meta name="twitter:description" content="1. Model Architecture setup and evaluation data flow(for ade150k) CATSeg setup:
backbone: D2SwinTransformer -&gt; Swintransformer -&gt; BasicLayer(2) -&gt; SwinTransformerBlock -&gt; WindowAttention
sem_seg_head: CATSegHead.from_config -&gt; CATSegPredictor -&gt;
Load CLIP model -&gt; Load text templates -&gt; class_embeddings(self.class_texts, prompt_templates, clip_model) -&gt; for each class:
bpe encode classname in different templates and save results in variable texts (80(number of templates), 77(number of sentence length)). CLIP encode texts : texts go through token_embedding(nn.Embedding) (80,77,768(hidden_dim)) texts go through a 12 layers of ResidualAttentionBlock (80,77,768) take features of texts from the eot_token (80,768) do the above for all classes (150(number of test classes),80,768)"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://banghao.live/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "CATseg: A complete walk through of the model architecture",
      "item": "https://banghao.live/blog/catseg/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CATseg: A complete walk through of the model architecture",
  "name": "CATseg: A complete walk through of the model architecture",
  "description": "1. Model Architecture setup and evaluation data flow(for ade150k) CATSeg setup:\nbackbone: D2SwinTransformer -\u0026gt; Swintransformer -\u0026gt; BasicLayer(2) -\u0026gt; SwinTransformerBlock -\u0026gt; WindowAttention\nsem_seg_head: CATSegHead.from_config -\u0026gt; CATSegPredictor -\u0026gt;\nLoad CLIP model -\u0026gt; Load text templates -\u0026gt; class_embeddings(self.class_texts, prompt_templates, clip_model) -\u0026gt; for each class:\nbpe encode classname in different templates and save results in variable texts (80(number of templates), 77(number of sentence length)). CLIP encode texts : texts go through token_embedding(nn.Embedding) (80,77,768(hidden_dim)) texts go through a 12 layers of ResidualAttentionBlock (80,77,768) take features of texts from the eot_token (80,768) do the above for all classes (150(number of test classes),80,768)",
  "keywords": [
    "Open Vocabulary Segmentation", "CLIP", "Swin Transformer"
  ],
  "articleBody": "1. Model Architecture setup and evaluation data flow(for ade150k) CATSeg setup:\nbackbone: D2SwinTransformer -\u003e Swintransformer -\u003e BasicLayer(2) -\u003e SwinTransformerBlock -\u003e WindowAttention\nsem_seg_head: CATSegHead.from_config -\u003e CATSegPredictor -\u003e\nLoad CLIP model -\u003e Load text templates -\u003e class_embeddings(self.class_texts, prompt_templates, clip_model) -\u003e for each class:\nbpe encode classname in different templates and save results in variable texts (80(number of templates), 77(number of sentence length)). CLIP encode texts : texts go through token_embedding(nn.Embedding) (80,77,768(hidden_dim)) texts go through a 12 layers of ResidualAttentionBlock (80,77,768) take features of texts from the eot_token (80,768) do the above for all classes (150(number of test classes),80,768)\nAggregator -\u003e 2 layers of AggregatorLayer:\nswin_block:\nSwinTransformerBlockWrapper:\nclass SwinTransformerBlockWrapper(nn.Module): def __init__(self, dim, appearance_guidance_dim, input_resolution, nheads=4, window_size=5): super().__init__() self.block_1 = SwinTransformerBlock(dim, appearance_guidance_dim, input_resolution, num_heads=nheads, head_dim=None, window_size=window_size, shift_size=0) self.block_2 = SwinTransformerBlock(dim, appearance_guidance_dim, input_resolution, num_heads=nheads, head_dim=None, window_size=window_size, shift_size=window_size // 2) self.guidance_norm = nn.LayerNorm(appearance_guidance_dim) if appearance_guidance_dim \u003e 0 else None attention:\nClassTransformerLayer:\nclass ClassTransformerLayer(nn.Module): def __init__(self, hidden_dim=64, guidance_dim=64, nheads=8, attention_type='linear', pooling_size=(4, 4)) -\u003e None: super().__init__() self.pool = nn.AvgPool2d(pooling_size) self.attention = AttentionLayer(hidden_dim, guidance_dim, nheads=nheads, attention_type=attention_type) self.MLP = nn.Sequential( nn.Linear(hidden_dim, hidden_dim * 4), nn.ReLU(), nn.Linear(hidden_dim * 4, hidden_dim) ) self.norm1 = nn.LayerNorm(hidden_dim) self.norm2 = nn.LayerNorm(hidden_dim) class LinearAttention(nn.Module): def __init__(self, eps=1e-6): super().__init__() self.feature_map = elu_feature_map self.eps = eps def forward(self, queries, keys, values): \"\"\" Multi-Head linear attention proposed in \"Transformers are RNNs\" Args: queries: [N, L, H, D] keys: [N, S, H, D] values: [N, S, H, D] q_mask: [N, L] kv_mask: [N, S] Returns: queried_values: (N, L, H, D) \"\"\" Q = self.feature_map(queries) K = self.feature_map(keys) v_length = values.size(1) values = values / v_length # prevent fp16 overflow KV = torch.einsum(\"nshd,nshv-\u003enhdv\", K, values) # (S,D)' @ S,V Z = 1 / (torch.einsum(\"nlhd,nhd-\u003enlh\", Q, K.sum(dim=1)) + self.eps) queried_values = torch.einsum(\"nlhd,nhdv,nlh-\u003enlhv\", Q, KV, Z) * v_length return queried_values.contiguous() class FullAttention(nn.Module): def __init__(self, use_dropout=False, attention_dropout=0.1): super().__init__() self.use_dropout = use_dropout self.dropout = nn.Dropout(attention_dropout) def forward(self, queries, keys, values, q_mask=None, kv_mask=None): \"\"\" Multi-head scaled dot-product attention, a.k.a full attention. Args: queries: [N, L, H, D] keys: [N, S, H, D] values: [N, S, H, D] q_mask: [N, L] kv_mask: [N, S] Returns: queried_values: (N, L, H, D) \"\"\" # Compute the unnormalized attention and apply the masks QK = torch.einsum(\"nlhd,nshd-\u003enlsh\", queries, keys) if kv_mask is not None: QK.masked_fill_(~(q_mask[:, :, None, None] * kv_mask[:, None, :, None]), float('-inf')) # Compute the attention and the weighted average softmax_temp = 1. / queries.size(3)**.5 # sqrt(D) A = torch.softmax(softmax_temp * QK, dim=2) if self.use_dropout: A = self.dropout(A) queried_values = torch.einsum(\"nlsh,nshd-\u003enlhd\", A, values) return queried_values.contiguous() class AttentionLayer(nn.Module): def __init__(self, hidden_dim, guidance_dim, nheads=8, attention_type='linear'): super().__init__() self.nheads = nheads self.q = nn.Linear(hidden_dim + guidance_dim, hidden_dim) self.k = nn.Linear(hidden_dim + guidance_dim, hidden_dim) self.v = nn.Linear(hidden_dim, hidden_dim) if attention_type == 'linear': self.attention = LinearAttention() elif attention_type == 'full': self.attention = FullAttention() else: raise NotImplementedError Remaining of Aggregator\nself.guidance_projection = nn.Sequential( nn.Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size=3, stride=1, padding=1), nn.ReLU(), ) if appearance_guidance_dim \u003e 0 else None self.text_guidance_projection = nn.Sequential( nn.Linear(text_guidance_dim, text_guidance_proj_dim), nn.ReLU(), ) if text_guidance_dim \u003e 0 else None self.decoder_guidance_projection = nn.ModuleList([ nn.Sequential( nn.Conv2d(d, dp, kernel_size=3, stride=1, padding=1), nn.ReLU(), ) for d, dp in zip(decoder_guidance_dims, decoder_guidance_proj_dims) ]) if decoder_guidance_dims[0] \u003e 0 else None self.decoder1 = Up(hidden_dim, decoder_dims[0], decoder_guidance_proj_dims[0]) self.decoder2 = Up(decoder_dims[0], decoder_dims[1], decoder_guidance_proj_dims[1]) self.head = nn.Conv2d(decoder_dims[1], 1, kernel_size=3, stride=1, padding=1) Up\nclass Up(nn.Module): \"\"\"Upscaling then double conv\"\"\" def __init__(self, in_channels, out_channels, guidance_channels): super().__init__() self.up = nn.ConvTranspose2d(in_channels, in_channels - guidance_channels, kernel_size=2, stride=2) self.conv = DoubleConv(in_channels, out_channels) def forward(self, x, guidance=None): x = self.up(x) if guidance is not None: T = x.size(0) // guidance.size(0) guidance = repeat(guidance, \"B C H W -\u003e (B T) C H W\", T=T) x = torch.cat([x, guidance], dim=1) return self.conv(x) CATSeg forward for each image:\nimage (3,640,854) -\u003e self.inference_sliding_window(batched_inputs)\nimage = F.interpolate(images[0].unsqueeze(0), size=out_res, mode='bilinear', align_corners=False).squeeze() -\u003e (3,640,640)\nimage = rearrange(unfold(image), \"(C H W) L-\u003e L C H W\", C=3, H=kernel) -\u003e (442368(3x384x384(kernel size)),4(number of such patch)) -\u003e (4,3,384,384)\nglobal_image = F.interpolate(images[0].unsqueeze(0), size=(kernel, kernel), mode='bilinear', align_corners=False) image = torch.cat((image, global_image), dim=0) -\u003e (5,3,384,384) 与下面呼应！\nfeatures = self.backbone(images) # features: a dictionary with length of 3 clip_features = self.sem_seg_head.predictor.clip_model.encode_image(clip_images, dense=True) # clip_images: (5, 3, 336, 336) # outputs: (5,150,96,96) outputs = self.sem_seg_head(clip_features, features) features: ![image-20240401003703609](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401003703609.png) clip_features: (5,577(24x24+1),768) outputs: (5,150(number of classes),96,96) After the three steps: outputs -\u003e (5,150,96,96)\n# outputs: (5,150,96,96) -\u003e (5,150,384,384) outputs = F.interpolate(outputs, size=kernel, mode=\"bilinear\", align_corners=False) # -\u003e (5,150,384,384) 与上面呼应！ outputs = outputs.sigmoid() global_output = outputs[-1:] global_output = F.interpolate(global_output, size=out_res, mode='bilinear', align_corners=False,) outputs = outputs[:-1]\t# -\u003e (4,150,384,384) outputs = fold(outputs.flatten(1).T) / fold(unfold(torch.ones([1] + out_res, device=self.device))) # fenzi: (4,22118400) -\u003e (22118400,4) -\u003e (150,640,640) # fenmu: (1,640,640) # This steps normalize the effects brought by the fold operation. outputs = (outputs + global_output) / 2. # -\u003e (1,150,640,640) height = batched_inputs[0].get(\"height\", out_res[0]) width = batched_inputs[0].get(\"width\", out_res[1]) output = sem_seg_postprocess(outputs[0], out_res, height, width) # -\u003e (150,512,683) The workflow within three main steps The workflow within features = self.backbone(images) # features: a dictionary with length of 3:\nclass D2SwinTransformer(SwinTransformer, Backbone): def forward(self, x): # x -\u003e (5,3,384,384) \"\"\" Args: x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``. Returns: dict[str-\u003eTensor]: names and the corresponding features \"\"\" assert ( x.dim() == 4 ), f\"SwinTransformer takes an input of shape (N, C, H, W). Got {x.shape} instead!\" outputs = {} y = super().forward(x) # y -\u003e a dict of three tensors: {(5,128,96,96), (5,256,48,48), (5,512,24,24)}. Same for shape of the outputs for k in y.keys(): if k in self._out_features: outputs[k] = y[k] return outputs class SwinTransformer(nn.Module): def forward(self, x): \"\"\"Forward function.\"\"\" x = self.patch_embed(x) # (5,3,384,384) -\u003e (5,128,96,96) 解释在下面 Wh, Ww = x.size(2), x.size(3) if self.ape: # interpolate the position embedding to the corresponding size absolute_pos_embed = F.interpolate( self.absolute_pos_embed, size=(Wh, Ww), mode=\"bicubic\" ) x = (x + absolute_pos_embed).flatten(2).transpose(1, 2) # B Wh*Ww C else: x = x.flatten(2).transpose(1, 2) # -\u003e (5,9216(96x96),128) x = self.pos_drop(x) # no change (5,9216,128) outs = {} for i in range(self.num_layers): layer = self.layers[i] x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww) # x_out -\u003e (5,9216,128)/(5,2304,256)/(5,576,256) if i in self.out_indices: norm_layer = getattr(self, f\"norm{i}\") x_out = norm_layer(x_out) # no change (5,9216,128)/(5,2304,256)/(5,576,512) out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous() # out: (5,128,96,96)/(5,256,48,48)/(5,512,24,24) outs[\"res{}\".format(i + 2)] = out return outs class PatchEmbed(nn.Module): def forward(self, x): \"\"\"Forward function.\"\"\" # padding _, _, H, W = x.size() if W % self.patch_size[1] != 0: x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1])) if H % self.patch_size[0] != 0: x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0])) x = self.proj(x) # B C Wh Ww (5,3,384,384) -\u003e (5,128,96,96) if self.norm is not None: Wh, Ww = x.size(2), x.size(3) x = x.flatten(2).transpose(1, 2) x = self.norm(x) x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww) return x # (5,128,96,96) 传回上面👆🏻 self.layers: ModuleList( (0): BasicLayer( (blocks): ModuleList( (0): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=128, out_features=384, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): Identity() (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=512, out_features=128, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=128, out_features=384, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.014) (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=512, out_features=128, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) ) (downsample): PatchMerging( (reduction): Linear(in_features=512, out_features=256, bias=False) (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True) ) ) (1): BasicLayer( (blocks): ModuleList( (0): SwinTransformerBlock( (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=256, out_features=768, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=256, out_features=256, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.029) (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=256, out_features=1024, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=1024, out_features=256, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=256, out_features=768, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=256, out_features=256, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.043) (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=256, out_features=1024, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=1024, out_features=256, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) ) (downsample): PatchMerging( (reduction): Linear(in_features=1024, out_features=512, bias=False) (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True) ) ) (2): BasicLayer( (blocks): ModuleList( (0): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.057) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.071) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (2): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.086) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (3): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.100) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (4): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.114) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (5): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.129) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (6): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.143) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (7): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.157) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (8): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.171) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (9): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.186) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (10): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.200) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (11): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.214) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (12): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.229) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (13): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.243) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (14): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.257) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (15): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.271) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (16): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.286) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) (17): SwinTransformerBlock( (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (qkv): Linear(in_features=512, out_features=1536, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=512, out_features=512, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): DropPath(drop_prob=0.300) (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=512, out_features=2048, bias=True) (act): GELU(approximate='none') (fc2): Linear(in_features=2048, out_features=512, bias=True) (drop): Dropout(p=0.0, inplace=False) ) ) ) ) ) class BasicLayer(nn.Module): def forward(self, x, H, W): \"\"\"Forward function. Args: x: Input feature, tensor size (B, H*W, C). H, W: Spatial resolution of the input feature. \"\"\" # calculate attention mask for SW-MSA Hp = int(np.ceil(H / self.window_size)) * self.window_size Wp = int(np.ceil(W / self.window_size)) * self.window_size img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device) # 1 Hp Wp 1 h_slices = ( slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None), ) w_slices = ( slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None), ) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 mask_windows = window_partition( img_mask, self.window_size ) # nW, window_size, window_size, 1 mask_windows = mask_windows.view(-1, self.window_size * self.window_size) attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2) attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill( attn_mask == 0, float(0.0) ) for blk in self.blocks: blk.H, blk.W = H, W if self.use_checkpoint: x = checkpoint.checkpoint(blk, x, attn_mask) else: x = blk(x, attn_mask) # (5,9216,128) -\u003e (5,9216,128) if self.downsample is not None: x_down = self.downsample(x, H, W) Wh, Ww = (H + 1) // 2, (W + 1) // 2 return x, H, W, x_down, Wh, Ww else: return x, H, W, x, H, W The workflow within clip_features = self.sem_seg_head.predictor.clip_model.encode_image(clip_images, dense=True) # clip_images: (5, 3, 336, 336), clip_features: (5, 577, 768):\nclass VisualTransformer(nn.Module): def forward(self, x: torch.Tensor, dense=False): # (5,3,336,336) x = self.conv1(x) # shape = [5, 1024, 24, 24] x = x.reshape(x.shape[0], x.shape[1], -1) # shape = [5, 1024, 576(24x24)] x = x.permute(0, 2, 1) # shape = [5, 576, 1024] x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1) # shape = [5, 576+1, 1024] if dense and (x.shape[1] != self.positional_embedding.shape[0]): x = x + self.resized_pos_embed(self.input_resolution, x.shape[1]).to(x.dtype) else: x = x + self.positional_embedding.to(x.dtype) # shape = [5, 577, 1024] x = self.ln_pre(x) # shape = [5, 577, 1024] x = x.permute(1, 0, 2) # NLD -\u003e LND x = self.transformer(x, dense) x = x.permute(1, 0, 2) # LND -\u003e NLD shape = [5, 577, 1024] if dense: x = self.ln_post(x[:, :, :]) else: x = self.ln_post(x[:, 0, :]) if self.proj is not None: x = x @ self.proj # shape -\u003e [5, 577, 768] return x # shape = [5, 577, 768] The workflow within outputs = self.sem_seg_head(clip_features, features):\nclass CATSegHead(nn.Module): def forward(self, features, guidance_features): \"\"\" Arguments: img_feats: (B, C, HW) affinity_features: (B, C, ) \"\"\" # features: (5,577,768) -\u003e (5,768,24,24) img_feat = rearrange(features[:, 1:, :], \"b (h w) c-\u003eb c h w\", h=self.feature_resolution[0], w=self.feature_resolution[1]) return self.predictor(img_feat, guidance_features) class CATSegPredictor(nn.Module): # self.transformer -\u003e Aggregator! def forward(self, x, vis_guidance): vis = [vis_guidance[k] for k in vis_guidance.keys()][::-1] # text: (150, 80, 768) text = self.text_features if self.training else self.text_features_test # text -\u003e (5, 150, 80, 768) text = text.repeat(x.shape[0], 1, 1, 1) out = self.transformer(x, text, vis) # This Aggregator part: below👇🏻 return out text_feats: (5,150,80,768), img_feats: (5,768,24,24)\nclass Aggregator(nn.Module): def __init__(self, text_guidance_dim=512, text_guidance_proj_dim=128, appearance_guidance_dim=512, appearance_guidance_proj_dim=128, decoder_dims = (64, 32), decoder_guidance_dims=(256, 128), decoder_guidance_proj_dims=(32, 16), num_layers=4, nheads=4, hidden_dim=128, pooling_size=(6, 6), feature_resolution=(24, 24), window_size=12, attention_type='linear', prompt_channel=80, ) -\u003e None: super().__init__() self.num_layers = num_layers self.hidden_dim = hidden_dim self.layers = nn.ModuleList([ AggregatorLayer( hidden_dim=hidden_dim, text_guidance_dim=text_guidance_proj_dim, appearance_guidance=appearance_guidance_proj_dim, nheads=nheads, input_resolution=feature_resolution, pooling_size=pooling_size, window_size=window_size, attention_type=attention_type ) for _ in range(num_layers) ]) self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=7, stride=1, padding=3) self.guidance_projection = nn.Sequential( nn.Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size=3, stride=1, padding=1), nn.ReLU(), ) if appearance_guidance_dim \u003e 0 else None self.text_guidance_projection = nn.Sequential( nn.Linear(text_guidance_dim, text_guidance_proj_dim), nn.ReLU(), ) if text_guidance_dim \u003e 0 else None self.decoder_guidance_projection = nn.ModuleList([ nn.Sequential( nn.Conv2d(d, dp, kernel_size=3, stride=1, padding=1), nn.ReLU(), ) for d, dp in zip(decoder_guidance_dims, decoder_guidance_proj_dims) ]) if decoder_guidance_dims[0] \u003e 0 else None self.decoder1 = Up(hidden_dim, decoder_dims[0], decoder_guidance_proj_dims[0]) self.decoder2 = Up(decoder_dims[0], decoder_dims[1], decoder_guidance_proj_dims[1]) self.head = nn.Conv2d(decoder_dims[1], 1, kernel_size=3, stride=1, padding=1) #---------------------------------------------------------------------------------# def feature_map(self, img_feats, text_feats): img_feats = F.normalize(img_feats, dim=1) # B C H W img_feats = repeat(img_feats, \"B C H W -\u003e B C T H W\", T=text_feats.shape[1]) text_feats = F.normalize(text_feats, dim=-1) # B T P C text_feats = text_feats.mean(dim=-2) text_feats = F.normalize(text_feats, dim=-1) # B T C text_feats = repeat(text_feats, \"B T C -\u003e B C T H W\", H=img_feats.shape[-2], W=img_feats.shape[-1]) return torch.cat((img_feats, text_feats), dim=1) # B 2C T H W def correlation(self, img_feats, text_feats): img_feats = F.normalize(img_feats, dim=1) # (5,768,24,24) text_feats = F.normalize(text_feats, dim=-1) # (5,150,80,768) corr = torch.einsum('bchw, btpc -\u003e bpthw', img_feats, text_feats) return corr # corr: (5,80,150,24,24) def corr_embed(self, x): B = x.shape[0] # x: (5,80,150,24,24) -\u003e (750, 80, 24, 24) corr_embed = rearrange(x, 'B P T H W -\u003e (B T) P H W') # x: (750, 80, 24, 24) -\u003e (750, 128, 24, 24) corr_embed = self.conv1(corr_embed) # x: (750, 128, 24, 24) -\u003e (5, 128, 150, 24, 24) corr_embed = rearrange(corr_embed, '(B T) C H W -\u003e B C T H W', B=B) return corr_embed def corr_projection(self, x, proj): corr_embed = rearrange(x, 'B C T H W -\u003e B T H W C') corr_embed = proj(corr_embed) corr_embed = rearrange(corr_embed, 'B T H W C -\u003e B C T H W') return corr_embed def upsample(self, x): B = x.shape[0] corr_embed = rearrange(x, 'B C T H W -\u003e (B T) C H W') corr_embed = F.interpolate(corr_embed, scale_factor=2, mode='bilinear', align_corners=True) corr_embed = rearrange(corr_embed, '(B T) C H W -\u003e B C T H W', B=B) return corr_embed def conv_decoder(self, x, guidance): B = x.shape[0] corr_embed = rearrange(x, 'B C T H W -\u003e (B T) C H W') corr_embed = self.decoder1(corr_embed, guidance[0]) corr_embed = self.decoder2(corr_embed, guidance[1]) corr_embed = self.head(corr_embed) corr_embed = rearrange(corr_embed, '(B T) () H W -\u003e B T H W', B=B) return corr_embed def forward(self, img_feats, text_feats, appearance_guidance): \"\"\" Arguments: img_feats: (B, C, H, W) text_feats: (B, T, P, C) apperance_guidance: tuple of (B, C, H, W) \"\"\" # text_feats: (5,150,80,768), img_feats: (5,768,24,24) corr = self.correlation(img_feats, text_feats) # corr: (5,80,150,24,24) corr_embed = self.corr_embed(corr) projected_guidance, projected_text_guidance, projected_decoder_guidance = None, None, [None, None] if self.guidance_projection is not None: # projected_guidance: (5,128,24,24) projected_guidance = self.guidance_projection(appearance_guidance[0]) if self.decoder_guidance_projection is not None: # 见下图👇🏻 projected_decoder_guidance = [proj(g) for proj, g in zip(self.decoder_guidance_projection, appearance_guidance[1:])] if self.text_guidance_projection is not None: # (5,150,80,768) -\u003e (5,150,768) text_feats = text_feats.mean(dim=-2) text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True) # (5,150,768) -\u003e (5,150,128) projected_text_guidance = self.text_guidance_projection(text_feats) # corr_embed: (5,80,150,24,24) -\u003e (5,80,150,24,24) -\u003e (5,80,150,24,24) for layer in self.layers: corr_embed = layer(corr_embed, projected_guidance, projected_text_guidance) # corr_embed: (5,80,150,24,24), 见最下面👇🏻 # logit: (5,150,96,96) logit = self.conv_decoder(corr_embed, projected_decoder_guidance) return logit ![image-20240401182112928](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401182112928.png)\nfor layer in self.layers: corr_embed = layer(corr_embed, projected_guidance, projected_text_guidance): with self.layers’s structure as follow:\nModuleList( (0): AggregatorLayer( (swin_block): SwinTransformerBlockWrapper( (block_1): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): Identity() (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (drop1): Dropout(p=0.0, inplace=False) (fc2): Linear(in_features=512, out_features=128, bias=True) (drop2): Dropout(p=0.0, inplace=False) ) ) (block_2): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): Identity() (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (drop1): Dropout(p=0.0, inplace=False) (fc2): Linear(in_features=512, out_features=128, bias=True) (drop2): Dropout(p=0.0, inplace=False) ) ) (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True) ) (attention): ClassTransformerLayer( (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0) (attention): AttentionLayer( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attention): LinearAttention() ) (MLP): Sequential( (0): Linear(in_features=128, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=128, bias=True) ) (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) ) ) (1): AggregatorLayer( (swin_block): SwinTransformerBlockWrapper( (block_1): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): Identity() (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (drop1): Dropout(p=0.0, inplace=False) (fc2): Linear(in_features=512, out_features=128, bias=True) (drop2): Dropout(p=0.0, inplace=False) ) ) (block_2): SwinTransformerBlock( (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (attn): WindowAttention( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attn_drop): Dropout(p=0.0, inplace=False) (proj): Linear(in_features=128, out_features=128, bias=True) (proj_drop): Dropout(p=0.0, inplace=False) (softmax): Softmax(dim=-1) ) (drop_path): Identity() (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (mlp): Mlp( (fc1): Linear(in_features=128, out_features=512, bias=True) (act): GELU(approximate='none') (drop1): Dropout(p=0.0, inplace=False) (fc2): Linear(in_features=512, out_features=128, bias=True) (drop2): Dropout(p=0.0, inplace=False) ) ) (guidance_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True) ) (attention): ClassTransformerLayer( (pool): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0) (attention): AttentionLayer( (q): Linear(in_features=256, out_features=128, bias=True) (k): Linear(in_features=256, out_features=128, bias=True) (v): Linear(in_features=128, out_features=128, bias=True) (attention): LinearAttention() ) (MLP): Sequential( (0): Linear(in_features=128, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=128, bias=True) ) (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True) (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True) ) ) ) and data flow in each AggregatorLayer is as follow (the change of shape is the same for two layers):\nfor layer in self.layers: # corr_embed: (5,128,150,24,24) # projected_guidance: (5,128,24,24) # projected_text_guidance: (5,150,128) corr_embed = layer(corr_embed, projected_guidance, projected_text_guidance) class AggregatorLayer(nn.Module): def __init__(self, hidden_dim=64, text_guidance_dim=512, appearance_guidance=512, nheads=4, input_resolution=(20, 20), pooling_size=(5, 5), window_size=(10, 10), attention_type='linear') -\u003e None: super().__init__() self.swin_block = SwinTransformerBlockWrapper(hidden_dim, appearance_guidance, input_resolution, nheads, window_size) self.attention = ClassTransformerLayer(hidden_dim, text_guidance_dim, nheads=nheads, attention_type=attention_type, pooling_size=pooling_size) def forward(self, x, appearance_guidance, text_guidance): \"\"\" Arguments: x: B C T H W \"\"\" # x: (5,128,150,24,24) # appearance_guidance: (5,128,24,24) # text_guidance: (5,150,128) # x: (5,128,150,24,24) -\u003e (5,128,150,24,24) x = self.swin_block(x, appearance_guidance) # x: (5,128,150,24,24) -\u003e (5,128,150,24,24) x = self.attention(x, text_guidance) return x For SwinTransformerBlockWrapper:\nclass SwinTransformerBlockWrapper(nn.Module): def __init__(self, dim, appearance_guidance_dim, input_resolution, nheads=4, window_size=5): super().__init__() self.block_1 = SwinTransformerBlock(dim, appearance_guidance_dim, input_resolution, num_heads=nheads, head_dim=None, window_size=window_size, shift_size=0) self.block_2 = SwinTransformerBlock(dim, appearance_guidance_dim, input_resolution, num_heads=nheads, head_dim=None, window_size=window_size, shift_size=window_size // 2) self.guidance_norm = nn.LayerNorm(appearance_guidance_dim) if appearance_guidance_dim \u003e 0 else None def forward(self, x, appearance_guidance): \"\"\" Arguments: x: B C T H W appearance_guidance: B C H W \"\"\" B, C, T, H, W = x.shape # x: (5,128,150,24,24) -\u003e (750,576,128) x = rearrange(x, 'B C T H W -\u003e (B T) (H W) C') if appearance_guidance is not None: # appearance_guidance: (5,128,24,24) -\u003e (750,576,128) -\u003e (750,576,128) appearance_guidance = self.guidance_norm(repeat(appearance_guidance, 'B C H W -\u003e (B T) (H W) C', T=T)) # x: (750,576,128) -\u003e (750,576,128) x = self.block_1(x, appearance_guidance) # x: (750,576,128) -\u003e (750,576,128) x = self.block_2(x, appearance_guidance) # x: (750,576,128) -\u003e (5,128,150,24,24) x = rearrange(x, '(B T) (H W) C -\u003e B C T H W', B=B, T=T, H=H, W=W) return x In SwinTransformerBlock:\nclass SwinTransformerBlock(nn.Module): def forward(self, x, appearance_guidance): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, \"input feature has wrong size\" shortcut = x x = self.norm1(x) # x: (750, 576, 128) -\u003e (750, 24, 24, 128) x = x.view(B, H, W, C) if appearance_guidance is not None: # appearance_guidance: (750, 576, 128) -\u003e (750, 24, 24, 128) appearance_guidance = appearance_guidance.view(B, H, W, -1) # x: (750, 24, 24, 128) -\u003e (750, 24, 24, 256) x = torch.cat([x, appearance_guidance], dim=-1) # cyclic shift if self.shift_size \u003e 0: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) else: shifted_x = x # partition windows x_windows = window_partition(shifted_x, self.window_size) # num_win*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, x_windows.shape[-1]) # num_win*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # num_win*B, window_size*window_size, C # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H' W' C # reverse cyclic shift if self.shift_size \u003e 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x x = x.view(B, H * W, C) # FFN x = shortcut + self.drop_path(x) x = x + self.drop_path(self.mlp(self.norm2(x))) # x: (750,576,128) return x For self.attention = ClassTransformerLayer:\nclass ClassTransformerLayer(nn.Module): def __init__(self, hidden_dim=64, guidance_dim=64, nheads=8, attention_type='linear', pooling_size=(4, 4)) -\u003e None: super().__init__() self.pool = nn.AvgPool2d(pooling_size) self.attention = AttentionLayer(hidden_dim, guidance_dim, nheads=nheads, attention_type=attention_type) self.MLP = nn.Sequential( nn.Linear(hidden_dim, hidden_dim * 4), nn.ReLU(), nn.Linear(hidden_dim * 4, hidden_dim) ) self.norm1 = nn.LayerNorm(hidden_dim) self.norm2 = nn.LayerNorm(hidden_dim) def pool_features(self, x): \"\"\" Intermediate pooling layer for computational efficiency. Arguments: x: B, C, T, H, W \"\"\" B = x.size(0) # x: (5,128,150,24,24) x = rearrange(x, 'B C T H W -\u003e (B T) C H W') x = self.pool(x) x = rearrange(x, '(B T) C H W -\u003e B C T H W', B=B) # x: (5,128,150,24,24) return x def forward(self, x, guidance): \"\"\" Arguments: x: B, C, T, H, W guidance: B, T, C \"\"\" B, _, _, H, W = x.size() # x: (5,128,150,24,24) # x_pool: (5,128,150,24,24) x_pool = self.pool_features(x) *_, H_pool, W_pool = x_pool.size() # x_pool: (5,128,150,24,24) -\u003e (2880,150,128) x_pool = rearrange(x_pool, 'B C T H W -\u003e (B H W) T C') if guidance is not None: # guidance: (5,150,128) -\u003e (2880,150,128) guidance = repeat(guidance, 'B T C -\u003e (B H W) T C', H=H_pool, W=W_pool) # x_pool: (2880,150,128) x_pool = x_pool + self.attention(self.norm1(x_pool), guidance) # 见下面👇🏻 x_pool = x_pool + self.MLP(self.norm2(x_pool)) # MLP # x_pool: (750,128,24,24) x_pool = rearrange(x_pool, '(B H W) T C -\u003e (B T) C H W', H=H_pool, W=W_pool) # x_pool: (750,128,24,24) x_pool = F.interpolate(x_pool, size=(H, W), mode='bilinear', align_corners=True) x_pool = rearrange(x_pool, '(B T) C H W -\u003e B C T H W', B=B) # x: (5,128,150,24,24) x = x + x_pool # Residual return x For self.attention(self.norm1(x_pool), guidance):\nclass AttentionLayer(nn.Module): def __init__(self, hidden_dim, guidance_dim, nheads=8, attention_type='linear'): super().__init__() self.nheads = nheads self.q = nn.Linear(hidden_dim + guidance_dim, hidden_dim) self.k = nn.Linear(hidden_dim + guidance_dim, hidden_dim) self.v = nn.Linear(hidden_dim, hidden_dim) if attention_type == 'linear': self.attention = LinearAttention() elif attention_type == 'full': self.attention = FullAttention() else: raise NotImplementedError def forward(self, x, guidance): \"\"\" Arguments: x: B, L, C guidance: B, L, C \"\"\" # q,k,v: (2880,150,128) q = self.q(torch.cat([x, guidance], dim=-1)) if guidance is not None else self.q(x) k = self.k(torch.cat([x, guidance], dim=-1)) if guidance is not None else self.k(x) v = self.v(x) # q,k,v: (2880,150,4,32) q = rearrange(q, 'B L (H D) -\u003e B L H D', H=self.nheads) k = rearrange(k, 'B S (H D) -\u003e B S H D', H=self.nheads) v = rearrange(v, 'B S (H D) -\u003e B S H D', H=self.nheads) # out: (2880,150,4,32) out = self.attention(q, k, v) # out: (2880,150,4,32) -\u003e (2880,150,128) out = rearrange(out, 'B L H D -\u003e B L (H D)') return out For self.conv_decoder(corr_embed, projected_decoder_guidance):\ndef conv_decoder(self, x, guidance): B = x.shape[0] # corr_embed: (750,128,24,24) corr_embed = rearrange(x, 'B C T H W -\u003e (B T) C H W') # corr_embed: (750,64,48,48) corr_embed = self.decoder1(corr_embed, guidance[0]) # corr_embed: (750,32,96,96) corr_embed = self.decoder2(corr_embed, guidance[1]) # corr_embed: (750,1,96,96) corr_embed = self.head(corr_embed) # corr_embed: (5,150,96,96) corr_embed = rearrange(corr_embed, '(B T) () H W -\u003e B T H W', B=B) return corr_embed class Up(nn.Module): \"\"\"Upscaling then double conv\"\"\" def __init__(self, in_channels, out_channels, guidance_channels): super().__init__() self.up = nn.ConvTranspose2d(in_channels, in_channels - guidance_channels, kernel_size=2, stride=2) self.conv = DoubleConv(in_channels, out_channels) def forward(self, x, guidance=None): # x: (750,128,24,24) -\u003e (750,96,48,48) x = self.up(x) if guidance is not None: T = x.size(0) // guidance.size(0) # guidance: (5,32,48,48) -\u003e (750,32,48,48) guidance = repeat(guidance, \"B C H W -\u003e (B T) C H W\", T=T) # x: (750,96,48,48) -\u003e (750,128,48,48) x = torch.cat([x, guidance], dim=1) # x: (750,128,48,48) -\u003e (750,64,48,48) return self.conv(x) 2. Unknown stuff Loss computation GroupNorm ",
  "wordCount" : "4632",
  "inLanguage": "en",
  "datePublished": "2024-03-15T05:21:09-05:00",
  "dateModified": "2024-03-15T05:21:09-05:00",
  "author":{
    "@type": "Person",
    "name": "Banghao Chi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://banghao.live/blog/catseg/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Banghao's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://banghao.live/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://banghao.live/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://banghao.live/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      CATseg: A complete walk through of the model architecture
    </h1>
    <div class="post-meta"><span title='2024-03-15 05:21:09 -0500 -0500'>March 15, 2024</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h3 id="1-model-architecture-setup-and-evaluation-data-flowfor-ade150k">1. Model Architecture setup and evaluation data flow(for ade150k)<a hidden class="anchor" aria-hidden="true" href="#1-model-architecture-setup-and-evaluation-data-flowfor-ade150k">#</a></h3>
<p><code>CATSeg</code> setup:</p>
<ul>
<li>
<p><code>backbone</code>: D2SwinTransformer -&gt; Swintransformer -&gt; BasicLayer(2) -&gt; SwinTransformerBlock -&gt; WindowAttention</p>
</li>
<li>
<p><code>sem_seg_head</code>: <code>CATSegHead.from_config</code> -&gt; <code>CATSegPredictor</code> -&gt;</p>
<ul>
<li>
<p>Load CLIP model -&gt; Load text templates -&gt; <code>class_embeddings(self.class_texts, prompt_templates, clip_model)</code> -&gt; for each class:</p>
<ul>
<li>bpe encode classname in different templates and save results in variable <code>texts</code> <strong>(80(number of templates), 77(number of sentence length))</strong>.</li>
<li>CLIP encode <code>texts</code> :
<ul>
<li><code>texts</code> go through <code>token_embedding</code>(<code>nn.Embedding</code>) <strong>(80,77,768(hidden_dim))</strong></li>
<li><code>texts</code> go through a 12 layers of ResidualAttentionBlock <strong>(80,77,768)</strong></li>
<li>take features of <code>texts</code> from the <code>eot_token</code> <strong>(80,768)</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p>do the above for all classes <strong>(150(number of test classes),80,768)</strong></p>
</li>
<li>
<p><code>Aggregator</code> -&gt; 2 layers of <code>AggregatorLayer</code>:</p>
<ul>
<li>
<p><code>swin_block</code>:</p>
<ul>
<li>
<p><code>SwinTransformerBlockWrapper</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlockWrapper</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, appearance_guidance_dim, input_resolution, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_1 <span style="color:#f92672">=</span> SwinTransformerBlock(dim,
</span></span><span style="display:flex;"><span>                                            appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution,
</span></span><span style="display:flex;"><span>                                            num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                                            window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_2 <span style="color:#f92672">=</span> SwinTransformerBlock(dim,
</span></span><span style="display:flex;"><span>                                            appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution,
</span></span><span style="display:flex;"><span>                                            num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                                            window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span>window_size <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(appearance_guidance_dim) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><code>attention</code>:</p>
<ul>
<li>
<p><code>ClassTransformerLayer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ClassTransformerLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(pooling_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> AttentionLayer(hidden_dim,
</span></span><span style="display:flex;"><span>                                        guidance_dim,
</span></span><span style="display:flex;"><span>                                        nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                        attention_type<span style="color:#f92672">=</span>attention_type)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>MLP <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, hidden_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LinearAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_map <span style="color:#f92672">=</span> elu_feature_map
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, queries, keys, values):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34; Multi-Head linear attention proposed in &#34;Transformers are RNNs&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queries: [N, L, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            keys: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            values: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            q_mask: [N, L]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            kv_mask: [N, S]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queried_values: (N, L, H, D)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_map(queries)
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_map(keys)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        v_length <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        values <span style="color:#f92672">=</span> values <span style="color:#f92672">/</span> v_length  <span style="color:#75715e"># prevent fp16 overflow</span>
</span></span><span style="display:flex;"><span>        KV <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nshd,nshv-&gt;nhdv&#34;</span>, K, values)  <span style="color:#75715e"># (S,D)&#39; @ S,V</span>
</span></span><span style="display:flex;"><span>        Z <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nhd-&gt;nlh&#34;</span>, Q, K<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>        queried_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nhdv,nlh-&gt;nlhv&#34;</span>, Q, KV, Z) <span style="color:#f92672">*</span> v_length
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> queried_values<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FullAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, use_dropout<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, attention_dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>use_dropout <span style="color:#f92672">=</span> use_dropout
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(attention_dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, queries, keys, values, q_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, kv_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34; Multi-head scaled dot-product attention, a.k.a full attention.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queries: [N, L, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            keys: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            values: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            q_mask: [N, L]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            kv_mask: [N, S]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queried_values: (N, L, H, D)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute the unnormalized attention and apply the masks</span>
</span></span><span style="display:flex;"><span>        QK <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nshd-&gt;nlsh&#34;</span>, queries, keys)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> kv_mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            QK<span style="color:#f92672">.</span>masked_fill_(<span style="color:#f92672">~</span>(q_mask[:, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> kv_mask[:, <span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>]), float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute the attention and the weighted average</span>
</span></span><span style="display:flex;"><span>        softmax_temp <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span> <span style="color:#f92672">/</span> queries<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">.5</span>  <span style="color:#75715e"># sqrt(D)</span>
</span></span><span style="display:flex;"><span>        A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(softmax_temp <span style="color:#f92672">*</span> QK, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_dropout:
</span></span><span style="display:flex;"><span>            A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(A)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        queried_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlsh,nshd-&gt;nlhd&#34;</span>, A, values)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> queried_values<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AttentionLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim, guidance_dim, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>nheads <span style="color:#f92672">=</span> nheads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;linear&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> LinearAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;full&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> FullAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Remaining of <code>Aggregator</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(text_guidance_dim, text_guidance_proj_dim),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">if</span> text_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>Conv2d(d, dp, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    ) <span style="color:#66d9ef">for</span> d, dp <span style="color:#f92672">in</span> zip(decoder_guidance_dims, decoder_guidance_proj_dims)
</span></span><span style="display:flex;"><span>]) <span style="color:#66d9ef">if</span> decoder_guidance_dims[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder1 <span style="color:#f92672">=</span> Up(hidden_dim, decoder_dims[<span style="color:#ae81ff">0</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder2 <span style="color:#f92672">=</span> Up(decoder_dims[<span style="color:#ae81ff">0</span>], decoder_dims[<span style="color:#ae81ff">1</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(decoder_dims[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><ul>
<li>
<p><code>Up</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Up</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channels, out_channels, guidance_channels):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>up <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels, in_channels <span style="color:#f92672">-</span> guidance_channels, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>) <span style="color:#f92672">//</span> guidance<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#34;B C H W -&gt; (B T) C H W&#34;</span>, T<span style="color:#f92672">=</span>T)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>conv(x)
</span></span></code></pre></div></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>CATSeg</code> forward for each image:</p>
<ul>
<li>
<p><code>image</code> <strong>(3,640,854)</strong> -&gt; <code>self.inference_sliding_window(batched_inputs)</code></p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>image <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>), size<span style="color:#f92672">=</span>out_res, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>squeeze()
</span></span></code></pre></div><p>-&gt; <strong>(3,640,640)</strong></p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>image <span style="color:#f92672">=</span> rearrange(unfold(image), <span style="color:#e6db74">&#34;(C H W) L-&gt; L C H W&#34;</span>, C<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, H<span style="color:#f92672">=</span>kernel)
</span></span></code></pre></div><p>-&gt; <strong>(442368(3x384x384(kernel size)),4(number of such patch))</strong> -&gt; <strong>(4,3,384,384)</strong></p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>global_image <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>                             size<span style="color:#f92672">=</span>(kernel, kernel),
</span></span><span style="display:flex;"><span>                             mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>,
</span></span><span style="display:flex;"><span>                             align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((image, global_image), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>-&gt; <strong>(5,3,384,384)</strong> 与下面呼应！</p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backbone(images) <span style="color:#75715e"># features: a dictionary with length of 3</span>
</span></span><span style="display:flex;"><span>clip_features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sem_seg_head<span style="color:#f92672">.</span>predictor<span style="color:#f92672">.</span>clip_model<span style="color:#f92672">.</span>encode_image(clip_images, dense<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># clip_images: (5, 3, 336, 336)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># outputs: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sem_seg_head(clip_features, features)
</span></span></code></pre></div><ul>
<li><code>features</code>:
![image-20240401003703609](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401003703609.png)</li>
<li><code>clip_features</code>: <strong>(5,577(24x24+1),768)</strong></li>
<li><code>outputs</code>: <strong>(5,150(number of classes),96,96)</strong></li>
</ul>
</li>
<li>
<p>After the three steps: <code>outputs</code> -&gt; <strong>(5,150,96,96)</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># outputs: (5,150,96,96) -&gt; (5,150,384,384)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(outputs,
</span></span><span style="display:flex;"><span>                        size<span style="color:#f92672">=</span>kernel,
</span></span><span style="display:flex;"><span>                        mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bilinear&#34;</span>,
</span></span><span style="display:flex;"><span>                        align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)   <span style="color:#75715e"># -&gt; (5,150,384,384) 与上面呼应！</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>sigmoid()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>global_output <span style="color:#f92672">=</span> outputs[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>global_output <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(global_output,
</span></span><span style="display:flex;"><span>                              size<span style="color:#f92672">=</span>out_res,
</span></span><span style="display:flex;"><span>                              mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>,
</span></span><span style="display:flex;"><span>                              align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,)
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> outputs[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]												 <span style="color:#75715e"># -&gt; (4,150,384,384)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> fold(outputs<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> fold(unfold(torch<span style="color:#f92672">.</span>ones([<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> out_res, device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device)))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fenzi: (4,22118400) -&gt; (22118400,4) -&gt; (150,640,640)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fenmu: (1,640,640)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This steps normalize the effects brought by the fold operation.</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> (outputs <span style="color:#f92672">+</span> global_output) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -&gt; (1,150,640,640)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>height <span style="color:#f92672">=</span> batched_inputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;height&#34;</span>, out_res[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>width <span style="color:#f92672">=</span> batched_inputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;width&#34;</span>, out_res[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> sem_seg_postprocess(outputs[<span style="color:#ae81ff">0</span>], out_res, height, width)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -&gt; (150,512,683)</span>
</span></span></code></pre></div></li>
</ul>
<h3 id="the-workflow-within-three-main-steps">The workflow within three main steps<a hidden class="anchor" aria-hidden="true" href="#the-workflow-within-three-main-steps">#</a></h3>
</li>
<li>
<p>The workflow within <code>features = self.backbone(images) # features: a dictionary with length of 3</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">D2SwinTransformer</span>(SwinTransformer, Backbone):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x): <span style="color:#75715e"># x -&gt; (5,3,384,384)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            dict[str-&gt;Tensor]: names and the corresponding features
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> (
</span></span><span style="display:flex;"><span>            x<span style="color:#f92672">.</span>dim() <span style="color:#f92672">==</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>        ), <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;SwinTransformer takes an input of shape (N, C, H, W). Got </span><span style="color:#e6db74">{</span>x<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74"> instead!&#34;</span>
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> super()<span style="color:#f92672">.</span>forward(x) <span style="color:#75715e"># y -&gt; a dict of three tensors: {(5,128,96,96), (5,256,48,48), (5,512,24,24)}. Same for shape of the outputs</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> y<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> k <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>_out_features:
</span></span><span style="display:flex;"><span>                outputs[k] <span style="color:#f92672">=</span> y[k]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> outputs
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>patch_embed(x) <span style="color:#75715e"># (5,3,384,384) -&gt; (5,128,96,96) 解释在下面</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        Wh, Ww <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">2</span>), x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>ape:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># interpolate the position embedding to the corresponding size</span>
</span></span><span style="display:flex;"><span>            absolute_pos_embed <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>absolute_pos_embed, size<span style="color:#f92672">=</span>(Wh, Ww), mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bicubic&#34;</span>
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> (x <span style="color:#f92672">+</span> absolute_pos_embed)<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># B Wh*Ww C</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># -&gt; (5,9216(96x96),128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pos_drop(x) <span style="color:#75715e"># no change (5,9216,128)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        outs <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_layers):
</span></span><span style="display:flex;"><span>            layer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[i]
</span></span><span style="display:flex;"><span>            x_out, H, W, x, Wh, Ww <span style="color:#f92672">=</span> layer(x, Wh, Ww) <span style="color:#75715e"># x_out -&gt; (5,9216,128)/(5,2304,256)/(5,576,256)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>out_indices:
</span></span><span style="display:flex;"><span>                norm_layer <span style="color:#f92672">=</span> getattr(self, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;norm</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>                x_out <span style="color:#f92672">=</span> norm_layer(x_out) <span style="color:#75715e"># no change (5,9216,128)/(5,2304,256)/(5,576,512)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                out <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, H, W, self<span style="color:#f92672">.</span>num_features[i])<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous() <span style="color:#75715e"># out: (5,128,96,96)/(5,256,48,48)/(5,512,24,24)</span>
</span></span><span style="display:flex;"><span>                outs[<span style="color:#e6db74">&#34;res</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>)] <span style="color:#f92672">=</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> outs
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PatchEmbed</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># padding</span>
</span></span><span style="display:flex;"><span>        _, _, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> W <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>pad(x, (<span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> W <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> H <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>pad(x, (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> H <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proj(x)  <span style="color:#75715e"># B C Wh Ww (5,3,384,384) -&gt; (5,128,96,96)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>norm <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            Wh, Ww <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">2</span>), x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm(x)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>embed_dim, Wh, Ww)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#75715e"># (5,128,96,96) 传回上面👆🏻</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>ModuleList(
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">0</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.014</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (downsample): PatchMerging(
</span></span><span style="display:flex;"><span>      (reduction): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>      (norm): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">1</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.029</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.043</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (downsample): PatchMerging(
</span></span><span style="display:flex;"><span>      (reduction): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>      (norm): LayerNorm((<span style="color:#ae81ff">1024</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">2</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.057</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.071</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">2</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.086</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">3</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.100</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">4</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.114</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">5</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.129</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">6</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.143</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">7</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.157</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">8</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.171</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">9</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.186</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">10</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.200</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">11</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.214</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">12</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.229</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">13</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.243</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">14</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.257</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">15</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.271</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">16</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.286</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">17</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.300</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BasicLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, H, W):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: Input feature, tensor size (B, H*W, C).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            H, W: Spatial resolution of the input feature.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># calculate attention mask for SW-MSA</span>
</span></span><span style="display:flex;"><span>        Hp <span style="color:#f92672">=</span> int(np<span style="color:#f92672">.</span>ceil(H <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>window_size)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        Wp <span style="color:#f92672">=</span> int(np<span style="color:#f92672">.</span>ceil(W <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>window_size)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        img_mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, Hp, Wp, <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>device)  <span style="color:#75715e"># 1 Hp Wp 1</span>
</span></span><span style="display:flex;"><span>        h_slices <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            slice(<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        w_slices <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            slice(<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        cnt <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> h <span style="color:#f92672">in</span> h_slices:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> w_slices:
</span></span><span style="display:flex;"><span>                img_mask[:, h, w, :] <span style="color:#f92672">=</span> cnt
</span></span><span style="display:flex;"><span>                cnt <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mask_windows <span style="color:#f92672">=</span> window_partition(
</span></span><span style="display:flex;"><span>            img_mask, self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        )  <span style="color:#75715e"># nW, window_size, window_size, 1</span>
</span></span><span style="display:flex;"><span>        mask_windows <span style="color:#f92672">=</span> mask_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size)
</span></span><span style="display:flex;"><span>        attn_mask <span style="color:#f92672">=</span> mask_windows<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> mask_windows<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        attn_mask <span style="color:#f92672">=</span> attn_mask<span style="color:#f92672">.</span>masked_fill(attn_mask <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#f92672">-</span><span style="color:#ae81ff">100.0</span>))<span style="color:#f92672">.</span>masked_fill(
</span></span><span style="display:flex;"><span>            attn_mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> blk <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>blocks:
</span></span><span style="display:flex;"><span>            blk<span style="color:#f92672">.</span>H, blk<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> H, W
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_checkpoint:
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> checkpoint<span style="color:#f92672">.</span>checkpoint(blk, x, attn_mask)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> blk(x, attn_mask) <span style="color:#75715e"># (5,9216,128) -&gt; (5,9216,128)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>downsample <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            x_down <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>downsample(x, H, W)
</span></span><span style="display:flex;"><span>            Wh, Ww <span style="color:#f92672">=</span> (H <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>, (W <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> x, H, W, x_down, Wh, Ww
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> x, H, W, x, H, W
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>The workflow within <code>clip_features = self.sem_seg_head.predictor.clip_model.encode_image(clip_images, dense=True) # clip_images: (5, 3, 336, 336), clip_features: (5, 577, 768)</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">VisualTransformer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor, dense<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># (5,3,336,336)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)  <span style="color:#75715e"># shape = [5, 1024, 24, 24]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reshape(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 1024, 576(24x24)]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 576, 1024]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>class_embedding<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype) <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>zeros(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, x<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], dtype<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>dtype, device<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>device), x], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 576+1, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> dense <span style="color:#f92672">and</span> (x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">!=</span> self<span style="color:#f92672">.</span>positional_embedding<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>resized_pos_embed(self<span style="color:#f92672">.</span>input_resolution, x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>positional_embedding<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype) <span style="color:#75715e"># shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_pre(x) <span style="color:#75715e"># shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># NLD -&gt; LND</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(x, dense)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># LND -&gt; NLD shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> dense:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_post(x[:, :, :])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_post(x[:, <span style="color:#ae81ff">0</span>, :])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>proj <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>proj <span style="color:#75715e"># shape -&gt; [5, 577, 768]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#75715e"># shape = [5, 577, 768]</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>The workflow within <code>outputs = self.sem_seg_head(clip_features, features)</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CATSegHead</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, features, guidance_features):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            img_feats: (B, C, HW)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            affinity_features: (B, C, )
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># features: (5,577,768) -&gt; (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        img_feat <span style="color:#f92672">=</span> rearrange(features[:, <span style="color:#ae81ff">1</span>:, :], <span style="color:#e6db74">&#34;b (h w) c-&gt;b c h w&#34;</span>, h<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>feature_resolution[<span style="color:#ae81ff">0</span>], w<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>feature_resolution[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>predictor(img_feat, guidance_features)
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CATSegPredictor</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>  	<span style="color:#75715e"># self.transformer -&gt; Aggregator!</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, vis_guidance):
</span></span><span style="display:flex;"><span>        vis <span style="color:#f92672">=</span> [vis_guidance[k] <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> vis_guidance<span style="color:#f92672">.</span>keys()][::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text: (150, 80, 768)</span>
</span></span><span style="display:flex;"><span>        text <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>text_features <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>training <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>text_features_test
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text -&gt; (5, 150, 80, 768)</span>
</span></span><span style="display:flex;"><span>        text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>repeat(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(x, text, vis) <span style="color:#75715e"># This Aggregator part: below👇🏻</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div></li>
<li>
<p><code>text_feats</code>: <strong>(5,150,80,768)</strong>, <code>img_feats</code>: <strong>(5,768,24,24)</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Aggregator</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,
</span></span><span style="display:flex;"><span>        text_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>        text_guidance_proj_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        appearance_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>        appearance_guidance_proj_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        decoder_dims <span style="color:#f92672">=</span> (<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>),
</span></span><span style="display:flex;"><span>        decoder_guidance_dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>),
</span></span><span style="display:flex;"><span>        decoder_guidance_proj_dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">16</span>),
</span></span><span style="display:flex;"><span>        num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>        nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>        hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>),
</span></span><span style="display:flex;"><span>        feature_resolution<span style="color:#f92672">=</span>(<span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">24</span>),
</span></span><span style="display:flex;"><span>        window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
</span></span><span style="display:flex;"><span>        attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>,
</span></span><span style="display:flex;"><span>        prompt_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span>,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_layers <span style="color:#f92672">=</span> num_layers
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            AggregatorLayer(
</span></span><span style="display:flex;"><span>                hidden_dim<span style="color:#f92672">=</span>hidden_dim, text_guidance_dim<span style="color:#f92672">=</span>text_guidance_proj_dim, appearance_guidance<span style="color:#f92672">=</span>appearance_guidance_proj_dim,
</span></span><span style="display:flex;"><span>                nheads<span style="color:#f92672">=</span>nheads, input_resolution<span style="color:#f92672">=</span>feature_resolution, pooling_size<span style="color:#f92672">=</span>pooling_size, window_size<span style="color:#f92672">=</span>window_size, attention_type<span style="color:#f92672">=</span>attention_type
</span></span><span style="display:flex;"><span>            ) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_layers)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(prompt_channel, hidden_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(text_guidance_dim, text_guidance_proj_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">if</span> text_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>Conv2d(d, dp, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            ) <span style="color:#66d9ef">for</span> d, dp <span style="color:#f92672">in</span> zip(decoder_guidance_dims, decoder_guidance_proj_dims)
</span></span><span style="display:flex;"><span>        ]) <span style="color:#66d9ef">if</span> decoder_guidance_dims[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder1 <span style="color:#f92672">=</span> Up(hidden_dim, decoder_dims[<span style="color:#ae81ff">0</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder2 <span style="color:#f92672">=</span> Up(decoder_dims[<span style="color:#ae81ff">0</span>], decoder_dims[<span style="color:#ae81ff">1</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(decoder_dims[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#---------------------------------------------------------------------------------#</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">feature_map</span>(self, img_feats, text_feats):
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(img_feats, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B C H W</span>
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> repeat(img_feats, <span style="color:#e6db74">&#34;B C H W -&gt; B C T H W&#34;</span>, T<span style="color:#f92672">=</span>text_feats<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B T P C</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> text_feats<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B T C</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> repeat(text_feats, <span style="color:#e6db74">&#34;B T C -&gt; B C T H W&#34;</span>, H<span style="color:#f92672">=</span>img_feats<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>], W<span style="color:#f92672">=</span>img_feats<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat((img_feats, text_feats), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B 2C T H W</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">correlation</span>(self, img_feats, text_feats):
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(img_feats, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (5,150,80,768)</span>
</span></span><span style="display:flex;"><span>        corr <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;bchw, btpc -&gt; bpthw&#39;</span>, img_feats, text_feats)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr <span style="color:#75715e"># corr: (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">corr_embed</span>(self, x):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,80,150,24,24) -&gt; (750, 80, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B P T H W -&gt; (B T) P H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 80, 24, 24) -&gt; (750, 128, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(corr_embed)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 128, 24, 24) -&gt; (5, 128, 150, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">corr_projection</span>(self, x, proj):
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; B T H W C&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> proj(corr_embed)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;B T H W C -&gt; B C T H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">upsample</span>(self, x):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(corr_embed, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv_decoder</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder1(corr_embed, guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder2(corr_embed, guidance[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>head(corr_embed)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) () H W -&gt; B T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, img_feats, text_feats, appearance_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            img_feats: (B, C, H, W)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            text_feats: (B, T, P, C)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            apperance_guidance: tuple of (B, C, H, W)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text_feats: (5,150,80,768), img_feats: (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        corr <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>correlation(img_feats, text_feats) <span style="color:#75715e"># corr: (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>corr_embed(corr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        projected_guidance, projected_text_guidance, projected_decoder_guidance <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>, [<span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># projected_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>            projected_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>guidance_projection(appearance_guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># 见下图👇🏻</span>
</span></span><span style="display:flex;"><span>            projected_decoder_guidance <span style="color:#f92672">=</span> [proj(g) <span style="color:#66d9ef">for</span> proj, g <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>decoder_guidance_projection, appearance_guidance[<span style="color:#ae81ff">1</span>:])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># (5,150,80,768) -&gt; (5,150,768)</span>
</span></span><span style="display:flex;"><span>            text_feats <span style="color:#f92672">=</span> text_feats<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            text_feats <span style="color:#f92672">=</span> text_feats <span style="color:#f92672">/</span> text_feats<span style="color:#f92672">.</span>norm(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># (5,150,768) -&gt; (5,150,128)</span>
</span></span><span style="display:flex;"><span>            projected_text_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>text_guidance_projection(text_feats)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (5,80,150,24,24) -&gt; (5,80,150,24,24) -&gt; (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>            corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance,
</span></span><span style="display:flex;"><span>                               projected_text_guidance)
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># corr_embed: (5,80,150,24,24), 见最下面👇🏻</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># logit: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>        logit <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_decoder(corr_embed, projected_decoder_guidance)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logit
</span></span></code></pre></div><p>![image-20240401182112928](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401182112928.png)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>		corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance, projected_text_guidance):
</span></span></code></pre></div><p>with <code>self.layers</code>&rsquo;s structure as follow:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ModuleList(
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">0</span>): AggregatorLayer(
</span></span><span style="display:flex;"><span>    (swin_block): SwinTransformerBlockWrapper(
</span></span><span style="display:flex;"><span>      (block_1): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (block_2): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (guidance_norm): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (attention): ClassTransformerLayer(
</span></span><span style="display:flex;"><span>      (pool): AvgPool2d(kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], stride<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>      (attention): AttentionLayer(
</span></span><span style="display:flex;"><span>        (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attention): LinearAttention()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (MLP): Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): ReLU()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">1</span>): AggregatorLayer(
</span></span><span style="display:flex;"><span>    (swin_block): SwinTransformerBlockWrapper(
</span></span><span style="display:flex;"><span>      (block_1): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (block_2): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (guidance_norm): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (attention): ClassTransformerLayer(
</span></span><span style="display:flex;"><span>      (pool): AvgPool2d(kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], stride<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>      (attention): AttentionLayer(
</span></span><span style="display:flex;"><span>        (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attention): LinearAttention()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (MLP): Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): ReLU()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>and data flow in each <code>AggregatorLayer</code> is as follow (the change of shape is the same for two layers):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>  	<span style="color:#75715e"># corr_embed: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># projected_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># projected_text_guidance: (5,150,128)</span>
</span></span><span style="display:flex;"><span>		corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance, projected_text_guidance)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AggregatorLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, text_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>                 appearance_guidance<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, input_resolution<span style="color:#f92672">=</span>(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">20</span>),
</span></span><span style="display:flex;"><span>                 pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), window_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>),
</span></span><span style="display:flex;"><span>                 attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>swin_block <span style="color:#f92672">=</span> SwinTransformerBlockWrapper(hidden_dim,
</span></span><span style="display:flex;"><span>                                                      appearance_guidance,
</span></span><span style="display:flex;"><span>                                                      input_resolution, nheads,
</span></span><span style="display:flex;"><span>                                                      window_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> ClassTransformerLayer(hidden_dim,
</span></span><span style="display:flex;"><span>                                               text_guidance_dim,
</span></span><span style="display:flex;"><span>                                               nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                               attention_type<span style="color:#f92672">=</span>attention_type,
</span></span><span style="display:flex;"><span>                                               pooling_size<span style="color:#f92672">=</span>pooling_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance, text_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B C T H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>    		<span style="color:#75715e"># appearance_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>    		<span style="color:#75715e"># text_guidance: (5,150,128)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>swin_block(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attention(x, text_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>SwinTransformerBlockWrapper</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlockWrapper</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                 input_resolution, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_1 <span style="color:#f92672">=</span> SwinTransformerBlock(dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution, num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_2 <span style="color:#f92672">=</span> SwinTransformerBlock(dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution, num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span>window_size <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(appearance_guidance_dim) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B C T H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            appearance_guidance: B C H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B, C, T, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) (H W) C&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> appearance_guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># appearance_guidance: (5,128,24,24) -&gt; (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>            appearance_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>guidance_norm(repeat(appearance_guidance, <span style="color:#e6db74">&#39;B C H W -&gt; (B T) (H W) C&#39;</span>, T<span style="color:#f92672">=</span>T))
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>block_1(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>block_2(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;(B T) (H W) C -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B, T<span style="color:#f92672">=</span>T, H<span style="color:#f92672">=</span>H, W<span style="color:#f92672">=</span>W)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>In <code>SwinTransformerBlock</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance):
</span></span><span style="display:flex;"><span>        H, W <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>input_resolution
</span></span><span style="display:flex;"><span>        B, L, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> L <span style="color:#f92672">==</span> H <span style="color:#f92672">*</span> W, <span style="color:#e6db74">&#34;input feature has wrong size&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        shortcut <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm1(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 576, 128) -&gt; (750, 24, 24, 128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(B, H, W, C)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> appearance_guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># appearance_guidance: (750, 576, 128) -&gt; (750, 24, 24, 128)</span>
</span></span><span style="display:flex;"><span>            appearance_guidance <span style="color:#f92672">=</span> appearance_guidance<span style="color:#f92672">.</span>view(B, H, W, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750, 24, 24, 128) -&gt; (750, 24, 24, 256)</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, appearance_guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># cyclic shift</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>shift_size <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            shifted_x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(x, shifts<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size), dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            shifted_x <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># partition windows</span>
</span></span><span style="display:flex;"><span>        x_windows <span style="color:#f92672">=</span> window_partition(shifted_x, self<span style="color:#f92672">.</span>window_size)  <span style="color:#75715e"># num_win*B, window_size, window_size, C</span>
</span></span><span style="display:flex;"><span>        x_windows <span style="color:#f92672">=</span> x_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size, x_windows<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])  <span style="color:#75715e"># num_win*B, window_size*window_size, C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># W-MSA/SW-MSA</span>
</span></span><span style="display:flex;"><span>        attn_windows <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn(x_windows, mask<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>attn_mask)  <span style="color:#75715e"># num_win*B, window_size*window_size, C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># merge windows</span>
</span></span><span style="display:flex;"><span>        attn_windows <span style="color:#f92672">=</span> attn_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size, self<span style="color:#f92672">.</span>window_size, C)
</span></span><span style="display:flex;"><span>        shifted_x <span style="color:#f92672">=</span> window_reverse(attn_windows, self<span style="color:#f92672">.</span>window_size, H, W)  <span style="color:#75715e"># B H&#39; W&#39; C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># reverse cyclic shift</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>shift_size <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(shifted_x, shifts<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>shift_size, self<span style="color:#f92672">.</span>shift_size), dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> shifted_x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(B, H <span style="color:#f92672">*</span> W, C)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># FFN</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> shortcut <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>drop_path(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>drop_path(self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>norm2(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>self.attention = ClassTransformerLayer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ClassTransformerLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>                 attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(pooling_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> AttentionLayer(hidden_dim, guidance_dim,
</span></span><span style="display:flex;"><span>                                        nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                        attention_type<span style="color:#f92672">=</span>attention_type)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>MLP <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, hidden_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pool_features</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Intermediate pooling layer for computational efficiency.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, C, T, H, W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, C, T, H, W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            guidance: B, T, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B, _, _, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool_features(x)
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">*</span>_, H_pool, W_pool <span style="color:#f92672">=</span> x_pool<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (5,128,150,24,24) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;B C T H W -&gt; (B H W) T C&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># guidance: (5,150,128) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#39;B T C -&gt; (B H W) T C&#39;</span>, H<span style="color:#f92672">=</span>H_pool, W<span style="color:#f92672">=</span>W_pool)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> x_pool <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>attention(self<span style="color:#f92672">.</span>norm1(x_pool), guidance) <span style="color:#75715e"># 见下面👇🏻</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> x_pool <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>MLP(self<span style="color:#f92672">.</span>norm2(x_pool)) <span style="color:#75715e"># MLP</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;(B H W) T C -&gt; (B T) C H W&#39;</span>, H<span style="color:#f92672">=</span>H_pool, W<span style="color:#f92672">=</span>W_pool)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(x_pool, size<span style="color:#f92672">=</span>(H, W), mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> x_pool <span style="color:#75715e"># Residual</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>self.attention(self.norm1(x_pool), guidance)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AttentionLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim, guidance_dim, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>nheads <span style="color:#f92672">=</span> nheads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;linear&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> LinearAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;full&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> FullAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, L, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            guidance: B, L, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># q,k,v: (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>q(torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>q(x)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>k(torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>k(x)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>v(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># q,k,v: (2880,150,4,32)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> rearrange(q, <span style="color:#e6db74">&#39;B L (H D) -&gt; B L H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> rearrange(k, <span style="color:#e6db74">&#39;B S (H D) -&gt; B S H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> rearrange(v, <span style="color:#e6db74">&#39;B S (H D) -&gt; B S H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># out: (2880,150,4,32)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attention(q, k, v)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># out: (2880,150,4,32) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> rearrange(out, <span style="color:#e6db74">&#39;B L H D -&gt; B L (H D)&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><p>For <code>self.conv_decoder(corr_embed, projected_decoder_guidance)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv_decoder</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>      	<span style="color:#75715e"># corr_embed: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,64,48,48)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder1(corr_embed, guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,32,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder2(corr_embed, guidance[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,1,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>head(corr_embed)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) () H W -&gt; B T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Up</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channels, out_channels, guidance_channels):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>up <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels,
</span></span><span style="display:flex;"><span>                                     in_channels <span style="color:#f92672">-</span> guidance_channels,
</span></span><span style="display:flex;"><span>                                     kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>      	<span style="color:#75715e"># x: (750,128,24,24) -&gt; (750,96,48,48)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>) <span style="color:#f92672">//</span> guidance<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># guidance: (5,32,48,48) -&gt; (750,32,48,48)</span>
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#34;B C H W -&gt; (B T) C H W&#34;</span>, T<span style="color:#f92672">=</span>T)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750,96,48,48) -&gt; (750,128,48,48)</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750,128,48,48) -&gt; (750,64,48,48)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>conv(x)
</span></span></code></pre></div></li>
</ul>
</li>
</ul>
<h3 id="2-unknown-stuff">2. Unknown stuff<a hidden class="anchor" aria-hidden="true" href="#2-unknown-stuff">#</a></h3>
<ul>
<li>Loss computation</li>
<li>GroupNorm</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://banghao.live/tags/open-vocabulary-segmentation/">Open Vocabulary Segmentation</a></li>
      <li><a href="https://banghao.live/tags/clip/">CLIP</a></li>
      <li><a href="https://banghao.live/tags/swin-transformer/">Swin Transformer</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on x"
            href="https://x.com/intent/tweet/?text=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture&amp;url=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f&amp;hashtags=OpenVocabularySegmentation%2cCLIP%2cSwinTransformer">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f&amp;title=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture&amp;summary=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture&amp;source=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f&title=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on whatsapp"
            href="https://api.whatsapp.com/send?text=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture%20-%20https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on telegram"
            href="https://telegram.me/share/url?text=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture&amp;url=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share CATseg: A complete walk through of the model architecture on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=CATseg%3a%20A%20complete%20walk%20through%20of%20the%20model%20architecture&u=https%3a%2f%2fbanghao.live%2fblog%2fcatseg%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://banghao.live/">Banghao&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
