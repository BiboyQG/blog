<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<script
  src="/blog/js/theme-toggle.min.3de3c47734bc776a4afab25ec9e41354fbca4c6cded1dbf7e15121cfaefe6350.js"
  integrity="sha256-PePEdzS8d2pK&#43;rJeyeQTVPvKTGze0dv34VEhz67&#43;Y1A="
></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://biboyqg.github.io/blog/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://biboyqg.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/" title="Home Page">
                    <span>Home Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      CATseg: A complete walk through of the model architecture
    </h1>
    <div class="post-meta"><span title='2024-03-15 05:21:09 -0500 -0500'>March 15, 2024</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h3 id="1-model-architecture-setup-and-evaluation-data-flowfor-ade150k">1. Model Architecture setup and evaluation data flow(for ade150k)<a hidden class="anchor" aria-hidden="true" href="#1-model-architecture-setup-and-evaluation-data-flowfor-ade150k">#</a></h3>
<p><code>CATSeg</code> setup:</p>
<ul>
<li>
<p><code>backbone</code>: D2SwinTransformer -&gt; Swintransformer -&gt; BasicLayer(2) -&gt; SwinTransformerBlock -&gt; WindowAttention</p>
</li>
<li>
<p><code>sem_seg_head</code>: <code>CATSegHead.from_config</code> -&gt; <code>CATSegPredictor</code> -&gt;</p>
<ul>
<li>
<p>Load CLIP model -&gt; Load text templates -&gt; <code>class_embeddings(self.class_texts, prompt_templates, clip_model)</code> -&gt; for each class:</p>
<ul>
<li>bpe encode classname in different templates and save results in variable <code>texts</code> <strong>(80(number of templates), 77(number of sentence length))</strong>.</li>
<li>CLIP encode <code>texts</code> :
<ul>
<li><code>texts</code> go through <code>token_embedding</code>(<code>nn.Embedding</code>) <strong>(80,77,768(hidden_dim))</strong></li>
<li><code>texts</code> go through a 12 layers of ResidualAttentionBlock <strong>(80,77,768)</strong></li>
<li>take features of <code>texts</code> from the <code>eot_token</code> <strong>(80,768)</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p>do the above for all classes <strong>(150(number of test classes),80,768)</strong></p>
</li>
<li>
<p><code>Aggregator</code> -&gt; 2 layers of <code>AggregatorLayer</code>:</p>
<ul>
<li>
<p><code>swin_block</code>:</p>
<ul>
<li>
<p><code>SwinTransformerBlockWrapper</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlockWrapper</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, appearance_guidance_dim, input_resolution, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_1 <span style="color:#f92672">=</span> SwinTransformerBlock(dim,
</span></span><span style="display:flex;"><span>                                            appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution,
</span></span><span style="display:flex;"><span>                                            num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                                            window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_2 <span style="color:#f92672">=</span> SwinTransformerBlock(dim,
</span></span><span style="display:flex;"><span>                                            appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution,
</span></span><span style="display:flex;"><span>                                            num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                                            window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span>window_size <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(appearance_guidance_dim) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p><code>attention</code>:</p>
<ul>
<li>
<p><code>ClassTransformerLayer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ClassTransformerLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(pooling_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> AttentionLayer(hidden_dim,
</span></span><span style="display:flex;"><span>                                        guidance_dim,
</span></span><span style="display:flex;"><span>                                        nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                        attention_type<span style="color:#f92672">=</span>attention_type)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>MLP <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, hidden_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LinearAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_map <span style="color:#f92672">=</span> elu_feature_map
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, queries, keys, values):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34; Multi-Head linear attention proposed in &#34;Transformers are RNNs&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queries: [N, L, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            keys: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            values: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            q_mask: [N, L]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            kv_mask: [N, S]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queried_values: (N, L, H, D)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        Q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_map(queries)
</span></span><span style="display:flex;"><span>        K <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_map(keys)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        v_length <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        values <span style="color:#f92672">=</span> values <span style="color:#f92672">/</span> v_length  <span style="color:#75715e"># prevent fp16 overflow</span>
</span></span><span style="display:flex;"><span>        KV <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nshd,nshv-&gt;nhdv&#34;</span>, K, values)  <span style="color:#75715e"># (S,D)&#39; @ S,V</span>
</span></span><span style="display:flex;"><span>        Z <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nhd-&gt;nlh&#34;</span>, Q, K<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>        queried_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nhdv,nlh-&gt;nlhv&#34;</span>, Q, KV, Z) <span style="color:#f92672">*</span> v_length
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> queried_values<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FullAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, use_dropout<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, attention_dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>use_dropout <span style="color:#f92672">=</span> use_dropout
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(attention_dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, queries, keys, values, q_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, kv_mask<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34; Multi-head scaled dot-product attention, a.k.a full attention.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queries: [N, L, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            keys: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            values: [N, S, H, D]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            q_mask: [N, L]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            kv_mask: [N, S]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            queried_values: (N, L, H, D)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute the unnormalized attention and apply the masks</span>
</span></span><span style="display:flex;"><span>        QK <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlhd,nshd-&gt;nlsh&#34;</span>, queries, keys)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> kv_mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            QK<span style="color:#f92672">.</span>masked_fill_(<span style="color:#f92672">~</span>(q_mask[:, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> kv_mask[:, <span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>]), float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute the attention and the weighted average</span>
</span></span><span style="display:flex;"><span>        softmax_temp <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span> <span style="color:#f92672">/</span> queries<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">.5</span>  <span style="color:#75715e"># sqrt(D)</span>
</span></span><span style="display:flex;"><span>        A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(softmax_temp <span style="color:#f92672">*</span> QK, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_dropout:
</span></span><span style="display:flex;"><span>            A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(A)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        queried_values <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;nlsh,nshd-&gt;nlhd&#34;</span>, A, values)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> queried_values<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AttentionLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim, guidance_dim, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>nheads <span style="color:#f92672">=</span> nheads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;linear&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> LinearAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;full&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> FullAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>Remaining of <code>Aggregator</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(text_guidance_dim, text_guidance_proj_dim),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">if</span> text_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>Conv2d(d, dp, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    ) <span style="color:#66d9ef">for</span> d, dp <span style="color:#f92672">in</span> zip(decoder_guidance_dims, decoder_guidance_proj_dims)
</span></span><span style="display:flex;"><span>]) <span style="color:#66d9ef">if</span> decoder_guidance_dims[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder1 <span style="color:#f92672">=</span> Up(hidden_dim, decoder_dims[<span style="color:#ae81ff">0</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>decoder2 <span style="color:#f92672">=</span> Up(decoder_dims[<span style="color:#ae81ff">0</span>], decoder_dims[<span style="color:#ae81ff">1</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(decoder_dims[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><ul>
<li>
<p><code>Up</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Up</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channels, out_channels, guidance_channels):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>up <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels, in_channels <span style="color:#f92672">-</span> guidance_channels, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>) <span style="color:#f92672">//</span> guidance<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#34;B C H W -&gt; (B T) C H W&#34;</span>, T<span style="color:#f92672">=</span>T)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>conv(x)
</span></span></code></pre></div></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>CATSeg</code> forward for each image:</p>
<ul>
<li>
<p><code>image</code> <strong>(3,640,854)</strong> -&gt; <code>self.inference_sliding_window(batched_inputs)</code></p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>image <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>), size<span style="color:#f92672">=</span>out_res, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>squeeze()
</span></span></code></pre></div><p>-&gt; <strong>(3,640,640)</strong></p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>image <span style="color:#f92672">=</span> rearrange(unfold(image), <span style="color:#e6db74">&#34;(C H W) L-&gt; L C H W&#34;</span>, C<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, H<span style="color:#f92672">=</span>kernel)
</span></span></code></pre></div><p>-&gt; <strong>(442368(3x384x384(kernel size)),4(number of such patch))</strong> -&gt; <strong>(4,3,384,384)</strong></p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>global_image <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>                             size<span style="color:#f92672">=</span>(kernel, kernel),
</span></span><span style="display:flex;"><span>                             mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>,
</span></span><span style="display:flex;"><span>                             align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((image, global_image), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>-&gt; <strong>(5,3,384,384)</strong> 与下面呼应！</p>
</li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backbone(images) <span style="color:#75715e"># features: a dictionary with length of 3</span>
</span></span><span style="display:flex;"><span>clip_features <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sem_seg_head<span style="color:#f92672">.</span>predictor<span style="color:#f92672">.</span>clip_model<span style="color:#f92672">.</span>encode_image(clip_images, dense<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#75715e"># clip_images: (5, 3, 336, 336)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># outputs: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sem_seg_head(clip_features, features)
</span></span></code></pre></div><ul>
<li><code>features</code>:
![image-20240401003703609](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401003703609.png)</li>
<li><code>clip_features</code>: <strong>(5,577(24x24+1),768)</strong></li>
<li><code>outputs</code>: <strong>(5,150(number of classes),96,96)</strong></li>
</ul>
</li>
<li>
<p>After the three steps: <code>outputs</code> -&gt; <strong>(5,150,96,96)</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># outputs: (5,150,96,96) -&gt; (5,150,384,384)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(outputs,
</span></span><span style="display:flex;"><span>                        size<span style="color:#f92672">=</span>kernel,
</span></span><span style="display:flex;"><span>                        mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bilinear&#34;</span>,
</span></span><span style="display:flex;"><span>                        align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)   <span style="color:#75715e"># -&gt; (5,150,384,384) 与上面呼应！</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>sigmoid()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>global_output <span style="color:#f92672">=</span> outputs[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>global_output <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(global_output,
</span></span><span style="display:flex;"><span>                              size<span style="color:#f92672">=</span>out_res,
</span></span><span style="display:flex;"><span>                              mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>,
</span></span><span style="display:flex;"><span>                              align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,)
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> outputs[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]												 <span style="color:#75715e"># -&gt; (4,150,384,384)</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> fold(outputs<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T) <span style="color:#f92672">/</span> fold(unfold(torch<span style="color:#f92672">.</span>ones([<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> out_res, device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device)))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fenzi: (4,22118400) -&gt; (22118400,4) -&gt; (150,640,640)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># fenmu: (1,640,640)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This steps normalize the effects brought by the fold operation.</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> (outputs <span style="color:#f92672">+</span> global_output) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -&gt; (1,150,640,640)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>height <span style="color:#f92672">=</span> batched_inputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;height&#34;</span>, out_res[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>width <span style="color:#f92672">=</span> batched_inputs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;width&#34;</span>, out_res[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> sem_seg_postprocess(outputs[<span style="color:#ae81ff">0</span>], out_res, height, width)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -&gt; (150,512,683)</span>
</span></span></code></pre></div></li>
</ul>
<h3 id="the-workflow-within-three-main-steps">The workflow within three main steps<a hidden class="anchor" aria-hidden="true" href="#the-workflow-within-three-main-steps">#</a></h3>
</li>
<li>
<p>The workflow within <code>features = self.backbone(images) # features: a dictionary with length of 3</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">D2SwinTransformer</span>(SwinTransformer, Backbone):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x): <span style="color:#75715e"># x -&gt; (5,3,384,384)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            dict[str-&gt;Tensor]: names and the corresponding features
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> (
</span></span><span style="display:flex;"><span>            x<span style="color:#f92672">.</span>dim() <span style="color:#f92672">==</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>        ), <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;SwinTransformer takes an input of shape (N, C, H, W). Got </span><span style="color:#e6db74">{</span>x<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74"> instead!&#34;</span>
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> super()<span style="color:#f92672">.</span>forward(x) <span style="color:#75715e"># y -&gt; a dict of three tensors: {(5,128,96,96), (5,256,48,48), (5,512,24,24)}. Same for shape of the outputs</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> y<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> k <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>_out_features:
</span></span><span style="display:flex;"><span>                outputs[k] <span style="color:#f92672">=</span> y[k]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> outputs
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>patch_embed(x) <span style="color:#75715e"># (5,3,384,384) -&gt; (5,128,96,96) 解释在下面</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        Wh, Ww <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">2</span>), x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>ape:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># interpolate the position embedding to the corresponding size</span>
</span></span><span style="display:flex;"><span>            absolute_pos_embed <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>absolute_pos_embed, size<span style="color:#f92672">=</span>(Wh, Ww), mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bicubic&#34;</span>
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> (x <span style="color:#f92672">+</span> absolute_pos_embed)<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># B Wh*Ww C</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># -&gt; (5,9216(96x96),128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pos_drop(x) <span style="color:#75715e"># no change (5,9216,128)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        outs <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>num_layers):
</span></span><span style="display:flex;"><span>            layer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layers[i]
</span></span><span style="display:flex;"><span>            x_out, H, W, x, Wh, Ww <span style="color:#f92672">=</span> layer(x, Wh, Ww) <span style="color:#75715e"># x_out -&gt; (5,9216,128)/(5,2304,256)/(5,576,256)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>out_indices:
</span></span><span style="display:flex;"><span>                norm_layer <span style="color:#f92672">=</span> getattr(self, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;norm</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>                x_out <span style="color:#f92672">=</span> norm_layer(x_out) <span style="color:#75715e"># no change (5,9216,128)/(5,2304,256)/(5,576,512)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>                out <span style="color:#f92672">=</span> x_out<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, H, W, self<span style="color:#f92672">.</span>num_features[i])<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous() <span style="color:#75715e"># out: (5,128,96,96)/(5,256,48,48)/(5,512,24,24)</span>
</span></span><span style="display:flex;"><span>                outs[<span style="color:#e6db74">&#34;res</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>)] <span style="color:#f92672">=</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> outs
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PatchEmbed</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># padding</span>
</span></span><span style="display:flex;"><span>        _, _, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> W <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>pad(x, (<span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">-</span> W <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> H <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>pad(x, (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">-</span> H <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>patch_size[<span style="color:#ae81ff">0</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proj(x)  <span style="color:#75715e"># B C Wh Ww (5,3,384,384) -&gt; (5,128,96,96)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>norm <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            Wh, Ww <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">2</span>), x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm(x)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>embed_dim, Wh, Ww)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#75715e"># (5,128,96,96) 传回上面👆🏻</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>ModuleList(
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">0</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">384</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.014</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (downsample): PatchMerging(
</span></span><span style="display:flex;"><span>      (reduction): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>      (norm): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">1</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.029</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">768</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.043</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">256</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (downsample): PatchMerging(
</span></span><span style="display:flex;"><span>      (reduction): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>      (norm): LayerNorm((<span style="color:#ae81ff">1024</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">2</span>): BasicLayer(
</span></span><span style="display:flex;"><span>    (blocks): ModuleList(
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">0</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.057</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">1</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.071</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">2</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.086</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">3</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.100</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">4</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.114</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">5</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.129</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">6</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.143</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">7</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.157</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">8</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.171</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">9</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.186</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">10</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.200</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">11</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.214</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">12</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.229</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">13</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.243</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">14</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.257</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">15</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.271</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">16</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.286</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (<span style="color:#ae81ff">17</span>): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (qkv): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): DropPath(drop_prob<span style="color:#f92672">=</span><span style="color:#ae81ff">0.300</span>)
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">512</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BasicLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, H, W):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: Input feature, tensor size (B, H*W, C).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            H, W: Spatial resolution of the input feature.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># calculate attention mask for SW-MSA</span>
</span></span><span style="display:flex;"><span>        Hp <span style="color:#f92672">=</span> int(np<span style="color:#f92672">.</span>ceil(H <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>window_size)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        Wp <span style="color:#f92672">=</span> int(np<span style="color:#f92672">.</span>ceil(W <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>window_size)) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        img_mask <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, Hp, Wp, <span style="color:#ae81ff">1</span>), device<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>device)  <span style="color:#75715e"># 1 Hp Wp 1</span>
</span></span><span style="display:flex;"><span>        h_slices <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            slice(<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        w_slices <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            slice(<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>window_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size),
</span></span><span style="display:flex;"><span>            slice(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#66d9ef">None</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        cnt <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> h <span style="color:#f92672">in</span> h_slices:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> w_slices:
</span></span><span style="display:flex;"><span>                img_mask[:, h, w, :] <span style="color:#f92672">=</span> cnt
</span></span><span style="display:flex;"><span>                cnt <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mask_windows <span style="color:#f92672">=</span> window_partition(
</span></span><span style="display:flex;"><span>            img_mask, self<span style="color:#f92672">.</span>window_size
</span></span><span style="display:flex;"><span>        )  <span style="color:#75715e"># nW, window_size, window_size, 1</span>
</span></span><span style="display:flex;"><span>        mask_windows <span style="color:#f92672">=</span> mask_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size)
</span></span><span style="display:flex;"><span>        attn_mask <span style="color:#f92672">=</span> mask_windows<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> mask_windows<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        attn_mask <span style="color:#f92672">=</span> attn_mask<span style="color:#f92672">.</span>masked_fill(attn_mask <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#f92672">-</span><span style="color:#ae81ff">100.0</span>))<span style="color:#f92672">.</span>masked_fill(
</span></span><span style="display:flex;"><span>            attn_mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> blk <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>blocks:
</span></span><span style="display:flex;"><span>            blk<span style="color:#f92672">.</span>H, blk<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> H, W
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>use_checkpoint:
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> checkpoint<span style="color:#f92672">.</span>checkpoint(blk, x, attn_mask)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> blk(x, attn_mask) <span style="color:#75715e"># (5,9216,128) -&gt; (5,9216,128)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>downsample <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            x_down <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>downsample(x, H, W)
</span></span><span style="display:flex;"><span>            Wh, Ww <span style="color:#f92672">=</span> (H <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>, (W <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> x, H, W, x_down, Wh, Ww
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> x, H, W, x, H, W
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>The workflow within <code>clip_features = self.sem_seg_head.predictor.clip_model.encode_image(clip_images, dense=True) # clip_images: (5, 3, 336, 336), clip_features: (5, 577, 768)</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">VisualTransformer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x: torch<span style="color:#f92672">.</span>Tensor, dense<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># (5,3,336,336)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(x)  <span style="color:#75715e"># shape = [5, 1024, 24, 24]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reshape(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 1024, 576(24x24)]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 576, 1024]</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([self<span style="color:#f92672">.</span>class_embedding<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype) <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>zeros(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, x<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], dtype<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>dtype, device<span style="color:#f92672">=</span>x<span style="color:#f92672">.</span>device), x], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape = [5, 576+1, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> dense <span style="color:#f92672">and</span> (x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">!=</span> self<span style="color:#f92672">.</span>positional_embedding<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>resized_pos_embed(self<span style="color:#f92672">.</span>input_resolution, x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>positional_embedding<span style="color:#f92672">.</span>to(x<span style="color:#f92672">.</span>dtype) <span style="color:#75715e"># shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_pre(x) <span style="color:#75715e"># shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># NLD -&gt; LND</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(x, dense)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># LND -&gt; NLD shape = [5, 577, 1024]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> dense:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_post(x[:, :, :])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln_post(x[:, <span style="color:#ae81ff">0</span>, :])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>proj <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>proj <span style="color:#75715e"># shape -&gt; [5, 577, 768]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#75715e"># shape = [5, 577, 768]</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>The workflow within <code>outputs = self.sem_seg_head(clip_features, features)</code>:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CATSegHead</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, features, guidance_features):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            img_feats: (B, C, HW)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            affinity_features: (B, C, )
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># features: (5,577,768) -&gt; (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        img_feat <span style="color:#f92672">=</span> rearrange(features[:, <span style="color:#ae81ff">1</span>:, :], <span style="color:#e6db74">&#34;b (h w) c-&gt;b c h w&#34;</span>, h<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>feature_resolution[<span style="color:#ae81ff">0</span>], w<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>feature_resolution[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>predictor(img_feat, guidance_features)
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CATSegPredictor</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>  	<span style="color:#75715e"># self.transformer -&gt; Aggregator!</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, vis_guidance):
</span></span><span style="display:flex;"><span>        vis <span style="color:#f92672">=</span> [vis_guidance[k] <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> vis_guidance<span style="color:#f92672">.</span>keys()][::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text: (150, 80, 768)</span>
</span></span><span style="display:flex;"><span>        text <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>text_features <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>training <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>text_features_test
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text -&gt; (5, 150, 80, 768)</span>
</span></span><span style="display:flex;"><span>        text <span style="color:#f92672">=</span> text<span style="color:#f92672">.</span>repeat(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer(x, text, vis) <span style="color:#75715e"># This Aggregator part: below👇🏻</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div></li>
<li>
<p><code>text_feats</code>: <strong>(5,150,80,768)</strong>, <code>img_feats</code>: <strong>(5,768,24,24)</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Aggregator</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,
</span></span><span style="display:flex;"><span>        text_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>        text_guidance_proj_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        appearance_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>        appearance_guidance_proj_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        decoder_dims <span style="color:#f92672">=</span> (<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>),
</span></span><span style="display:flex;"><span>        decoder_guidance_dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>),
</span></span><span style="display:flex;"><span>        decoder_guidance_proj_dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">16</span>),
</span></span><span style="display:flex;"><span>        num_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>        nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>        hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>        pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>),
</span></span><span style="display:flex;"><span>        feature_resolution<span style="color:#f92672">=</span>(<span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">24</span>),
</span></span><span style="display:flex;"><span>        window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
</span></span><span style="display:flex;"><span>        attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>,
</span></span><span style="display:flex;"><span>        prompt_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span>,
</span></span><span style="display:flex;"><span>    ) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_layers <span style="color:#f92672">=</span> num_layers
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hidden_dim <span style="color:#f92672">=</span> hidden_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            AggregatorLayer(
</span></span><span style="display:flex;"><span>                hidden_dim<span style="color:#f92672">=</span>hidden_dim, text_guidance_dim<span style="color:#f92672">=</span>text_guidance_proj_dim, appearance_guidance<span style="color:#f92672">=</span>appearance_guidance_proj_dim,
</span></span><span style="display:flex;"><span>                nheads<span style="color:#f92672">=</span>nheads, input_resolution<span style="color:#f92672">=</span>feature_resolution, pooling_size<span style="color:#f92672">=</span>pooling_size, window_size<span style="color:#f92672">=</span>window_size, attention_type<span style="color:#f92672">=</span>attention_type
</span></span><span style="display:flex;"><span>            ) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_layers)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(prompt_channel, hidden_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(appearance_guidance_dim, appearance_guidance_proj_dim, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(text_guidance_dim, text_guidance_proj_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>        ) <span style="color:#66d9ef">if</span> text_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>Conv2d(d, dp, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>                nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            ) <span style="color:#66d9ef">for</span> d, dp <span style="color:#f92672">in</span> zip(decoder_guidance_dims, decoder_guidance_proj_dims)
</span></span><span style="display:flex;"><span>        ]) <span style="color:#66d9ef">if</span> decoder_guidance_dims[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder1 <span style="color:#f92672">=</span> Up(hidden_dim, decoder_dims[<span style="color:#ae81ff">0</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder2 <span style="color:#f92672">=</span> Up(decoder_dims[<span style="color:#ae81ff">0</span>], decoder_dims[<span style="color:#ae81ff">1</span>], decoder_guidance_proj_dims[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(decoder_dims[<span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#---------------------------------------------------------------------------------#</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">feature_map</span>(self, img_feats, text_feats):
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(img_feats, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B C H W</span>
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> repeat(img_feats, <span style="color:#e6db74">&#34;B C H W -&gt; B C T H W&#34;</span>, T<span style="color:#f92672">=</span>text_feats<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B T P C</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> text_feats<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B T C</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> repeat(text_feats, <span style="color:#e6db74">&#34;B T C -&gt; B C T H W&#34;</span>, H<span style="color:#f92672">=</span>img_feats<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>], W<span style="color:#f92672">=</span>img_feats<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat((img_feats, text_feats), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># B 2C T H W</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">correlation</span>(self, img_feats, text_feats):
</span></span><span style="display:flex;"><span>        img_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(img_feats, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        text_feats <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(text_feats, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># (5,150,80,768)</span>
</span></span><span style="display:flex;"><span>        corr <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;bchw, btpc -&gt; bpthw&#39;</span>, img_feats, text_feats)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr <span style="color:#75715e"># corr: (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">corr_embed</span>(self, x):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,80,150,24,24) -&gt; (750, 80, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B P T H W -&gt; (B T) P H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 80, 24, 24) -&gt; (750, 128, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv1(corr_embed)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 128, 24, 24) -&gt; (5, 128, 150, 24, 24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">corr_projection</span>(self, x, proj):
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; B T H W C&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> proj(corr_embed)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;B T H W C -&gt; B C T H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">upsample</span>(self, x):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(corr_embed, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv_decoder</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder1(corr_embed, guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder2(corr_embed, guidance[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>head(corr_embed)
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) () H W -&gt; B T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, img_feats, text_feats, appearance_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            img_feats: (B, C, H, W)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            text_feats: (B, T, P, C)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            apperance_guidance: tuple of (B, C, H, W)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># text_feats: (5,150,80,768), img_feats: (5,768,24,24)</span>
</span></span><span style="display:flex;"><span>        corr <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>correlation(img_feats, text_feats) <span style="color:#75715e"># corr: (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>corr_embed(corr)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        projected_guidance, projected_text_guidance, projected_decoder_guidance <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>, [<span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># projected_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>            projected_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>guidance_projection(appearance_guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>decoder_guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># 见下图👇🏻</span>
</span></span><span style="display:flex;"><span>            projected_decoder_guidance <span style="color:#f92672">=</span> [proj(g) <span style="color:#66d9ef">for</span> proj, g <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>decoder_guidance_projection, appearance_guidance[<span style="color:#ae81ff">1</span>:])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>text_guidance_projection <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># (5,150,80,768) -&gt; (5,150,768)</span>
</span></span><span style="display:flex;"><span>            text_feats <span style="color:#f92672">=</span> text_feats<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>            text_feats <span style="color:#f92672">=</span> text_feats <span style="color:#f92672">/</span> text_feats<span style="color:#f92672">.</span>norm(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># (5,150,768) -&gt; (5,150,128)</span>
</span></span><span style="display:flex;"><span>            projected_text_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>text_guidance_projection(text_feats)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (5,80,150,24,24) -&gt; (5,80,150,24,24) -&gt; (5,80,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>            corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance,
</span></span><span style="display:flex;"><span>                               projected_text_guidance)
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># corr_embed: (5,80,150,24,24), 见最下面👇🏻</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># logit: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>        logit <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_decoder(corr_embed, projected_decoder_guidance)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logit
</span></span></code></pre></div><p>![image-20240401182112928](/Users/biboyqg/Library/Application Support/typora-user-images/image-20240401182112928.png)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>		corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance, projected_text_guidance):
</span></span></code></pre></div><p>with <code>self.layers</code>&rsquo;s structure as follow:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ModuleList(
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">0</span>): AggregatorLayer(
</span></span><span style="display:flex;"><span>    (swin_block): SwinTransformerBlockWrapper(
</span></span><span style="display:flex;"><span>      (block_1): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (block_2): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (guidance_norm): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (attention): ClassTransformerLayer(
</span></span><span style="display:flex;"><span>      (pool): AvgPool2d(kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], stride<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>      (attention): AttentionLayer(
</span></span><span style="display:flex;"><span>        (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attention): LinearAttention()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (MLP): Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): ReLU()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>  (<span style="color:#ae81ff">1</span>): AggregatorLayer(
</span></span><span style="display:flex;"><span>    (swin_block): SwinTransformerBlockWrapper(
</span></span><span style="display:flex;"><span>      (block_1): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (block_2): SwinTransformerBlock(
</span></span><span style="display:flex;"><span>        (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attn): WindowAttention(
</span></span><span style="display:flex;"><span>          (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (attn_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (proj): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (proj_drop): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (softmax): Softmax(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        (drop_path): Identity()
</span></span><span style="display:flex;"><span>        (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (mlp): Mlp(
</span></span><span style="display:flex;"><span>          (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (act): GELU(approximate<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>)
</span></span><span style="display:flex;"><span>          (drop1): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>          (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>          (drop2): Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (guidance_norm): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    (attention): ClassTransformerLayer(
</span></span><span style="display:flex;"><span>      (pool): AvgPool2d(kernel_size<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], stride<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>      (attention): AttentionLayer(
</span></span><span style="display:flex;"><span>        (q): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (k): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (v): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (attention): LinearAttention()
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (MLP): Sequential(
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">0</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">1</span>): ReLU()
</span></span><span style="display:flex;"><span>        (<span style="color:#ae81ff">2</span>): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      )
</span></span><span style="display:flex;"><span>      (norm1): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      (norm2): LayerNorm((<span style="color:#ae81ff">128</span>,), eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-05</span>, elementwise_affine<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>and data flow in each <code>AggregatorLayer</code> is as follow (the change of shape is the same for two layers):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
</span></span><span style="display:flex;"><span>  	<span style="color:#75715e"># corr_embed: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># projected_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># projected_text_guidance: (5,150,128)</span>
</span></span><span style="display:flex;"><span>		corr_embed <span style="color:#f92672">=</span> layer(corr_embed, projected_guidance, projected_text_guidance)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AggregatorLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, text_guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>                 appearance_guidance<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, input_resolution<span style="color:#f92672">=</span>(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">20</span>),
</span></span><span style="display:flex;"><span>                 pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), window_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>),
</span></span><span style="display:flex;"><span>                 attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>swin_block <span style="color:#f92672">=</span> SwinTransformerBlockWrapper(hidden_dim,
</span></span><span style="display:flex;"><span>                                                      appearance_guidance,
</span></span><span style="display:flex;"><span>                                                      input_resolution, nheads,
</span></span><span style="display:flex;"><span>                                                      window_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> ClassTransformerLayer(hidden_dim,
</span></span><span style="display:flex;"><span>                                               text_guidance_dim,
</span></span><span style="display:flex;"><span>                                               nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                               attention_type<span style="color:#f92672">=</span>attention_type,
</span></span><span style="display:flex;"><span>                                               pooling_size<span style="color:#f92672">=</span>pooling_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance, text_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B C T H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>    		<span style="color:#75715e"># appearance_guidance: (5,128,24,24)</span>
</span></span><span style="display:flex;"><span>    		<span style="color:#75715e"># text_guidance: (5,150,128)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>swin_block(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attention(x, text_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>SwinTransformerBlockWrapper</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlockWrapper</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                 input_resolution, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, window_size<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_1 <span style="color:#f92672">=</span> SwinTransformerBlock(dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution, num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>block_2 <span style="color:#f92672">=</span> SwinTransformerBlock(dim, appearance_guidance_dim,
</span></span><span style="display:flex;"><span>                                            input_resolution, num_heads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                            head_dim<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, window_size<span style="color:#f92672">=</span>window_size,
</span></span><span style="display:flex;"><span>                                            shift_size<span style="color:#f92672">=</span>window_size <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>guidance_norm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(appearance_guidance_dim) <span style="color:#66d9ef">if</span> appearance_guidance_dim <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span> <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B C T H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            appearance_guidance: B C H W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B, C, T, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) (H W) C&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> appearance_guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># appearance_guidance: (5,128,24,24) -&gt; (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>            appearance_guidance <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>guidance_norm(repeat(appearance_guidance, <span style="color:#e6db74">&#39;B C H W -&gt; (B T) (H W) C&#39;</span>, T<span style="color:#f92672">=</span>T))
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>block_1(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (750,576,128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>block_2(x, appearance_guidance)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128) -&gt; (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;(B T) (H W) C -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B, T<span style="color:#f92672">=</span>T, H<span style="color:#f92672">=</span>H, W<span style="color:#f92672">=</span>W)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>In <code>SwinTransformerBlock</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SwinTransformerBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, appearance_guidance):
</span></span><span style="display:flex;"><span>        H, W <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>input_resolution
</span></span><span style="display:flex;"><span>        B, L, C <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> L <span style="color:#f92672">==</span> H <span style="color:#f92672">*</span> W, <span style="color:#e6db74">&#34;input feature has wrong size&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        shortcut <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>norm1(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750, 576, 128) -&gt; (750, 24, 24, 128)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(B, H, W, C)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> appearance_guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># appearance_guidance: (750, 576, 128) -&gt; (750, 24, 24, 128)</span>
</span></span><span style="display:flex;"><span>            appearance_guidance <span style="color:#f92672">=</span> appearance_guidance<span style="color:#f92672">.</span>view(B, H, W, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750, 24, 24, 128) -&gt; (750, 24, 24, 256)</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, appearance_guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># cyclic shift</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>shift_size <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            shifted_x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(x, shifts<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size, <span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>shift_size), dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            shifted_x <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># partition windows</span>
</span></span><span style="display:flex;"><span>        x_windows <span style="color:#f92672">=</span> window_partition(shifted_x, self<span style="color:#f92672">.</span>window_size)  <span style="color:#75715e"># num_win*B, window_size, window_size, C</span>
</span></span><span style="display:flex;"><span>        x_windows <span style="color:#f92672">=</span> x_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>window_size, x_windows<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])  <span style="color:#75715e"># num_win*B, window_size*window_size, C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># W-MSA/SW-MSA</span>
</span></span><span style="display:flex;"><span>        attn_windows <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn(x_windows, mask<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>attn_mask)  <span style="color:#75715e"># num_win*B, window_size*window_size, C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># merge windows</span>
</span></span><span style="display:flex;"><span>        attn_windows <span style="color:#f92672">=</span> attn_windows<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>window_size, self<span style="color:#f92672">.</span>window_size, C)
</span></span><span style="display:flex;"><span>        shifted_x <span style="color:#f92672">=</span> window_reverse(attn_windows, self<span style="color:#f92672">.</span>window_size, H, W)  <span style="color:#75715e"># B H&#39; W&#39; C</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># reverse cyclic shift</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>shift_size <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(shifted_x, shifts<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>shift_size, self<span style="color:#f92672">.</span>shift_size), dims<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> shifted_x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(B, H <span style="color:#f92672">*</span> W, C)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># FFN</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> shortcut <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>drop_path(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>drop_path(self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>norm2(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (750,576,128)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>self.attention = ClassTransformerLayer</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ClassTransformerLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, guidance_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>                 attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, pooling_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(pooling_size)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> AttentionLayer(hidden_dim, guidance_dim,
</span></span><span style="display:flex;"><span>                                        nheads<span style="color:#f92672">=</span>nheads,
</span></span><span style="display:flex;"><span>                                        attention_type<span style="color:#f92672">=</span>attention_type)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>MLP <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, hidden_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>norm2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LayerNorm(hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pool_features</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Intermediate pooling layer for computational efficiency.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, C, T, H, W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, C, T, H, W
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            guidance: B, T, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        B, _, _, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool_features(x)
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">*</span>_, H_pool, W_pool <span style="color:#f92672">=</span> x_pool<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (5,128,150,24,24) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;B C T H W -&gt; (B H W) T C&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>          	<span style="color:#75715e"># guidance: (5,150,128) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#39;B T C -&gt; (B H W) T C&#39;</span>, H<span style="color:#f92672">=</span>H_pool, W<span style="color:#f92672">=</span>W_pool)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> x_pool <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>attention(self<span style="color:#f92672">.</span>norm1(x_pool), guidance) <span style="color:#75715e"># 见下面👇🏻</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> x_pool <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>MLP(self<span style="color:#f92672">.</span>norm2(x_pool)) <span style="color:#75715e"># MLP</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;(B H W) T C -&gt; (B T) C H W&#39;</span>, H<span style="color:#f92672">=</span>H_pool, W<span style="color:#f92672">=</span>W_pool)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x_pool: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(x_pool, size<span style="color:#f92672">=</span>(H, W), mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        x_pool <span style="color:#f92672">=</span> rearrange(x_pool, <span style="color:#e6db74">&#39;(B T) C H W -&gt; B C T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: (5,128,150,24,24)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> x_pool <span style="color:#75715e"># Residual</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>For <code>self.attention(self.norm1(x_pool), guidance)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AttentionLayer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, hidden_dim, guidance_dim, nheads<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, attention_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>nheads <span style="color:#f92672">=</span> nheads
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim <span style="color:#f92672">+</span> guidance_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;linear&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> LinearAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> attention_type <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;full&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>attention <span style="color:#f92672">=</span> FullAttention()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">NotImplementedError</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Arguments:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            x: B, L, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            guidance: B, L, C
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># q,k,v: (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>q(torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>q(x)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>k(torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span> <span style="color:#66d9ef">else</span> self<span style="color:#f92672">.</span>k(x)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>v(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># q,k,v: (2880,150,4,32)</span>
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> rearrange(q, <span style="color:#e6db74">&#39;B L (H D) -&gt; B L H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> rearrange(k, <span style="color:#e6db74">&#39;B S (H D) -&gt; B S H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> rearrange(v, <span style="color:#e6db74">&#39;B S (H D) -&gt; B S H D&#39;</span>, H<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>nheads)
</span></span><span style="display:flex;"><span>				<span style="color:#75715e"># out: (2880,150,4,32)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attention(q, k, v)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># out: (2880,150,4,32) -&gt; (2880,150,128)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> rearrange(out, <span style="color:#e6db74">&#39;B L H D -&gt; B L (H D)&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><p>For <code>self.conv_decoder(corr_embed, projected_decoder_guidance)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">conv_decoder</span>(self, x, guidance):
</span></span><span style="display:flex;"><span>        B <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>      	<span style="color:#75715e"># corr_embed: (750,128,24,24)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(x, <span style="color:#e6db74">&#39;B C T H W -&gt; (B T) C H W&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,64,48,48)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder1(corr_embed, guidance[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,32,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder2(corr_embed, guidance[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (750,1,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>head(corr_embed)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># corr_embed: (5,150,96,96)</span>
</span></span><span style="display:flex;"><span>        corr_embed <span style="color:#f92672">=</span> rearrange(corr_embed, <span style="color:#e6db74">&#39;(B T) () H W -&gt; B T H W&#39;</span>, B<span style="color:#f92672">=</span>B)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> corr_embed
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Up</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channels, out_channels, guidance_channels):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>up <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels,
</span></span><span style="display:flex;"><span>                                     in_channels <span style="color:#f92672">-</span> guidance_channels,
</span></span><span style="display:flex;"><span>                                     kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, guidance<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>      	<span style="color:#75715e"># x: (750,128,24,24) -&gt; (750,96,48,48)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> guidance <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            T <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>) <span style="color:#f92672">//</span> guidance<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># guidance: (5,32,48,48) -&gt; (750,32,48,48)</span>
</span></span><span style="display:flex;"><span>            guidance <span style="color:#f92672">=</span> repeat(guidance, <span style="color:#e6db74">&#34;B C H W -&gt; (B T) C H W&#34;</span>, T<span style="color:#f92672">=</span>T)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750,96,48,48) -&gt; (750,128,48,48)</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, guidance], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># x: (750,128,48,48) -&gt; (750,64,48,48)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>conv(x)
</span></span></code></pre></div></li>
</ul>
</li>
</ul>
<h3 id="2-unknown-stuff">2. Unknown stuff<a hidden class="anchor" aria-hidden="true" href="#2-unknown-stuff">#</a></h3>
<ul>
<li>Loss computation</li>
<li>GroupNorm</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://biboyqg.github.io/blog/tags/open-vocabulary-segmentation/">Open Vocabulary Segmentation</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/clip/">CLIP</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/swin-transformer/">Swin Transformer</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://biboyqg.github.io/blog/blog/log/">
    <span class="title">« Prev</span>
    <br>
    <span>Daily Log</span>
  </a>
  <a class="next" href="https://biboyqg.github.io/blog/blog/argparse/">
    <span class="title">Next »</span>
    <br>
    <span>Argparse: a user-friendly tool to write CLI interface</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    <footer class="footer">
  <span
    >&copy; 2025
    <a href="https://biboyqg.github.io/blog/">Banghao&#39;s Blog</a></span
  >
</footer>
</body>

</html>
