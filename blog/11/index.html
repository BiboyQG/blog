<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Meeting Discussion (11) | Banghao&#39;s Blog</title>
<meta name="keywords" content="meeting-discussions, research, Quantization">
<meta name="description" content="1. Table of Contents The feasibility of quantization and SQ on SparseConv3d: ✅ The discussion of sparse conv3d: Find where the computation for sparse conv3d is done: ✅ Inside ops.implicit_gemm: ✅ How can we implement it: Quantization of the activation and weight: ✅ Application of SQ during quantization process: ✅ Evaluation of current method: Whether SQ is effective or not depends on the value of the inputs and weight: ✅ Actual implementation: In progress… 2.">
<meta name="author" content="Banghao Chi">
<link rel="canonical" href="https://banghao.live/blog/11/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8d6d0999b7a3d50c6d5a00541fc8078c4695a82720ad160b4078dab1d4edf114.css" integrity="sha256-jW0Jmbej1QxtWgBUH8gHjEaVqCcgrRYLQHjasdTt8RQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://banghao.live/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://banghao.live/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://banghao.live/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://banghao.live/apple-touch-icon.png">
<link rel="mask-icon" href="https://banghao.live/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://banghao.live/blog/11/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Meeting Discussion (11)" />
<meta property="og:description" content="1. Table of Contents The feasibility of quantization and SQ on SparseConv3d: ✅ The discussion of sparse conv3d: Find where the computation for sparse conv3d is done: ✅ Inside ops.implicit_gemm: ✅ How can we implement it: Quantization of the activation and weight: ✅ Application of SQ during quantization process: ✅ Evaluation of current method: Whether SQ is effective or not depends on the value of the inputs and weight: ✅ Actual implementation: In progress… 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://banghao.live/blog/11/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-06-16T18:48:25+08:00" />
<meta property="article:modified_time" content="2024-06-16T18:48:25+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Meeting Discussion (11)"/>
<meta name="twitter:description" content="1. Table of Contents The feasibility of quantization and SQ on SparseConv3d: ✅ The discussion of sparse conv3d: Find where the computation for sparse conv3d is done: ✅ Inside ops.implicit_gemm: ✅ How can we implement it: Quantization of the activation and weight: ✅ Application of SQ during quantization process: ✅ Evaluation of current method: Whether SQ is effective or not depends on the value of the inputs and weight: ✅ Actual implementation: In progress… 2."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://banghao.live/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Meeting Discussion (11)",
      "item": "https://banghao.live/blog/11/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Meeting Discussion (11)",
  "name": "Meeting Discussion (11)",
  "description": "1. Table of Contents The feasibility of quantization and SQ on SparseConv3d: ✅ The discussion of sparse conv3d: Find where the computation for sparse conv3d is done: ✅ Inside ops.implicit_gemm: ✅ How can we implement it: Quantization of the activation and weight: ✅ Application of SQ during quantization process: ✅ Evaluation of current method: Whether SQ is effective or not depends on the value of the inputs and weight: ✅ Actual implementation: In progress… 2.",
  "keywords": [
    "meeting-discussions", "research", "Quantization"
  ],
  "articleBody": "1. Table of Contents The feasibility of quantization and SQ on SparseConv3d: ✅ The discussion of sparse conv3d: Find where the computation for sparse conv3d is done: ✅ Inside ops.implicit_gemm: ✅ How can we implement it: Quantization of the activation and weight: ✅ Application of SQ during quantization process: ✅ Evaluation of current method: Whether SQ is effective or not depends on the value of the inputs and weight: ✅ Actual implementation: In progress… 2.1 Find where the computation for sparse conv3d is done. Firstly I went through the source code of spconv (except for CUDA kernel part), and I found out how the computation for sparse conv3d is done:\nAfter some preprocessing, we got features and weight_cur.\nFrom the graph, I found that features and weight_cur are then passed into ops.implicit_gemm, so I then print them to find what type they are, which turns out that both of them are Tensor. I then print out the shape and the results are similar to the following:\nfeature.shape: torch.Size([124310, 5]) weight_cur.shape: torch.Size([16, 3, 3, 3, 5]) Here, what I think that makes it a little difficult is that the shape of the feature is indeed a 2d matrix, but the shape of the weight is just a standard shape for Conv3d, which makes the gemm process of feature and weight not that intuitive for me. So I then looked into ops.implicit_gemm to see if there’s any hope.\n2.2 Inside ops.implicit_gemm Inside ops.implicit_gemm, both features and weight are cast to a data type tensorview, which is commonly utilized in cumm, a python asyncio-based gemm simulator written by the creator of spconv package. After the transformation of the data type, both of them are passed into ConvGemmOps.implicit_gemm, which is as follow, utilizing pccm (which needs some c or cuda), another package written by the creator of spconv, to achieve the computation of the out_features (inside red box below, this picture is wrong, will change it in the future):\n3.1 Quantization of the activation and weight There’s actually an equivalent logic here if we want to quantize the activation and weight of the spconv3d layer under one specific hypothesis:\nIf the outcome of spconv3d and conv3d are the same.\nIt turns out that this is true, and therefore, we can start working on the quantization of both activation and weight:\nFor activation: Since we only apply per-tensor quantization to the activation, we just need to get the element that has the max absolute value of the whole activation tensor to apply quantization.\nFor weight: Since the weight for sparse 3d convolutional and normal 3d convolution is the same, per-channel quantization should be similar without difficulty.\n3.2 Application of SmoothQuant during quantization process If we want to apply SmoothQuant to our quantization as well, we need an extra hypothesis below:\nIf the computation of spconv3d can be transformed into img2col + gemm format.\nAnd it turns out that the img2col process is just similar to the normal 2d unfold process, but instead we unfold three times on three different dimensions.\nHence, we can actually first convert the input/activationof the spconv3d layer into dense format, transform it into a 2d matrix format using img2col and get the max list similar to the 2d process. The part for the weight is also similar, we unfold it into a 2d matrix format, and then get the max list. After getting two lists, we can compute the scale list and scale the magnitude of activation down while scaling up the weight to migrate the quantization difficulty from activation to weight.\n4.1 Evaluation of current method Here, the x-axis are the same random normal inputs with different scaling, while y-axis is the L1 Loss between the method that the line represents and the original Pytorch conv3d implementation.\nWe can get a conclusion from the graph, which is: whether SQ is effective or not depends on the value of the inputs and weight. We can also witness greater loss within the blue line, which means that further improvement for the current method is needed.\n4.2 Evaluation with different scaling factor Here, the x-axis are different random normal inputs with different scaling factor (from SmoothQuant), while y-axis is the L1 Loss between our method and original Pytorch conv3d implementation with specific example 10X inputs. From the graph, we may guess that our method is effectively dealing with the quantization difficulty from the activation.\n5. What’s next? Apply naive W8A8 quantization to Centerpoint: check the accuracy: If accuracy doesn’t drop much, maybe there’s no need of SmoothQuant. If drops much, focusing on how to apply SmoothQuant to spconv3d after quantization. Check the baseline of my method: Figure out why there’s greater loss along with the scaling of the example inputs without any quantization, and try to fix it. Check the activation outlier: After going through the above first two points, we can check the activation to see where the outliers are if they indeed exist. Expand on the data structure of SparseConvTensor on our next meeting with clearer explanation. ",
  "wordCount" : "836",
  "inLanguage": "en",
  "datePublished": "2024-06-16T18:48:25+08:00",
  "dateModified": "2024-06-16T18:48:25+08:00",
  "author":{
    "@type": "Person",
    "name": "Banghao Chi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://banghao.live/blog/11/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Banghao's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://banghao.live/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://banghao.live/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://banghao.live/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/" title="Home Page">
                    <span>Home Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Meeting Discussion (11)
    </h1>
    <div class="post-meta"><span title='2024-06-16 18:48:25 +0800 +0800'>June 16, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h5 id="1-table-of-contents">1. Table of Contents<a hidden class="anchor" aria-hidden="true" href="#1-table-of-contents">#</a></h5>
<ul>
<li>The feasibility of quantization and SQ on SparseConv3d: ✅</li>
<li>The discussion of sparse conv3d:
<ul>
<li>Find where the computation for sparse conv3d is done: ✅</li>
<li>Inside <code>ops.implicit_gemm</code>: ✅</li>
</ul>
</li>
<li>How can we implement it:
<ul>
<li>Quantization of the activation and weight: ✅</li>
<li>Application of SQ during quantization process: ✅</li>
</ul>
</li>
<li>Evaluation of current method:
<ul>
<li>Whether SQ is effective or not depends on the value of the inputs and weight: ✅</li>
</ul>
</li>
<li>Actual implementation: In progress…</li>
</ul>
<h5 id="21-find-where-the-computation-for-sparse-conv3d-is-done">2.1 Find where the computation for sparse conv3d is done.<a hidden class="anchor" aria-hidden="true" href="#21-find-where-the-computation-for-sparse-conv3d-is-done">#</a></h5>
<p>Firstly I went through the source code of <strong>spconv</strong> (except for CUDA kernel part), and I found out how the computation for sparse conv3d is done:</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/08/1ipARL3g5vB9s2I.png" alt="img"  />
</p>
<p>After some preprocessing, we got <code>features</code> and <code>weight_cur</code>.</p>
<p>From the graph, I found that <code>features</code> and <code>weight_cur</code> are then passed into <code>ops.implicit_gemm</code>, so I then print them to find what type they are, which turns out that both of them are <code>Tensor</code>. I then print out the shape and the results are similar to the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>feature.shape: torch.Size([124310, 5])
</span></span><span style="display:flex;"><span>weight_cur.shape: torch.Size([16, 3, 3, 3, 5])
</span></span></code></pre></div><p>Here, what I think that makes it a little difficult is that the shape of the feature is indeed a 2d matrix, but the shape of the weight is just a standard shape for Conv3d, which makes the gemm process of feature and weight not that intuitive for me. So I then looked into <code>ops.implicit_gemm</code> to see if there&rsquo;s any hope.</p>
<h5 id="22-inside-opsimplicit_gemm">2.2 Inside <code>ops.implicit_gemm</code><a hidden class="anchor" aria-hidden="true" href="#22-inside-opsimplicit_gemm">#</a></h5>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/08/Ul5OBcqaDbFWfLQ.png" alt="img"  />
</p>
<p>Inside <code>ops.implicit_gemm</code>, both features and weight are cast to a data type <code>tensorview</code>, which is commonly utilized in <code>cumm</code>, a python asyncio-based gemm simulator written by the creator of <code>spconv</code> package. After the transformation of the data type, both of them are passed into <code>ConvGemmOps.implicit_gemm</code>, which is as follow, utilizing <code>pccm</code> (which needs some c or cuda), another package written by the creator of <code>spconv</code>, to achieve the computation of the <code>out_features</code> (inside red box below, this picture is wrong, will change it in the future):</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/08/Cms8cVFWf7dPDpU.png" alt="img"  />
</p>
<h5 id="31-quantization-of-the-activation-and-weight">3.1 Quantization of the activation and weight<a hidden class="anchor" aria-hidden="true" href="#31-quantization-of-the-activation-and-weight">#</a></h5>
<p>There&rsquo;s actually an equivalent logic here if we want to quantize the activation and weight of the <code>spconv3d</code> layer under one specific hypothesis:</p>
<blockquote>
<p>If the outcome of <code>spconv3d</code> and <code>conv3d</code> are the same.</p>
</blockquote>
<p>It turns out that this is true, and therefore, we can start working on the quantization of both activation and weight:</p>
<ul>
<li>
<p><strong>For activation</strong>: Since we only apply per-tensor quantization to the activation, we just need to get the element that has the max absolute value of the whole activation tensor to apply quantization.</p>
</li>
<li>
<p><strong>For weight</strong>: Since the weight for sparse 3d convolutional and normal 3d convolution is the same, per-channel quantization should be similar without difficulty.</p>
</li>
</ul>
<h5 id="32-application-of-smoothquant-during-quantization-process">3.2 Application of SmoothQuant during quantization process<a hidden class="anchor" aria-hidden="true" href="#32-application-of-smoothquant-during-quantization-process">#</a></h5>
<p>If we want to apply SmoothQuant to our quantization as well, we need an extra hypothesis below:</p>
<blockquote>
<p>If the computation of <code>spconv3d</code> can be transformed into <strong>img2col + gemm</strong> format.</p>
</blockquote>
<p>And it turns out that the <code>img2col</code> process is just similar to the normal 2d unfold process, but instead we unfold three times on three different dimensions.</p>
<p>Hence, we can actually first convert the input/activationof the <code>spconv3d</code> layer into dense format, transform it into a 2d matrix format using <strong>img2col</strong> and get the max list similar to the 2d process. The part for the weight is also similar, we unfold it into a 2d matrix format, and then get the max list. After getting two lists, we can compute the scale list and scale the magnitude of activation down while scaling up the weight to migrate the quantization difficulty from activation to weight.</p>
<h5 id="41-evaluation-of-current-method">4.1 Evaluation of current method<a hidden class="anchor" aria-hidden="true" href="#41-evaluation-of-current-method">#</a></h5>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/16/Hg9IAkMiFoKZyxu.png" alt="L1loss_3d"  />
</p>
<p>Here, the x-axis are the same random normal inputs with different scaling, while y-axis is the L1 Loss between the method that the line represents and the original Pytorch conv3d implementation.</p>
<p>We can get a conclusion from the graph, which is: whether SQ is effective or not depends on the value of the inputs and weight. We can also witness greater loss within the blue line, which means that further improvement for the current method is needed.</p>
<h5 id="42-evaluation-with-different-scaling-factor">4.2 Evaluation with different scaling factor<a hidden class="anchor" aria-hidden="true" href="#42-evaluation-with-different-scaling-factor">#</a></h5>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/16/rnuO4vDxYQZGBqk.png" alt="scaling_3d"  />
</p>
<p>Here, the x-axis are different random normal inputs with different scaling factor (from SmoothQuant), while y-axis is the L1 Loss between our method and original Pytorch conv3d implementation with specific example 10X inputs. From the graph, we may guess that our method is effectively dealing with the quantization difficulty from the activation.</p>
<h5 id="5-whats-next">5. What&rsquo;s next?<a hidden class="anchor" aria-hidden="true" href="#5-whats-next">#</a></h5>
<ul>
<li>Apply <strong>naive W8A8</strong> quantization to <strong>Centerpoint</strong>:
<ul>
<li>check the accuracy:
<ul>
<li>If accuracy doesn’t drop much, maybe there’s no need of SmoothQuant.</li>
<li>If drops much, focusing on how to apply SmoothQuant to spconv3d after quantization.</li>
</ul>
</li>
</ul>
</li>
<li>Check the baseline of my method:
<ul>
<li>Figure out why there’s greater loss along with the scaling of the example inputs without any quantization, and try to fix it.</li>
</ul>
</li>
<li>Check the activation outlier:
<ul>
<li>After going through the above first two points, we can check the activation to see where the outliers are if they indeed exist.</li>
</ul>
</li>
<li>Expand on the <strong>data structure</strong> of <strong>SparseConvTensor</strong> on our next meeting with clearer explanation.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://banghao.live/tags/meeting-discussions/">Meeting-Discussions</a></li>
      <li><a href="https://banghao.live/tags/research/">Research</a></li>
      <li><a href="https://banghao.live/tags/quantization/">Quantization</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://banghao.live/blog/asag/">
    <span class="title">Next »</span>
    <br>
    <span>LLMarking</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://banghao.live/">Banghao&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
