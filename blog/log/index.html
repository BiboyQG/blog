<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Daily Log | Banghao&#39;s Blog</title>
<meta name="keywords" content="Deep Learning">
<meta name="description" content="3.12 Managed to understand the whole code base of the CLIP repo from OpenAI. Planned to take a look at CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.
3.13 1. DETR Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:
Long training period Difficulty of detecting small objects but has advantages in:">
<meta name="author" content="Banghao Chi">
<link rel="canonical" href="https://banghao.live/blog/log/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a4976a5c7c0315815eb87ad1fc547ff1a74ab11a2feef9cf71b3f484cdc2d82c.css" integrity="sha256-pJdqXHwDFYFeuHrR/FR/8adKsRov7vnPcbP0hM3C2Cw=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://banghao.live/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://banghao.live/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://banghao.live/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://banghao.live/apple-touch-icon.png">
<link rel="mask-icon" href="https://banghao.live/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://banghao.live/blog/log/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Daily Log" />
<meta property="og:description" content="3.12 Managed to understand the whole code base of the CLIP repo from OpenAI. Planned to take a look at CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.
3.13 1. DETR Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:
Long training period Difficulty of detecting small objects but has advantages in:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://banghao.live/blog/log/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-03-20T06:35:10-05:00" />
<meta property="article:modified_time" content="2024-03-20T06:35:10-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Daily Log"/>
<meta name="twitter:description" content="3.12 Managed to understand the whole code base of the CLIP repo from OpenAI. Planned to take a look at CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.
3.13 1. DETR Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:
Long training period Difficulty of detecting small objects but has advantages in:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://banghao.live/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Daily Log",
      "item": "https://banghao.live/blog/log/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Daily Log",
  "name": "Daily Log",
  "description": "3.12 Managed to understand the whole code base of the CLIP repo from OpenAI. Planned to take a look at CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.\n3.13 1. DETR Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:\nLong training period Difficulty of detecting small objects but has advantages in:",
  "keywords": [
    "Deep Learning"
  ],
  "articleBody": "3.12 Managed to understand the whole code base of the CLIP repo from OpenAI. Planned to take a look at CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.\n3.13 1. DETR Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:\nLong training period Difficulty of detecting small objects but has advantages in:\nUse object queiries to replace anchor generation Use Hangurian algorithm to replace NMS post-processing stage which basically replace things that are not learnable into learnable parameters.\nThe follow-up work: Deformable DETR, DINO, Omni-DETR, Up-DETR, PNP-DETR, SMAC-DETR, DAB-DETR, SAM-DETR, DN-, OW-, OV-, Pixel2Seq……\n2. Picture generation survey GAN: Merit: Pictures are real. Downside: Unstable training process (the only noise is the initial noise at the beginning of the training). The outputs lack diversity, meaning that they are close to the original pictures. AE (auto-encoder): DAE (Denoising auto-encoder): Added noise $X_c$ This paper proves that “Images have a high degree of redundancy”. This work is also similar to MAE.\n3. DALL·E 2 Diffusion model is actually a multi-layered VAE. 3.14 3D object detection 1. Datasets KITTY: the most classic dataset used in 3d obejct detection. Category Truncation Occlusion Alpha Bbox_X1 Bbox_Y1 Bbox_X2 Bbox_Y2 Dimensions_3D_Height Dimensions_3D_Width Dimensions_3D_Length Location_X Location_Y Location_Z Yaw Pedestrian 0.00 0 -0.20 712.40 143.00 810.73 307.92 1.89 0.48 1.20 1.84 1.47 8.41 0.01 Can use Open3D for further use.\nWaymo: a large dataset released by Waymo in 2018. nuScenes: a large-scale autonomous driving dataset (used token for query). Argoverse2 Lyft 2. Model Zoo From the time zone Most survey roughly divide the models into 4 parts: Multi-viewed, Voxel-based, Point-based, and Point-Voxel-based methods. But the overall, the improvement process from the perspective of the time zone is more likely to be digested by beginners.\n(An illustration of point-based 3D object detection methods.)\nBefore 2017 VeloFCN: which transforms 3d point clouds to 2d front view. This is not a that good idea, as many points can be mapped to the same position as well as the lack of depth information. MV3D: Combine LiDAR Bird view (BV), LiDAR Front view (FV), and RGB Image information and fuse them together to get the overall feature. This method as for me is quite astonishing, since I think it is the pioneer model that introduce multi-modality into 3d object detection. The backend detector is typically R-CNN at that time, so it costs a lot of time. During 2017 Two break-through workds came out: VoxelNet and PointNet++. VoxelNet extracted the feature from the perspective of 3d voxel while PointNet++ from the aspect of point.\nVoxelNet: First, the point cloud is quantized into a uniform 3D grid (as shown in “grouping” in the figure below). Within each grid, a fixed number of points are randomly sampled (with repetitions if there are not enough points). Each point is represented by a 7-dimensional feature, including the X, Y, Z coordinates of the point, its reflectance intensity (R), and the position difference (ΔX, ΔY, ΔZ) relative to the grid’s centroid (the mean position of all points within the grid). Fully connected layers are used to extract features from each point, and then the features of each point are concatenated with the mean features of all points within the grid to form new point features. The advantage of this feature is that it preserves both the characteristics of individual points and the characteristics of a small local area (the grid) surrounding the point. This process of point feature extraction can be repeated multiple times to enhance the descriptive power of the features (as shown in “Stacked Voxel Feature Encoding” in the figure below). Finally, a max pooling operation is performed on all points within the grid to obtain a fixed-length feature vector. All of the above is the feature extracting network. Accompany with RPN, the network can accomplish 3d object detection. PointNet++: The primary approach involves using clustering to generate multiple candidate regions (each region being a set of points), and within each candidate region, PointNet is used to extract features of the points. This process is repeated multiple times in a hierarchical manner, where the multiple sets of points output by the clustering algorithm at each iteration are treated as abstracted point clouds for subsequent processing (Set Abstraction, SA). The point features obtained in this manner have a large receptive field and contain rich contextual information from the local neighborhood. Finally, PointNet classification is performed on the sets of points produced by multiple layers of SA to distinguish between objects and the background. Similarly, this method can also be applied to point cloud segmentation. The adantage and also the downside as compared PointNet++ with VoxelNet:\nMerit: There’s not that big information loss or gap in PointNet++, and there are barely any hyper-parameter for you to set up. Didn’t use 3D conv. Downside: Didn’t use mature 2D conv to extract feature to ensure both accuracy and efficiency. Too many MLP leads to low efficiency. Between 2018 to 2020 During this period, lots of follow-up works came out after the invention of VoxelNet and PointNet++.\nTowards voxel-based:\nSECOND: utilized sparse conv method, accelerating the speed to 26 FPS and lower and also reduces the usage of graphics memory.\nPointPillar: Instead of using 3D conv, it stack all the voxels into pillars so that it can utilize the mature hardware acceleration about 2D conv, making its speed up to 62 FPS\nTowards point-based: The SA process in PointNet++ makes the overall process slow, so many follow-up methods came up with the idea of utilizing 2D conv to solve this problem.\nPoint-RCNN: First, PointNet++ is used to extract features from points. These features are then used for foreground segmentation to distinguish between points on objects and background points. At the same time, each foreground point also outputs a 3D candidate bounding box (BBox). The next step involves further feature extraction from points within the candidate BBox, determining the object category to which the BBox belongs, and refining its position and size. Those familiar with 2D object detection might recognize this as a typical two-stage detection model. Indeed, but the difference is that Point-RCNN generates candidates only on foreground points, thereby avoiding the immense computational cost associated with generating dense candidate boxes in 3D space. Nevertheless, as a two-stage detector and considering the substantial computational demand of PointNet++ itself, Point-RCNN still operates at a relatively low efficiency of about 13 FPS. Point-RCNN was later extended to Part-A2, which achieved improvements in both speed and accuracy.\n3D-SSD: analyzes the components of previous point-based methods and concludes that the Feature Propagation (FP) layer and the refinement layer are bottlenecks for system speed. The role of the FP layer is to remap the abstracted point features from the Set Abstraction (SA) layer back to the original point cloud, analogous to the Point Cloud Decoder in Point-RCNN as depicted in the figure above. This step is crucial because the abstract points output by SA do not effectively cover all objects, leading to significant information loss. 3D-SSD introduces a new clustering method that considers the similarity between points in both geometric and feature spaces. Through this improved clustering method, the output of the SA layer can be directly used to generate object proposals, avoiding the computational cost associated with the FP layer. Furthermore, to circumvent the region pooling in the refinement phase, 3D-SSD directly uses representative points from the SA output. It utilizes the improved clustering algorithm mentioned earlier to find neighboring points and employs a simple MLP to predict categories and 3D bounding boxes for objects. 3D-SSD can be considered an anchor-free, single-stage detector, aligning with the development trend in the object detection domain. With these improvements, 3D-SSD achieves a processing speed of 25 FPS.\nIntegration of voxel-based and point-based methods:\nOn one side, voxels heavily rely on the granularity of quantization parameters: larger grids lead to significant information loss, while smaller grids escalate computational and memory demands. Imagine trying to piece together a puzzle with either too large or too minuscule pieces – neither scenario is ideal for capturing the full picture efficiently. On the other side, points pose their own set of challenges, particularly in extracting contextual features from their neighborhoods and dealing with irregular memory access patterns. In fact, about 80% of the runtime is often consumed by data construction rather than the actual feature extraction process. It’s akin to spending most of your time organizing your tools instead of painting.\nVoxelNet SECOND PointPillar PointRCNN 3D-SSD AP 64.17% 75.96% 74.31% 75.64% 79.57% FPS 2.0 26.3 62.0 10.0 25.0 The fundamental strategy for merging the strengths of voxels and points involves leveraging lower-resolution voxels to capture contextual features (as seen in PV-CNN) or generate object candidates (like in Fast Point RCNN), or even both (examples include PV-RCNN and SA-SSD). Then, these are combined with the original point cloud, preserving both the nuanced features of individual points and the spatial relationships among them. Imagine blending the broad strokes of a paintbrush with the precision of a pencil to create a detailed and context-rich artwork.\nThe research in PV-CNN, PV-RCNN, Voxel R-CNN, CenterPoint TO BE CONTINUED…\nLiDAR-based model Point-based model 2022 SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection (AAAI 22) 2021 3D Object Detection with Pointformer (CVPR 21)\nRelation Graph Network for 3D Object Detection in Point Clouds (T-IP 21)\n3D-CenterNet: 3D object detection network for point clouds with center estimation priority (PR 21)\n2020 3DSSD: Point-based 3D Single Stage Object Detector (CVPR 20)\nPoint-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud (CVPR 20)\nJoint 3D Instance Segmentation and Object Detection for Autonomous Driving (CVPR 20)\nImproving 3D Object Detection through Progressive Population Based Augmentation (ECCV 20)\nFalse Positive Removal for 3D Vehicle Detection with Penetrated Point Classifier (ICIP 20)\n2019 PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud (CVPR 19)\nAttentional PointNet for 3D-Object Detection in Point Clouds (CVPRW 19)\nSTD: Sparse-to-Dense 3D Object Detector for Point Cloud (ICCV 19)\nStarNet: Targeted Computation for Object Detection in Point Clouds (arXiv 19)\nPointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement (arXiv 19)\n2018 IPOD: Intensive Point-based Object Detector for Point Cloud (arXiv 18) Grid-based 3D Object Detection (Voxels \u0026 Pillars) 2021 Object DGCNN: 3D Object Detection using Dynamic Graphs (NeurIPS 21)\nCenter-based 3D Object Detection and Tracking (CVPR 21)\nVoxel Transformer for 3D Object Detection (ICCV 21)\nLiDAR-Aug: A General Rendering-based Augmentation Framework for 3D Object Detection (CVPR 21)\nRAD: Realtime and Accurate 3D Object Detection on Embedded Systems (CVPRW 21)\nAGO-Net: Association-Guided 3D Point Cloud Object Detection Network (T-PAMI 21)\nCIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud (AAAI 21)\nVoxel R-CNN: Towards High Performance Voxel-based 3D Object Detection (AAAI 21)\nAnchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud (ACM MM 21)\nIntegration of Coordinate and Geometric Surface Normal for 3D Point Cloud Object Detection (IJCNN 21)\nPSANet: Pyramid Splitting and Aggregation Network for 3D Object Detection in Point Cloud (Sensors 21)\n2020 Every View Counts: Cross-View Consistency in 3D Object Detection with Hybrid-Cylindrical-Spherical Voxelization (NeurIPS 20)\nHVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection (CVPR 20)\nAssociate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection (CVPR 20)\nDOPS: Learning to Detect 3D Objects and Predict their 3D Shapes (CVPR 20)\nObject as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots (ECCV 20)\nSSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds (ECCV 20)\nPillar-based Object Detection for Autonomous Driving (ECCV 20)\nFrom Points to Parts: 3D Object Detection From Point Cloud With Part-Aware and Part-Aggregation Network (T-PAMI 20)\nReconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds (CoRL 20)\nSegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud (ICRA 20)\nTANet: Robust 3D Object Detection from Point Clouds with Triple Attention (AAAI 20)\nSARPNET: Shape attention regional proposal network for liDAR-based 3D object detection (NeuroComputing 20)\nVoxel-FPN: Multi-Scale Voxel Feature Aggregation for 3D Object Detection from LIDAR Point Clouds (Sensors 20)\nBirdNet+: End-to-End 3D Object Detection in LiDAR Bird’s Eye View (ITSC 20)\n1st Place Solution for Waymo Open Dataset Challenge - 3D Detection and Domain Adaptation (arXiv 20)\nAFDet: Anchor Free One Stage 3D Object Detection (arXiv 20)\n2019 PointPillars: Fast Encoders for Object Detection from Point Clouds (CVPR 19)\nEnd-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (CoRL 19)\nIoU Loss for 2D/3D Object Detection (3DV 19)\nAccurate and Real-time Object Detection based on Bird’s Eye View on 3D Point Clouds (3DV 19)\nFocal Loss in 3D Object Detection (RA-L 19)\n3D-GIoU: 3D Generalized Intersection over Union for Object Detection in Point Cloud (Sensors 19)\nFVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds (CISP 19)\nClass-balanced Grouping and Sampling for Point Cloud 3D Object Detection (arXiv 19)\nPatch Refinement - Localized 3D Object Detection (arXiv 19)\n2018 VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection (CVPR 18)\nPIXOR: Real-time 3D Object Detection from Point Clouds (CVPR 18)\nSECOND: Sparsely Embedded Convolutional Detection (Sensors 18)\nRT3D: Real-Time 3-D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving (RA-L 18)\nBirdNet: a 3D Object Detection Framework from LiDAR Information (ITSC 18)\nYOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud (ECCVW 18)\nComplex-YOLO: An Euler-Region-Proposal for Real-time 3D Object Detection on Point Clouds (ECCVW 28)\n2017 or earlier 3D Fully Convolutional Network for Vehicle Detection in Point Cloud (IROS 17)\nVote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks (ICRA 17)\nVehicle Detection from 3D Lidar Using Fully Convolutional Network (RSS 16)\nVoting for Voting in Online Point Cloud Object Detection (RSS 15)\n3D Object Detection with Mixed Representations (point-voxel based) 2022 Behind the Curtain: Learning Occluded Shapes for 3D Object Detection (AAAI 22) 2021 LiDAR R-CNN: An Efficient and Universal 3D Object Detector (CVPR 21)\nPVGNet: A Bottom-Up One-Stage 3D Object Detector with Integrated Multi-Level Features (CVPR 21)\nHVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection (CVPR 21)\nPyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection (ICCV 21)\nImproving 3D Object Detection with Channel-wise Transformer (ICCV 21)\nSA-Det3D: Self-Attention Based Context-Aware 3D Object Detection (ICCVW 21)\nFrom Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder (ACM MM 21)\nRV-FuseNet: Range View Based Fusion of Time-Series LiDAR Data for Joint 3D Object Detection and Motion Forecasting (IROS 21)\nPattern-Aware Data Augmentation for LiDAR 3D Object Detection (ITSC 21)\nFrom Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection (T-CSVT 21)\nPseudo-Image and Sparse Points: Vehicle Detection With 2D LiDAR Revisited by Deep Learning-Based Methods (T-ITS 21)\nDual-Branch CNNs for Vehicle Detection and Tracking on LiDAR Data (T-ITS 21)\nImproved Point-Voxel Region Convolutional Neural Network: 3D Object Detectors for Autonomous Driving (T-ITS 21)\nDSP-Net: Dense-to-Sparse Proposal Generation Approach for 3D Object Detection on Point Cloud (IJCNN 21)\nP2V-RCNN: Point to Voxel Feature Learning for 3D Object Detection From Point Clouds (IEEE Access 21)\nPV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection (arXiv 21)\nM3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers (arXiv 21)\n2020 PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection (CVPR 20)\nStructure Aware Single-stage 3D Object Detection from Point Cloud (CVPR 20)\nSearching Efficient 3D Architectures with Sparse Point-Voxel Convolution (ECCV 20)\nInfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling (ECCV 20)\nSVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds (arXiv 20)\n2019 Point-Voxel CNN for Efficient 3D Deep Learning (NeurIPS 19)\nFast Point R-CNN (ICCV 19)\n2018 LMNet: Real-time Multiclass Object Detection on CPU Using 3D LiDAR (ACIRS 18) LiDAR \u0026 Camera Fusion for 3D Object Detection (multi-modal) 2022 AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection (arXiv 22)\nFast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection (WACV 22)\n2021 Multimodal Virtual Point 3D Detection (NeurIPS 21)\nPointAugmenting: Cross-Modal Augmentation for 3D Object Detection (CVPR 21)\nFrustum-PointPillars: A Multi-Stage Approach for 3D Object Detection using RGB Camera and LiDAR (ICCVW 21)\nMulti-Stage Fusion for Multi-Class 3D Lidar Detection (ICCVW 21)\nCross-Modality 3D Object Detection (WACV 21)\nSparse-PointNet: See Further in Autonomous Vehicles (RA-L 21)\nFusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection (ITSC 21)\nMF-Net: Meta Fusion Network for 3D object detection (IJCNN 21)\nMulti-Scale Spatial Transformer Network for LiDAR-Camera 3D Object Detection (IJCNN 21)\nBoost 3-D Object Detection via Point Clouds Segmentation and Fused 3-D GIoU-L1 Loss (T-NNLS)\nRangeLVDet: Boosting 3D Object Detection in LIDAR with Range Image and RGB Image (Sensors Journal 21)\nLiDAR Cluster First and Camera Inference Later: A New Perspective Towards Autonomous Driving (arXiv 21)\nExploring Data Augmentation for Multi-Modality 3D Object Detection (arXiv 21)\n2020 PointPainting: Sequential Fusion for 3D Object Detection (CVPR 20)\n3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection (ECCV 20)\nEPNet: Enhancing Point Features with Image Semantics for 3D Object Detection (ECCV 20)\nPI-RCNN: An Efficient Multi-Sensor 3D Object Detector with Point-Based Attentive Cont-Conv Fusion Module (AAAI 20)\nCLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection IROS 20\nLRPD: Long Range 3D Pedestrian Detection Leveraging Specific Strengths of LiDAR and RGB (ITSC 20)\nFusion of 3D LIDAR and Camera Data for Object Detection in Autonomous Vehicle Applications (Sensors Journal 20)\nSemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using LiDAR Point Cloud and Semantic Segmentation (MFI 20)\n2019 Multi-Task Multi-Sensor Fusion for 3D Object Detection (CVPR 19)\nComplexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds (CVPRW 19)\nSensor Fusion for Joint 3D Object Detection and Semantic Segmentation (CVPRW 19)\nMVX-Net: Multimodal VoxelNet for 3D Object Detection (ICRA 19)\nSEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data (ICRA 19)\n3D Object Detection Using Scale Invariant and Feature Reweighting Networks (AAAI 19)\nFrustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection (IROS 19)\nDeep End-to-end 3D Person Detection from Camera and Lidar (ITSC 19)\nRoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement (IV 19)\nSCANet: Spatial-channel attention network for 3D object detection (ICASSP 19)\nOne-Stage Multi-Sensor Data Fusion Convolutional Neural Network for 3D Object Detection (Sensors 19)\n2018 Frustum PointNets for 3D Object Detection from RGB-D Data (CVPR 18)\nPointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation (CVPR 18)\nDeep Continuous Fusion for Multi-Sensor 3D Object Detection (ECCV 18)\nJoint 3D Proposal Generation and Object Detection from View Aggregation (IROS 18)\nA General Pipeline for 3D Detection of Vehicles (ICRA 18)\nFusing Bird’s Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection (IV 18)\nRobust Camera Lidar Sensor Fusion Via Deep Gated Information Fusion Network (IV 18)\n2017 or earlier Multi-View 3D Object Detection Network for Autonomous Driving (CVPR 17) Tried to dive into code base PointPillars later on (below).\n3.15 PointPillars \u0026 Follow-up related to CLIP 1. PointPillars Setup the environment for PointPillars. The code base doesn’t need any of the 3D Object Detection framework like OpenPCDet or MMDetection3d. Figure out the network architecture of PointPillars by looking into the code except for the network forwarding (left for tomorrow). The following thing to be examined will be the generation of the GT box, loss function, training, and evaluation of the model. 2. Open Vocabulary Segmentation paper review Language-driven Semantic Segmentation (Lseg) Advantange: Accomplished using CLIP to do OVS. Downside: Still a supervised learning method. Accuracy is still uncomparable as compared to 1-shot method. Didn’t use the text as a supervised signal, meaning that the model is still relying on manual annotated mask to accomplish segmentation. GroupViT: Semantic Segmentation Emerges from Text Supervision (GroupViT) Advantage: Added Grouping Block (cluster the group) and learnable Group tokens (clustering centers). Downside: Still a patch-level segmentation. The final output uses bilinear interpolation. 3. Comparison between different frameworks of 3D object detection OpenPCDet GitHub: OpenPCDet\nCommunity: Active\nCode Quality: Lightweight and readable. Some top journal papers in recent years were developed based on this framework.\nDeployment:\nMore convenient than other frameworks, with existing deployment implementations available. Example deployment solutions include:\nCUDA-PointPillars PointPillars_MultiHead_40FPS OpenPCDet modified by hova88 Recommendation: A recommended starting point for beginners interested in learning about object detection frameworks.\nmmdetection3d GitHub: mmdetection3d Community: Active Documentation: Official documentation available, facilitating easier onboarding. Scope: Compared to OpenPCDet, mmdetection3d encompasses a broader range of scenarios, including 3D detection and segmentation for images, point clouds, and multimodal data sources. Code Quality: Well-packaged, but might be more challenging for beginners. The framework is also the basis for some top journal papers in recent years. Model Deployment: Still in experimental phase. Recommendation: Suitable for those familiar with 3D object detection, offering richer content for further learning. Det3D GitHub: Det3D Community Feedback: Previously reviewed but not extensively used in recent developments. Similar to OpenPCDet in being lightweight, but has not been updated recently. Deployment Solutions: Some existing deployment solutions are based on this framework, such as CenterPoint’s deployment: CenterPoint Deployment Recommendation: Lower priority for learning compared to OpenPCDet and mmdetection3d. Paddle3D GitHub: Paddle3D Community Feedback: Newly open-sourced as of August 2022, with unclear performance outcomes at the moment. After the review, I plan to get on OpenPCDet first for its ease of understanding.\n3.16 PointPillars Learned how anchor is generated and aligned with the GT boxes. Loss function computation is also understood, which uses Focal Loss for loss, and smoothL1 for regression loss. The training process is figured out, which uses torch.optim.AdamW() as the optimizer and torch.optim.lr_scheduler.OneCycleLR() as the scheduler. The following things left to be done is the prediction for a single cloud points (involves NMS), the visualization step, and the metrics and evaluation methods used in 3D object detection area. Ran KITTI benchmark on the model to check the accuracy. 3.17 Quantization \u0026 Swin-transformer 1. Quantization concept (apart from Pytorch) Linear:\nAffine: the importance of Z offset.\nint8 ( -128 ~ 127 ) uint8 ( 0 ~ 255 ) Minmax (above) Histogram (two arrows shrink towards each other until below or equal to the required coverage percentage) Entropy: TO BE CONTINUED… Symmetric: more concise as compared to asymmetric method.\nNon-linear\n2. Swin-transformer: Used the concept of conv to let the attention be focused first only on local part and then global perspective The architecture of the network is: Completely gone through the code base of Swin-transformer. 3.18 Quantization about Matrix Multiplication 1. im2col (3 channels)\n(3 kernals, transform to below)\nTherefore, conv operation is successfully transformed into matrix multiplication (one kernal):\nThe case for three kernel is the same: 2. Quantization in matrix multiplication (conv) As we can see, in order to leverage GEMM(INT8) acceleration, we must kill the variable k in s.\nIf we kill k in s, then on the right hand side, it’s just basic INT8 matrix multiplication with the left hand side is the scale. Killing the k also means that the scale s will be shared across row respective to X and across column respective to W:\nThen per-channel is easy to explain:\n3.19 Quantization with code 1. Different methods Dynamic Quantization: The easiest method of quantization PyTorch supports is called dynamic quantization. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”).\nimport torch.quantization quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8) Additional adjustment may be involved, like replace the original add and concat operation with nn.quantized.FloatFunctional\nPost-Training Static Quantization:\nStatic quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time\n# set quantization config for server (x86) deploymentmyModel.qconfig = torch.quantization.get_default_config('fbgemm') # insert observers torch.quantization.prepare(myModel, inplace=True) # Calibrate the model and collect statistics # convert to quantized version torch.quantization.convert(myModel, inplace=True) Quantization Aware Training:\nQuantization-aware training(QAT) is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.\n# specify quantization config for QAT qat_model.qconfig=torch.quantization.get_default_qat_qconfig('fbgemm') # prepare QAT torch.quantization.prepare_qat(qat_model, inplace=True) # convert to quantized version, removing dropout, to check for accuracy on each epochquantized_model=torch.quantization.convert(qat_model.eval(), inplace=False) During the re-training process, we’ll forward the loss of the mimic int8 values and the true values to the optimizer, so that this quantization loss will be optimized during re-training.\nForward process: use one of the methods from MinMax, Histogram, and Entropy to mimic the quantization Backward process: use smooth derivative method so that we can get the derivarive TO BE CONTINUED… 2. Different frameworks pytorch-quantization from Nvidia V.S. torch.ao.quantization from native Pytorch torch.ao.quantization aims primarily on CPU while pytorch-quantization focuses on Nvidia platform deployment. Overall same procedure. More people using the first one. No one answers this question. Things that I plan to do next week 1. More understanding in quantization Theoretical part: A White Paper on Neural Network Quantization Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference Code Practice: If torch.ao.quantization: Official Docs Official Tutorials else: pytorch-quantization module docs and tutorial. 2. Span on more code of 3D object detection model Complete the review of code for PointPillars’ predict and visualization process (link above). BevFormer CenterNet and CenterPoint and more… 3. Empirical experiment on quantization on 3d object detection model Highly possible that the performance will not be that good on PointPillars since:\nby LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection.\nIf bad, I’ll go understand the essay to explore why.\nApply all three types of methods on these models, check the performance and sort out why.\nBy next week Get CenterPoint run on evaluation. Quantize CenterPoint using the three methods to check the performance as compared to un-quantized one: Latency for each inference. Use CUDA memory API to calculate total memory usage by the model Either calculate the activation memory usage by hand or use some other tools After all these, try apply SmoothQuant to the model and see the result. ",
  "wordCount" : "4426",
  "inLanguage": "en",
  "datePublished": "2024-03-20T06:35:10-05:00",
  "dateModified": "2024-03-20T06:35:10-05:00",
  "author":{
    "@type": "Person",
    "name": "Banghao Chi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://banghao.live/blog/log/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Banghao's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://banghao.live/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://banghao.live/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://banghao.live/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.live/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://banghao.studio/login" title="Forum App">
                    <span>Forum App</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Daily Log
    </h1>
    <div class="post-meta"><span title='2024-03-20 06:35:10 -0500 -0500'>March 20, 2024</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h1 id="312">3.12<a hidden class="anchor" aria-hidden="true" href="#312">#</a></h1>
<p>Managed to understand the whole code base of the <a href="https://openai.com/research/clip">CLIP</a> repo from OpenAI. Planned to take a look at <a href="https://arxiv.org/abs/2303.11797">CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation</a>, to understand how to implement Open-Vocabulary Segmentation (OVS) using CLIP.</p>
<h1 id="313">3.13<a hidden class="anchor" aria-hidden="true" href="#313">#</a></h1>
<h3 id="1-detr">1. DETR<a hidden class="anchor" aria-hidden="true" href="#1-detr">#</a></h3>
<p>Got a basic understanding of DETR, which is an awesome end-to-end 2D object detection architecture, with its downside lies in:</p>
<ul>
<li>Long training period</li>
<li>Difficulty of detecting small objects</li>
</ul>
<p>but has advantages in:</p>
<ul>
<li>Use object queiries to replace anchor generation</li>
<li>Use Hangurian algorithm to replace NMS post-processing stage</li>
</ul>
<p>which basically replace things that are not learnable into learnable parameters.</p>
<p>The follow-up work: Deformable DETR, DINO, Omni-DETR, Up-DETR, PNP-DETR, SMAC-DETR, DAB-DETR, SAM-DETR, DN-, OW-, OV-, Pixel2Seq&hellip;&hellip;</p>
<h3 id="2-picture-generation-survey">2. Picture generation survey<a hidden class="anchor" aria-hidden="true" href="#2-picture-generation-survey">#</a></h3>
<ul>
<li>GAN:
<ul>
<li>Merit:
<ul>
<li>Pictures are real.</li>
</ul>
</li>
<li>Downside:
<ul>
<li>Unstable training process (the only noise is the initial noise at the beginning of the training).</li>
<li>The outputs lack diversity, meaning that they are close to the original pictures.</li>
</ul>
</li>
</ul>
</li>
<li>AE (auto-encoder):<img loading="lazy" src="../assets/image-20240313212557543.png" alt="image-20240313212557543"  />
</li>
<li>DAE (Denoising auto-encoder):<img loading="lazy" src="../assets/image-20240313212652120.png" alt="image-20240313212652120"  />
</li>
</ul>
<p>Added noise $X_c$ This paper proves that &ldquo;Images have a high degree of redundancy&rdquo;. This work is also similar to MAE.</p>
<h3 id="3-dalle-2">3. DALL·E 2<a hidden class="anchor" aria-hidden="true" href="#3-dalle-2">#</a></h3>
<ul>
<li>Diffusion model is actually a multi-layered VAE.</li>
</ul>
<h1 id="314-3d-object-detection">3.14 3D object detection<a hidden class="anchor" aria-hidden="true" href="#314-3d-object-detection">#</a></h1>
<h5 id="1-datasets">1. Datasets<a hidden class="anchor" aria-hidden="true" href="#1-datasets">#</a></h5>
<ul>
<li>KITTY: the most classic dataset used in 3d obejct detection.
<img loading="lazy" src="../assets/image-20240320132044562.png" alt="image-20240320132044562"  />

<img loading="lazy" src="../assets/image-20240320132433043.png" alt="image-20240320132433043"  />

<table>
<thead>
<tr>
<th>Category</th>
<th>Truncation</th>
<th>Occlusion</th>
<th>Alpha</th>
<th>Bbox_X1</th>
<th>Bbox_Y1</th>
<th>Bbox_X2</th>
<th>Bbox_Y2</th>
<th>Dimensions_3D_Height</th>
<th>Dimensions_3D_Width</th>
<th>Dimensions_3D_Length</th>
<th>Location_X</th>
<th>Location_Y</th>
<th>Location_Z</th>
<th>Yaw</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pedestrian</td>
<td>0.00</td>
<td>0</td>
<td>-0.20</td>
<td>712.40</td>
<td>143.00</td>
<td>810.73</td>
<td>307.92</td>
<td>1.89</td>
<td>0.48</td>
<td>1.20</td>
<td>1.84</td>
<td>1.47</td>
<td>8.41</td>
<td>0.01</td>
</tr>
</tbody>
</table>
</li>
</ul>
<p><img loading="lazy" src="../assets/image-20240320132739424.png" alt="image-20240320132739424"  />
</p>
<p>Can use Open3D for further use.</p>
<ul>
<li>Waymo: a large dataset released by Waymo in 2018.</li>
<li>nuScenes: a large-scale autonomous driving dataset (used token for query).
<img loading="lazy" src="../assets/image-20240320133312528.png" alt="image-20240320133312528"  />

<img loading="lazy" src="../assets/image-20240320133625985.png" alt="image-20240320133625985"  />
</li>
<li>Argoverse2</li>
<li>Lyft</li>
</ul>
<h5 id="2-model-zoo">2. Model Zoo<a hidden class="anchor" aria-hidden="true" href="#2-model-zoo">#</a></h5>
<h1 id="from-the-time-zone">From the time zone<a hidden class="anchor" aria-hidden="true" href="#from-the-time-zone">#</a></h1>
<p><img loading="lazy" src="../assets/image-20240320103630883.png" alt="image-20240320103630883"  />
</p>
<p>Most survey roughly divide the models into 4 parts: Multi-viewed, Voxel-based, Point-based, and Point-Voxel-based methods. But the overall, the improvement process from the perspective of the time zone is more likely to be digested by beginners.</p>
<p><img loading="lazy" src="../assets/image-20240320103719709.png" alt="image-20240320103719709"  />
</p>
<p>(An illustration of point-based 3D object detection methods.)</p>
<h3 id="before-2017">Before 2017<a hidden class="anchor" aria-hidden="true" href="#before-2017">#</a></h3>
<ul>
<li><strong>VeloFCN</strong>: which transforms 3d point clouds to 2d front view. This is not a that good idea, as many points can be mapped to the same position as well as the lack of depth information.</li>
<li><strong>MV3D</strong>: Combine LiDAR Bird view (BV), LiDAR Front view (FV), and RGB Image information and fuse them together to get the overall feature. This method as for me is quite astonishing, since I think it is the pioneer model that introduce multi-modality into 3d object detection. The backend detector is typically R-CNN at that time, so it costs a lot of time.</li>
</ul>
<p><img loading="lazy" src="../assets/image-20240320104933704.png" alt="image-20240320104933704"  />
</p>
<h3 id="during-2017">During 2017<a hidden class="anchor" aria-hidden="true" href="#during-2017">#</a></h3>
<p>Two break-through workds came out: <strong>VoxelNet</strong> and <strong>PointNet++</strong>. VoxelNet extracted the feature from the perspective of 3d voxel while PointNet++ from the aspect of point.</p>
<ul>
<li><strong>VoxelNet</strong>: First, the point cloud is quantized into a uniform 3D grid (as shown in &ldquo;grouping&rdquo; in the figure below). Within each grid, a fixed number of points are randomly sampled (with repetitions if there are not enough points). Each point is represented by a 7-dimensional feature, including the X, Y, Z coordinates of the point, its reflectance intensity (R), and the position difference (ΔX, ΔY, ΔZ) relative to the grid&rsquo;s centroid (the mean position of all points within the grid). Fully connected layers are used to extract features from each point, and then the features of each point are concatenated with the mean features of all points within the grid to form new point features. The advantage of this feature is that it preserves both the characteristics of individual points and the characteristics of a small local area (the grid) surrounding the point. This process of point feature extraction can be repeated multiple times to enhance the descriptive power of the features (as shown in &ldquo;Stacked Voxel Feature Encoding&rdquo; in the figure below). Finally, a max pooling operation is performed on all points within the grid to obtain a fixed-length feature vector. All of the above is the feature extracting network. Accompany with RPN, the network can accomplish 3d object detection.</li>
</ul>
<p><img loading="lazy" src="../assets/image-20240320105551659.png" alt="image-20240320105551659"  />
</p>
<ul>
<li><strong>PointNet++</strong>: The primary approach involves using clustering to generate multiple candidate regions (each region being a set of points), and within each candidate region, PointNet is used to extract features of the points. This process is repeated multiple times in a hierarchical manner, where the multiple sets of points output by the clustering algorithm at each iteration are treated as abstracted point clouds for subsequent processing (Set Abstraction, SA). The point features obtained in this manner have a large receptive field and contain rich contextual information from the local neighborhood. Finally, PointNet classification is performed on the sets of points produced by multiple layers of SA to distinguish between objects and the background. Similarly, this method can also be applied to point cloud segmentation.</li>
</ul>
<p><img loading="lazy" src="../assets/image-20240320110001677.png" alt="image-20240320110001677"  />
</p>
<p><img loading="lazy" src="../assets/image-20240320110042687.png" alt="image-20240320110042687"  />
</p>
<p>The adantage and also the downside as compared PointNet++ with VoxelNet:</p>
<ul>
<li>Merit:
<ul>
<li>There&rsquo;s not that big information loss or gap in PointNet++, and there are barely any hyper-parameter for you to set up.</li>
<li>Didn&rsquo;t use 3D conv.</li>
</ul>
</li>
<li>Downside:
<ul>
<li>Didn&rsquo;t use mature 2D conv to extract feature to ensure both accuracy and efficiency.</li>
<li>Too many MLP leads to low efficiency.</li>
</ul>
</li>
</ul>
<h3 id="between-2018-to-2020">Between 2018 to 2020<a hidden class="anchor" aria-hidden="true" href="#between-2018-to-2020">#</a></h3>
<p>During this period, lots of follow-up works came out after the invention of <strong>VoxelNet</strong> and <strong>PointNet++</strong>.</p>
<ul>
<li>
<p>Towards voxel-based:</p>
<ul>
<li>
<p><strong>SECOND</strong>: utilized sparse conv method, accelerating the speed to 26 FPS and lower and also reduces the usage of graphics memory.</p>
</li>
<li>
<p><strong>PointPillar</strong>: Instead of using 3D conv, it stack all the voxels into pillars so that it can utilize the mature hardware acceleration about 2D conv, making its speed up to 62 FPS</p>
</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="../assets/image-20240320124503012.png" alt="image-20240320124503012"  />
</p>
<ul>
<li>
<p>Towards point-based: The SA process in <strong>PointNet++</strong> makes the overall process slow, so many follow-up methods came up with the idea of utilizing 2D conv to solve this problem.</p>
<ul>
<li>
<p><strong>Point-RCNN</strong>: First, PointNet++ is used to extract features from points. These features are then used for foreground segmentation to distinguish between points on objects and background points. At the same time, each foreground point also outputs a 3D candidate bounding box (BBox). The next step involves further feature extraction from points within the candidate BBox, determining the object category to which the BBox belongs, and refining its position and size. Those familiar with 2D object detection might recognize this as a typical two-stage detection model. Indeed, but the difference is that Point-RCNN generates candidates only on foreground points, thereby avoiding the immense computational cost associated with generating dense candidate boxes in 3D space. Nevertheless, as a two-stage detector and considering the substantial computational demand of PointNet++ itself, Point-RCNN still operates at a relatively low efficiency of about 13 FPS. Point-RCNN was later extended to <strong>Part-A2</strong>, which achieved improvements in both speed and accuracy.</p>
<p><img loading="lazy" src="../assets/image-20240320130208632.png" alt="image-20240320130208632"  />
</p>
</li>
<li>
<p><strong>3D-SSD</strong>: analyzes the components of previous point-based methods and concludes that the Feature Propagation (FP) layer and the refinement layer are bottlenecks for system speed. The role of the FP layer is to remap the abstracted point features from the Set Abstraction (SA) layer back to the original point cloud, analogous to the Point Cloud Decoder in Point-RCNN as depicted in the figure above. This step is crucial because the abstract points output by SA do not effectively cover all objects, leading to significant information loss. 3D-SSD introduces a new clustering method that considers the similarity between points in both geometric and feature spaces. Through this improved clustering method, the output of the SA layer can be directly used to generate object proposals, avoiding the computational cost associated with the FP layer. Furthermore, to circumvent the region pooling in the refinement phase, 3D-SSD directly uses representative points from the SA output. It utilizes the improved clustering algorithm mentioned earlier to find neighboring points and employs a simple MLP to predict categories and 3D bounding boxes for objects. 3D-SSD can be considered an anchor-free, single-stage detector, aligning with the development trend in the object detection domain. With these improvements, 3D-SSD achieves a processing speed of 25 FPS.</p>
<p><img loading="lazy" src="../assets/image-20240320130621693.png" alt="image-20240320130621693"  />
</p>
</li>
</ul>
</li>
<li>
<p>Integration of voxel-based and point-based methods:</p>
<p>On one side, voxels heavily rely on the granularity of quantization parameters: larger grids lead to significant information loss, while smaller grids escalate computational and memory demands. Imagine trying to piece together a puzzle with either too large or too minuscule pieces – neither scenario is ideal for capturing the full picture efficiently. On the other side, points pose their own set of challenges, particularly in extracting contextual features from their neighborhoods and dealing with irregular memory access patterns. In fact, about 80% of the runtime is often consumed by data construction rather than the actual feature extraction process. It&rsquo;s akin to spending most of your time organizing your tools instead of painting.</p>
<table>
<thead>
<tr>
<th></th>
<th>VoxelNet</th>
<th>SECOND</th>
<th>PointPillar</th>
<th>PointRCNN</th>
<th>3D-SSD</th>
</tr>
</thead>
<tbody>
<tr>
<td>AP</td>
<td>64.17%</td>
<td>75.96%</td>
<td>74.31%</td>
<td>75.64%</td>
<td>79.57%</td>
</tr>
<tr>
<td>FPS</td>
<td>2.0</td>
<td>26.3</td>
<td>62.0</td>
<td>10.0</td>
<td>25.0</td>
</tr>
</tbody>
</table>
<p>The fundamental strategy for merging the strengths of voxels and points involves leveraging lower-resolution voxels to capture contextual features (as seen in <strong>PV-CNN</strong>) or generate object candidates (like in <strong>Fast Point RCNN</strong>), or even both (examples include <strong>PV-RCNN</strong> and <strong>SA-SSD</strong>). Then, these are combined with the original point cloud, preserving both the nuanced features of individual points and the spatial relationships among them. Imagine blending the broad strokes of a paintbrush with the precision of a pencil to create a detailed and context-rich artwork.</p>
<p>The research in <strong>PV-CNN</strong>, <strong>PV-RCNN</strong>, <strong>Voxel R-CNN</strong>, <strong>CenterPoint</strong> <strong>TO BE CONTINUED</strong>&hellip;</p>
</li>
</ul>
<h1 id="lidar-based-model">LiDAR-based model<a hidden class="anchor" aria-hidden="true" href="#lidar-based-model">#</a></h1>
<h2 id="point-based-model">Point-based model<a hidden class="anchor" aria-hidden="true" href="#point-based-model">#</a></h2>
<h2 id="2022">2022<a hidden class="anchor" aria-hidden="true" href="#2022">#</a></h2>
<ul>
<li>SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection <a href="https://arxiv.org/pdf/2201.01976.pdf">(AAAI 22)</a></li>
</ul>
<h2 id="2021">2021<a hidden class="anchor" aria-hidden="true" href="#2021">#</a></h2>
<ul>
<li>
<p>3D Object Detection with Pointformer <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_3D_Object_Detection_With_Pointformer_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>Relation Graph Network for 3D Object Detection in Point Clouds <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234727">(T-IP 21)</a></p>
</li>
<li>
<p>3D-CenterNet: 3D object detection network for point clouds with center estimation priority <a href="https://www.sciencedirect.com/science/article/pii/S0031320321000716">(PR 21)</a></p>
</li>
</ul>
<h2 id="2020">2020<a hidden class="anchor" aria-hidden="true" href="#2020">#</a></h2>
<ul>
<li>
<p>3DSSD: Point-based 3D Single Stage Object Detector <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_3DSSD_Point-Based_3D_Single_Stage_Object_Detector_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Point-GNN_Graph_Neural_Network_for_3D_Object_Detection_in_a_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Joint 3D Instance Segmentation and Object Detection for Autonomous Driving <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Improving 3D Object Detection through Progressive Population Based Augmentation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660273.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>False Positive Removal for 3D Vehicle Detection with Penetrated Point Classifier <a href="https://arxiv.org/pdf/2005.13153.pdf">(ICIP 20)</a></p>
</li>
</ul>
<h2 id="2019">2019<a hidden class="anchor" aria-hidden="true" href="#2019">#</a></h2>
<ul>
<li>
<p>PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.pdf">(CVPR 19)</a></p>
</li>
<li>
<p>Attentional PointNet for 3D-Object Detection in Point Clouds <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf">(CVPRW 19)</a></p>
</li>
<li>
<p>STD: Sparse-to-Dense 3D Object Detector for Point Cloud <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.pdf">(ICCV 19)</a></p>
</li>
<li>
<p>StarNet: Targeted Computation for Object Detection in Point Clouds <a href="https://arxiv.org/pdf/1908.11069.pdf">(arXiv 19)</a></p>
</li>
<li>
<p>PointRGCN: Graph Convolution Networks for 3D Vehicles Detection Refinement <a href="https://arxiv.org/pdf/1911.12236.pdf">(arXiv 19)</a></p>
</li>
</ul>
<h2 id="2018">2018<a hidden class="anchor" aria-hidden="true" href="#2018">#</a></h2>
<ul>
<li>IPOD: Intensive Point-based Object Detector for Point Cloud <a href="https://arxiv.org/pdf/1812.05276.pdf">(arXiv 18)</a></li>
</ul>
<h2 id="grid-based-3d-object-detection-voxels--pillars">Grid-based 3D Object Detection (Voxels &amp; Pillars)<a hidden class="anchor" aria-hidden="true" href="#grid-based-3d-object-detection-voxels--pillars">#</a></h2>
<h2 id="2021-1">2021<a hidden class="anchor" aria-hidden="true" href="#2021-1">#</a></h2>
<ul>
<li>
<p>Object DGCNN: 3D Object Detection using Dynamic Graphs <a href="https://arxiv.org/pdf/2110.06923.pdf">(NeurIPS 21)</a></p>
</li>
<li>
<p>Center-based 3D Object Detection and Tracking <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yin_Center-Based_3D_Object_Detection_and_Tracking_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>Voxel Transformer for 3D Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Mao_Voxel_Transformer_for_3D_Object_Detection_ICCV_2021_paper.pdf">(ICCV 21)</a></p>
</li>
<li>
<p>LiDAR-Aug: A General Rendering-based Augmentation Framework for 3D Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_LiDAR-Aug_A_General_Rendering-Based_Augmentation_Framework_for_3D_Object_Detection_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>RAD: Realtime and Accurate 3D Object Detection on Embedded Systems <a href="https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Aghdam_RAD_Realtime_and_Accurate_3D_Object_Detection_on_Embedded_Systems_CVPRW_2021_paper.pdf">(CVPRW 21)</a></p>
</li>
<li>
<p>AGO-Net: Association-Guided 3D Point Cloud Object Detection Network <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511841">(T-PAMI 21)</a></p>
</li>
<li>
<p>CIA-SSD: Confident IoU-Aware Single-Stage Object Detector From Point Cloud <a href="https://arxiv.org/pdf/2012.03015.pdf">(AAAI 21)</a></p>
</li>
<li>
<p>Voxel R-CNN: Towards High Performance Voxel-based 3D Object Detection <a href="https://www.aaai.org/AAAI21Papers/AAAI-3337.DengJ.pdf">(AAAI 21)</a></p>
</li>
<li>
<p>Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475208">(ACM MM 21)</a></p>
</li>
<li>
<p>Integration of Coordinate and Geometric Surface Normal for 3D Point Cloud Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9534281">(IJCNN 21)</a></p>
</li>
<li>
<p>PSANet: Pyramid Splitting and Aggregation Network for 3D Object Detection in Point Cloud <a href="https://www.mdpi.com/1424-8220/21/1/136/pdf">(Sensors 21)</a></p>
</li>
</ul>
<h2 id="2020-1">2020<a hidden class="anchor" aria-hidden="true" href="#2020-1">#</a></h2>
<ul>
<li>
<p>Every View Counts: Cross-View Consistency in 3D Object Detection with Hybrid-Cylindrical-Spherical Voxelization <a href="https://drive.google.com/file/d/1oXLz0SwJVn7HM85g2LUiJh6ydvvnxMqS/view">(NeurIPS 20)</a></p>
</li>
<li>
<p>HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_HVNet_Hybrid_Voxel_Network_for_LiDAR_Based_3D_Object_Detection_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Du_Associate-3Ddet_Perceptual-to-Conceptual_Association_for_3D_Point_Cloud_Object_Detection_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Najibi_DOPS_Learning_to_Detect_3D_Objects_and_Predict_Their_3D_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660069.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700579.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>Pillar-based Object Detection for Autonomous Driving <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670018.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>From Points to Parts: 3D Object Detection From Point Cloud With Part-Aware and Part-Aggregation Network <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018080">(T-PAMI 20)</a></p>
</li>
<li>
<p>Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds <a href="https://arxiv.org/pdf/2004.02724.pdf">(CoRL 20)</a></p>
</li>
<li>
<p>SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196556">(ICRA 20)</a></p>
</li>
<li>
<p>TANet: Robust 3D Object Detection from Point Clouds with Triple Attention <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6837/6691">(AAAI 20)</a></p>
</li>
<li>
<p>SARPNET: Shape attention regional proposal network for liDAR-based 3D object detection <a href="https://www.sciencedirect.com/science/article/pii/S0925231219313827">(NeuroComputing 20)</a></p>
</li>
<li>
<p>Voxel-FPN: Multi-Scale Voxel Feature Aggregation for 3D Object Detection from LIDAR Point Clouds <a href="https://www.mdpi.com/1424-8220/20/3/704/pdf">(Sensors 20)</a></p>
</li>
<li>
<p>BirdNet+: End-to-End 3D Object Detection in LiDAR Bird’s Eye View <a href="https://arxiv.org/pdf/2003.04188.pdf">(ITSC 20)</a></p>
</li>
<li>
<p>1st Place Solution for Waymo Open Dataset Challenge - 3D Detection and Domain Adaptation <a href="https://arxiv.org/pdf/2006.15505.pdf">(arXiv 20)</a></p>
</li>
<li>
<p>AFDet: Anchor Free One Stage 3D Object Detection <a href="https://arxiv.org/pdf/2006.12671.pdf">(arXiv 20)</a></p>
</li>
</ul>
<h2 id="2019-1">2019<a hidden class="anchor" aria-hidden="true" href="#2019-1">#</a></h2>
<ul>
<li>
<p>PointPillars: Fast Encoders for Object Detection from Point Clouds <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.pdf">(CVPR 19)</a></p>
</li>
<li>
<p>End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds <a href="http://proceedings.mlr.press/v100/zhou20a/zhou20a.pdf">(CoRL 19)</a></p>
</li>
<li>
<p>IoU Loss for 2D/3D Object Detection <a href="https://arxiv.org/pdf/1908.03851.pdf">(3DV 19)</a></p>
</li>
<li>
<p>Accurate and Real-time Object Detection based on Bird’s Eye View on 3D Point Clouds <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885850">(3DV 19)</a></p>
</li>
<li>
<p>Focal Loss in 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624385">(RA-L 19)</a></p>
</li>
<li>
<p>3D-GIoU: 3D Generalized Intersection over Union for Object Detection in Point Cloud <a href="https://www.mdpi.com/1424-8220/19/19/4093/pdf">(Sensors 19)</a></p>
</li>
<li>
<p>FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965844">(CISP 19)</a></p>
</li>
<li>
<p>Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection <a href="https://arxiv.org/pdf/1908.09492.pdf">(arXiv 19)</a></p>
</li>
<li>
<p>Patch Refinement - Localized 3D Object Detection <a href="https://arxiv.org/pdf/1910.04093.pdf">(arXiv 19)</a></p>
</li>
</ul>
<h2 id="2018-1">2018<a hidden class="anchor" aria-hidden="true" href="#2018-1">#</a></h2>
<ul>
<li>
<p>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf">(CVPR 18)</a></p>
</li>
<li>
<p>PIXOR: Real-time 3D Object Detection from Point Clouds <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf">(CVPR 18)</a></p>
</li>
<li>
<p>SECOND: Sparsely Embedded Convolutional Detection <a href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf">(Sensors 18)</a></p>
</li>
<li>
<p>RT3D: Real-Time 3-D Vehicle Detection in LiDAR Point Cloud for Autonomous Driving <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8403277">(RA-L 18)</a></p>
</li>
<li>
<p>BirdNet: a 3D Object Detection Framework from LiDAR Information <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8569311">(ITSC 18)</a></p>
</li>
<li>
<p>YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Ali_YOLO3D_End-to-end_real-time_3D_Oriented_Object_Bounding_Box_Detection_from_ECCVW_2018_paper.pdf">(ECCVW 18)</a></p>
</li>
<li>
<p>Complex-YOLO: An Euler-Region-Proposal for Real-time 3D Object Detection on Point Clouds <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Simony_Complex-YOLO_An_Euler-Region-Proposal_for_Real-time_3D_Object_Detection_on_Point_ECCVW_2018_paper.pdf">(ECCVW 28)</a></p>
</li>
</ul>
<h2 id="2017-or-earlier">2017 or earlier<a hidden class="anchor" aria-hidden="true" href="#2017-or-earlier">#</a></h2>
<ul>
<li>
<p>3D Fully Convolutional Network for Vehicle Detection in Point Cloud <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8205955">(IROS 17)</a></p>
</li>
<li>
<p>Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient
Convolutional Neural Networks <a href="http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Engelcke2016ARXIV.pdf">(ICRA 17)</a></p>
</li>
<li>
<p>Vehicle Detection from 3D Lidar Using Fully Convolutional Network <a href="https://arxiv.org/pdf/1608.07916.pdf">(RSS 16)</a></p>
</li>
<li>
<p>Voting for Voting in Online Point Cloud Object Detection <a href="http://roboticsproceedings.org/rss11/p35.pdf">(RSS 15)</a></p>
</li>
</ul>
<h2 id="3d-object-detection-with-mixed-representations-point-voxel-based">3D Object Detection with Mixed Representations (point-voxel based)<a hidden class="anchor" aria-hidden="true" href="#3d-object-detection-with-mixed-representations-point-voxel-based">#</a></h2>
<h2 id="2022-1">2022<a hidden class="anchor" aria-hidden="true" href="#2022-1">#</a></h2>
<ul>
<li>Behind the Curtain: Learning Occluded Shapes for 3D Object Detection <a href="https://arxiv.org/pdf/2112.02205.pdf">(AAAI 22)</a></li>
</ul>
<h2 id="2021-2">2021<a hidden class="anchor" aria-hidden="true" href="#2021-2">#</a></h2>
<ul>
<li>
<p>LiDAR R-CNN: An Efficient and Universal 3D Object Detector <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_LiDAR_R-CNN_An_Efficient_and_Universal_3D_Object_Detector_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>PVGNet: A Bottom-Up One-Stage 3D Object Detector with Integrated Multi-Level Features <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Miao_PVGNet_A_Bottom-Up_One-Stage_3D_Object_Detector_With_Integrated_Multi-Level_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Noh_HVPR_Hybrid_Voxel-Point_Representation_for_Single-Stage_3D_Object_Detection_CVPR_2021_paper.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Mao_Pyramid_R-CNN_Towards_Better_Performance_and_Adaptability_for_3D_Object_ICCV_2021_paper.pdf">(ICCV 21)</a></p>
</li>
<li>
<p>Improving 3D Object Detection with Channel-wise Transformer <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Sheng_Improving_3D_Object_Detection_With_Channel-Wise_Transformer_ICCV_2021_paper.pdf">(ICCV 21)</a></p>
</li>
<li>
<p>SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection <a href="https://arxiv.org/pdf/2101.02672.pdf">(ICCVW 21)</a></p>
</li>
<li>
<p>From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder <a href="https://dl.acm.org/doi/pdf/10.1145/3474085.3475314">(ACM MM 21)</a></p>
</li>
<li>
<p>RV-FuseNet: Range View Based Fusion of Time-Series LiDAR Data for Joint 3D Object Detection and Motion Forecasting <a href="https://arxiv.org/pdf/2005.10863.pdf">(IROS 21)</a></p>
</li>
<li>
<p>Pattern-Aware Data Augmentation for LiDAR 3D Object Detection <a href="https://arxiv.org/pdf/2112.00050.pdf">(ITSC 21)</a></p>
</li>
<li>
<p>From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500203">(T-CSVT 21)</a></p>
</li>
<li>
<p>Pseudo-Image and Sparse Points: Vehicle Detection With 2D LiDAR Revisited by Deep Learning-Based Methods <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152088">(T-ITS 21)</a></p>
</li>
<li>
<p>Dual-Branch CNNs for Vehicle Detection and Tracking on LiDAR Data <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142426">(T-ITS 21)</a></p>
</li>
<li>
<p>Improved Point-Voxel Region Convolutional Neural Network: 3D Object Detectors for Autonomous Driving <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440849">(T-ITS 21)</a></p>
</li>
<li>
<p>DSP-Net: Dense-to-Sparse Proposal Generation Approach for 3D Object Detection on Point Cloud <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534412">(IJCNN 21)</a></p>
</li>
<li>
<p>P2V-RCNN: Point to Voxel Feature Learning for 3D Object Detection From Point Clouds <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474438">(IEEE Access 21)</a></p>
</li>
<li>
<p>PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection <a href="https://arxiv.org/pdf/2102.00463.pdf">(arXiv 21)</a></p>
</li>
<li>
<p>M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers <a href="https://arxiv.org/pdf/2104.11896.pdf">(arXiv 21)</a></p>
</li>
</ul>
<h2 id="2020-2">2020<a hidden class="anchor" aria-hidden="true" href="#2020-2">#</a></h2>
<ul>
<li>
<p>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Structure Aware Single-stage 3D Object Detection from Point Cloud <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Structure_Aware_Single-Stage_3D_Object_Detection_From_Point_Cloud_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730681.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling <a href="https://arxiv.org/pdf/2007.08556.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds <a href="https://arxiv.org/pdf/2006.04043.pdf">(arXiv 20)</a></p>
</li>
</ul>
<h2 id="2019-2">2019<a hidden class="anchor" aria-hidden="true" href="#2019-2">#</a></h2>
<ul>
<li>
<p>Point-Voxel CNN for Efficient 3D Deep Learning <a href="https://proceedings.neurips.cc/paper/2019/file/5737034557ef5b8c02c0e46513b98f90-Paper.pdf">(NeurIPS 19)</a></p>
</li>
<li>
<p>Fast Point R-CNN <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Fast_Point_R-CNN_ICCV_2019_paper.pdf">(ICCV 19)</a></p>
</li>
</ul>
<h2 id="2018-2">2018<a hidden class="anchor" aria-hidden="true" href="#2018-2">#</a></h2>
<ul>
<li>LMNet: Real-time Multiclass Object Detection on CPU Using 3D LiDAR <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8467245">(ACIRS 18)</a></li>
</ul>
<h1 id="lidar--camera-fusion-for-3d-object-detection-multi-modal">LiDAR &amp; Camera Fusion for 3D Object Detection (multi-modal)<a hidden class="anchor" aria-hidden="true" href="#lidar--camera-fusion-for-3d-object-detection-multi-modal">#</a></h1>
<h2 id="2022-2">2022<a hidden class="anchor" aria-hidden="true" href="#2022-2">#</a></h2>
<ul>
<li>
<p>AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection <a href="https://arxiv.org/pdf/2201.06493.pdf">(arXiv 22)</a></p>
</li>
<li>
<p>Fast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Pang_Fast-CLOCs_Fast_Camera-LiDAR_Object_Candidates_Fusion_for_3D_Object_Detection_WACV_2022_paper.pdf">(WACV 22)</a></p>
</li>
</ul>
<h2 id="2021-3">2021<a hidden class="anchor" aria-hidden="true" href="#2021-3">#</a></h2>
<ul>
<li>
<p>Multimodal Virtual Point 3D Detection <a href="https://proceedings.neurips.cc/paper/2021/file/895daa408f494ad58006c47a30f51c1f-Paper.pdf">(NeurIPS 21)</a></p>
</li>
<li>
<p>PointAugmenting: Cross-Modal Augmentation for 3D Object Detection <a href="https://vision.sjtu.edu.cn/files/cvpr21_pointaugmenting.pdf">(CVPR 21)</a></p>
</li>
<li>
<p>Frustum-PointPillars: A Multi-Stage Approach for 3D Object Detection using RGB Camera and LiDAR <a href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Paigwar_Frustum-PointPillars_A_Multi-Stage_Approach_for_3D_Object_Detection_Using_RGB_ICCVW_2021_paper.pdf">(ICCVW 21)</a></p>
</li>
<li>
<p>Multi-Stage Fusion for Multi-Class 3D Lidar Detection <a href="https://openaccess.thecvf.com/content/ICCV2021W/AVVision/papers/Wang_Multi-Stage_Fusion_for_Multi-Class_3D_Lidar_Detection_ICCVW_2021_paper.pdf">(ICCVW 21)</a></p>
</li>
<li>
<p>Cross-Modality 3D Object Detection <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhu_Cross-Modality_3D_Object_Detection_WACV_2021_paper.pdf">(WACV 21)</a></p>
</li>
<li>
<p>Sparse-PointNet: See Further in Autonomous Vehicles <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483647">(RA-L 21)</a></p>
</li>
<li>
<p>FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection <a href="https://arxiv.org/pdf/2106.12449.pdf">(ITSC 21)</a></p>
</li>
<li>
<p>MF-Net: Meta Fusion Network for 3D object detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534374">(IJCNN 21)</a></p>
</li>
<li>
<p>Multi-Scale Spatial Transformer Network for LiDAR-Camera 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533588">(IJCNN 21)</a></p>
</li>
<li>
<p>Boost 3-D Object Detection via Point Clouds Segmentation and Fused 3-D GIoU-L1 Loss <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241806">(T-NNLS)</a></p>
</li>
<li>
<p>RangeLVDet: Boosting 3D Object Detection in LIDAR with Range Image and RGB Image <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612185">(Sensors Journal 21)</a></p>
</li>
<li>
<p>LiDAR Cluster First and Camera Inference Later: A New Perspective Towards Autonomous Driving <a href="https://arxiv.org/pdf/2111.09799.pdf">(arXiv 21)</a></p>
</li>
<li>
<p>Exploring Data Augmentation for Multi-Modality 3D Object Detection <a href="https://arxiv.org/pdf/2012.12741.pdf">(arXiv 21)</a></p>
</li>
</ul>
<h2 id="2020-3">2020<a hidden class="anchor" aria-hidden="true" href="#2020-3">#</a></h2>
<ul>
<li>
<p>PointPainting: Sequential Fusion for 3D Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Vora_PointPainting_Sequential_Fusion_for_3D_Object_Detection_CVPR_2020_paper.pdf">(CVPR 20)</a></p>
</li>
<li>
<p>3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720715.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600035.pdf">(ECCV 20)</a></p>
</li>
<li>
<p>PI-RCNN: An Efficient Multi-Sensor 3D Object Detector with Point-Based Attentive Cont-Conv Fusion Module <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6933/6787">(AAAI 20)</a></p>
</li>
<li>
<p>CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341791">IROS 20</a></p>
</li>
<li>
<p>LRPD: Long Range 3D Pedestrian Detection Leveraging Specific Strengths of LiDAR and RGB <a href="https://arxiv.org/pdf/2006.09738.pdf">(ITSC 20)</a></p>
</li>
<li>
<p>Fusion of 3D LIDAR and Camera Data for Object Detection in Autonomous Vehicle Applications <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957313">(Sensors Journal 20)</a></p>
</li>
<li>
<p>SemanticVoxels: Sequential Fusion for 3D Pedestrian Detection using LiDAR Point Cloud and Semantic Segmentation <a href="https://arxiv.org/pdf/2009.12276.pdf">(MFI 20)</a></p>
</li>
</ul>
<h2 id="2019-3">2019<a hidden class="anchor" aria-hidden="true" href="#2019-3">#</a></h2>
<ul>
<li>
<p>Multi-Task Multi-Sensor Fusion for 3D Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Multi-Task_Multi-Sensor_Fusion_for_3D_Object_Detection_CVPR_2019_paper.pdf">(CVPR 19)</a></p>
</li>
<li>
<p>Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Simon_Complexer-YOLO_Real-Time_3D_Object_Detection_and_Tracking_on_Semantic_Point_CVPRW_2019_paper.pdf">(CVPRW 19)</a></p>
</li>
<li>
<p>Sensor Fusion for Joint 3D Object Detection and Semantic Segmentation <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Meyer_Sensor_Fusion_for_Joint_3D_Object_Detection_and_Semantic_Segmentation_CVPRW_2019_paper.pdf">(CVPRW 19)</a></p>
</li>
<li>
<p>MVX-Net: Multimodal VoxelNet for 3D Object Detection <a href="https://arxiv.org/pdf/1904.01649.pdf">(ICRA 19)</a></p>
</li>
<li>
<p>SEG-VoxelNet for 3D Vehicle Detection from RGB and LiDAR Data <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793492">(ICRA 19)</a></p>
</li>
<li>
<p>3D Object Detection Using Scale Invariant and Feature Reweighting Networks <a href="https://ojs.aaai.org/index.php/AAAI/article/download/4963/4836">(AAAI 19)</a></p>
</li>
<li>
<p>Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968513">(IROS 19)</a></p>
</li>
<li>
<p>Deep End-to-end 3D Person Detection from Camera and Lidar <a href="http://pure.tudelft.nl/ws/portalfiles/portal/68940754/roth2019itsc_lidar_person_detection.pdf">(ITSC 19)</a></p>
</li>
<li>
<p>RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813895">(IV 19)</a></p>
</li>
<li>
<p>SCANet: Spatial-channel attention network for 3D object detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682746">(ICASSP 19)</a></p>
</li>
<li>
<p>One-Stage Multi-Sensor Data Fusion Convolutional Neural Network for 3D Object Detection <a href="https://www.mdpi.com/1424-8220/19/6/1434/pdf">(Sensors 19)</a></p>
</li>
</ul>
<h2 id="2018-3">2018<a hidden class="anchor" aria-hidden="true" href="#2018-3">#</a></h2>
<ul>
<li>
<p>Frustum PointNets for 3D Object Detection from RGB-D Data <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf">(CVPR 18)</a></p>
</li>
<li>
<p>PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf">(CVPR 18)</a></p>
</li>
<li>
<p>Deep Continuous Fusion for Multi-Sensor 3D Object Detection <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf">(ECCV 18)</a></p>
</li>
<li>
<p>Joint 3D Proposal Generation and Object Detection from View Aggregation <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594049">(IROS 18)</a></p>
</li>
<li>
<p>A General Pipeline for 3D Detection of Vehicles <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461232">(ICRA 18)</a></p>
</li>
<li>
<p>Fusing Bird’s Eye View LIDAR Point Cloud and Front View Camera Image for 3D Object Detection <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8500387">(IV 18)</a></p>
</li>
<li>
<p>Robust Camera Lidar Sensor Fusion Via Deep Gated Information Fusion Network <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500711">(IV 18)</a></p>
</li>
</ul>
<h2 id="2017-or-earlier-1">2017 or earlier<a hidden class="anchor" aria-hidden="true" href="#2017-or-earlier-1">#</a></h2>
<ul>
<li>Multi-View 3D Object Detection Network for Autonomous Driving <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf">(CVPR 17)</a></li>
</ul>
<p>Tried to dive into code base <a href="https://github.com/zhulf0804/PointPillars">PointPillars</a> later on (below).</p>
<h1 id="315-pointpillars--follow-up-related-to-clip">3.15 PointPillars &amp; Follow-up related to CLIP<a hidden class="anchor" aria-hidden="true" href="#315-pointpillars--follow-up-related-to-clip">#</a></h1>
<h3 id="1-pointpillars">1. PointPillars<a hidden class="anchor" aria-hidden="true" href="#1-pointpillars">#</a></h3>
<ul>
<li>Setup the environment for PointPillars. The code base doesn&rsquo;t need any of the 3D Object Detection framework like OpenPCDet or MMDetection3d.</li>
<li>Figure out the network architecture of PointPillars by looking into the code except for the network forwarding (left for tomorrow).</li>
<li>The following thing to be examined will be the generation of the GT box, loss function, training, and evaluation of the model.</li>
</ul>
<h3 id="2-open-vocabulary-segmentation-paper-review">2. Open Vocabulary Segmentation paper review<a hidden class="anchor" aria-hidden="true" href="#2-open-vocabulary-segmentation-paper-review">#</a></h3>
<ul>
<li><a href="https://arxiv.org/abs/2201.03546">Language-driven Semantic Segmentation</a> (Lseg)
<ul>
<li>Advantange:
<ul>
<li>Accomplished using CLIP to do OVS.</li>
</ul>
</li>
<li>Downside:
<ul>
<li>Still a supervised learning method.</li>
<li>Accuracy is still uncomparable as compared to 1-shot method.</li>
<li>Didn&rsquo;t use the text as a supervised signal, meaning that the model is still relying on manual annotated mask to accomplish segmentation.</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2202.11094">GroupViT: Semantic Segmentation Emerges from Text Supervision</a> (GroupViT)
<ul>
<li>Advantage:
<ul>
<li>Added Grouping Block (cluster the group) and learnable Group tokens (clustering centers).</li>
</ul>
</li>
<li>Downside:
<ul>
<li>Still a patch-level segmentation. The final output uses bilinear interpolation.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-comparison-between-different-frameworks-of-3d-object-detection">3. Comparison between different frameworks of 3D object detection<a hidden class="anchor" aria-hidden="true" href="#3-comparison-between-different-frameworks-of-3d-object-detection">#</a></h3>
<h3 id="openpcdet">OpenPCDet<a hidden class="anchor" aria-hidden="true" href="#openpcdet">#</a></h3>
<ul>
<li>
<p><strong>GitHub:</strong> <a href="https://github.com/open-mmlab/OpenPCDet">OpenPCDet</a></p>
</li>
<li>
<p><strong>Community:</strong> Active</p>
</li>
<li>
<p><strong>Code Quality:</strong> Lightweight and readable. Some top journal papers in recent years were developed based on this framework.</p>
</li>
<li>
<p><strong>Deployment</strong>:</p>
<p>More convenient than other frameworks, with existing deployment implementations available. Example deployment solutions include:</p>
<ul>
<li><a href="https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars">CUDA-PointPillars</a></li>
<li><a href="https://github.com/hova88/PointPillars_MultiHead_40FPS">PointPillars_MultiHead_40FPS</a></li>
<li><a href="https://github.com/hova88/OpenPCDet">OpenPCDet modified by hova88</a></li>
</ul>
</li>
<li>
<p><strong>Recommendation:</strong> A recommended starting point for beginners interested in learning about object detection frameworks.</p>
</li>
</ul>
<h3 id="mmdetection3d">mmdetection3d<a hidden class="anchor" aria-hidden="true" href="#mmdetection3d">#</a></h3>
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/open-mmlab/mmdetection3d">mmdetection3d</a></li>
<li><strong>Community:</strong> Active</li>
<li><strong>Documentation:</strong> Official documentation available, facilitating easier onboarding.</li>
<li><strong>Scope:</strong> Compared to OpenPCDet, mmdetection3d encompasses a broader range of scenarios, including 3D detection and segmentation for images, point clouds, and multimodal data sources.</li>
<li><strong>Code Quality:</strong> Well-packaged, but might be more challenging for beginners. The framework is also the basis for some top journal papers in recent years.</li>
<li><strong>Model Deployment:</strong> Still in experimental phase.</li>
<li><strong>Recommendation:</strong> Suitable for those familiar with 3D object detection, offering richer content for further learning.</li>
</ul>
<h3 id="det3d">Det3D<a hidden class="anchor" aria-hidden="true" href="#det3d">#</a></h3>
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/poodarchu/Det3D">Det3D</a></li>
<li><strong>Community Feedback:</strong> Previously reviewed but not extensively used in recent developments. Similar to OpenPCDet in being lightweight, but has not been updated recently.</li>
<li><strong>Deployment Solutions:</strong> Some existing deployment solutions are based on this framework, such as CenterPoint&rsquo;s deployment: <a href="https://github.com/CarkusL/CenterPoint">CenterPoint Deployment</a></li>
<li><strong>Recommendation:</strong> Lower priority for learning compared to OpenPCDet and mmdetection3d.</li>
</ul>
<h3 id="paddle3d">Paddle3D<a hidden class="anchor" aria-hidden="true" href="#paddle3d">#</a></h3>
<ul>
<li><strong>GitHub:</strong> <a href="https://github.com/PaddlePaddle/Paddle3D">Paddle3D</a></li>
<li><strong>Community Feedback:</strong> Newly open-sourced as of August 2022, with unclear performance outcomes at the moment.</li>
</ul>
<p>After the review, I plan to get on OpenPCDet first for its ease of understanding.</p>
<h1 id="316-pointpillars">3.16 PointPillars<a hidden class="anchor" aria-hidden="true" href="#316-pointpillars">#</a></h1>
<ul>
<li>Learned how anchor is generated and aligned with the GT boxes.</li>
<li>Loss function computation is also understood, which uses Focal Loss for <code>&lt;cls&gt;</code> loss, and smoothL1 for regression loss.</li>
<li>The training process is figured out, which uses <code>torch.optim.AdamW()</code> as the optimizer and <code>torch.optim.lr_scheduler.OneCycleLR()</code> as the scheduler.</li>
<li>The following things left to be done is the prediction for a single cloud points (involves <code>NMS</code>), the visualization step, and the metrics and evaluation methods used in 3D object detection area.</li>
<li>Ran KITTI benchmark on the model to check the accuracy.
<img loading="lazy" src="../assets/iShot_2024-03-20_16.33.08.png" alt="benchmark"  />
</li>
</ul>
<h1 id="317-quantization--swin-transformer">3.17 Quantization &amp; Swin-transformer<a hidden class="anchor" aria-hidden="true" href="#317-quantization--swin-transformer">#</a></h1>
<h3 id="1-quantization-concept-apart-from-pytorch">1. Quantization concept (apart from Pytorch)<a hidden class="anchor" aria-hidden="true" href="#1-quantization-concept-apart-from-pytorch">#</a></h3>
<ul>
<li>
<p>Linear:</p>
<ul>
<li>
<p>Affine: the importance of Z offset.</p>
<p><img loading="lazy" src="../assets/image-20240320135437695.png" alt="image-20240320135437695"  />
</p>
<ul>
<li>int8 ( -128 ~ 127 )</li>
<li>uint8 ( 0 ~ 255 )</li>
<li>Minmax (above)</li>
<li>Histogram (two arrows shrink towards each other until below or equal to the required coverage percentage)
<img loading="lazy" src="../assets/image-20240320135842827.png" alt="image-20240320135842827"  />
</li>
<li>Entropy: <strong>TO BE CONTINUED</strong>&hellip;</li>
</ul>
</li>
<li>
<p>Symmetric: more concise as compared to asymmetric method.</p>
<p><img loading="lazy" src="assets/image-20240320135639201.png" alt="image-20240320135639201"  />
</p>
</li>
</ul>
</li>
<li>
<p>Non-linear</p>
</li>
</ul>
<h3 id="2-swin-transformerhttpsarxivorgabs210314030">2. <a href="https://arxiv.org/abs/2103.14030">Swin-transformer</a>:<a hidden class="anchor" aria-hidden="true" href="#2-swin-transformerhttpsarxivorgabs210314030">#</a></h3>
<ul>
<li>Used the concept of conv to let the attention be focused first only on local part and then global perspective
<img loading="lazy" src="../assets/image-20240320134821271.png" alt="image-20240320134821271"  />
</li>
<li>The architecture of the network is:
<img loading="lazy" src="../assets/image-20240320134904678.png" alt="image-20240320134904678"  />
</li>
<li>Completely gone through the code base of <a href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/swin_transformer">Swin-transformer</a>.</li>
</ul>
<h1 id="318-quantization-about-matrix-multiplication">3.18 Quantization about Matrix Multiplication<a hidden class="anchor" aria-hidden="true" href="#318-quantization-about-matrix-multiplication">#</a></h1>
<h3 id="1-im2col">1. im2col<a hidden class="anchor" aria-hidden="true" href="#1-im2col">#</a></h3>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>(3 channels)</p>
<!-- raw HTML omitted -->
<p>(3 kernals, transform to below)</p>
<!-- raw HTML omitted -->
<p>Therefore, conv operation is successfully transformed into matrix multiplication (one kernal):</p>
<p><img loading="lazy" src="../assets/image-20240320140657978.png" alt="image-20240320140657978"  />
</p>
<p>The case for three kernel is the same:
<!-- raw HTML omitted --></p>
<h3 id="2-quantization-in-matrix-multiplication-convhttpsmpweixinqqcoms__bizmzg3odu2mzy5ma3d3dmid2247488318idx1sn048c1b78f3b2cb25c05abb115f20d6c6chksmcf108b3bf867022d1b214928102d65ed691c81955b59ca02bccdee92584ad9aa8e390e1d2978token1388685340langzh_cnrd">2. <a href="https://mp.weixin.qq.com/s?__biz=Mzg3ODU2MzY5MA%3D%3D&amp;mid=2247488318&amp;idx=1&amp;sn=048c1b78f3b2cb25c05abb115f20d6c6&amp;chksm=cf108b3bf867022d1b214928102d65ed691c81955b59ca02bccdee92584ad9aa8e390e1d2978&amp;token=1388685340&amp;lang=zh_CN#rd">Quantization in matrix multiplication (conv)</a><a hidden class="anchor" aria-hidden="true" href="#2-quantization-in-matrix-multiplication-convhttpsmpweixinqqcoms__bizmzg3odu2mzy5ma3d3dmid2247488318idx1sn048c1b78f3b2cb25c05abb115f20d6c6chksmcf108b3bf867022d1b214928102d65ed691c81955b59ca02bccdee92584ad9aa8e390e1d2978token1388685340langzh_cnrd">#</a></h3>
<p><img loading="lazy" src="../assets/image-20240320140913509.png" alt="image-20240320140913509"  />
</p>
<p><img loading="lazy" src="../assets/image-20240320140932612.png" alt="image-20240320140932612"  />
</p>
<p><img loading="lazy" src="../assets/image-20240320140948137.png" alt="image-20240320140948137"  />
</p>
<p><img loading="lazy" src="../assets/image-20240320141000040.png" alt="image-20240320141000040"  />
</p>
<p>As we can see, in order to leverage GEMM(INT8) acceleration, we must kill the variable <code>k</code> in <code>s</code>.</p>
<p><img loading="lazy" src="../assets/image-20240320141114105.png" alt="image-20240320141114105"  />
</p>
<p>If we kill <code>k</code> in <code>s</code>, then on the right hand side, it&rsquo;s just basic INT8 matrix multiplication with the left hand side is the scale. Killing the <code>k</code> also means that the scale <code>s</code> will be shared across row respective to <code>X</code> and across column respective to <code>W</code>:</p>
<p><img loading="lazy" src="../assets/image-20240320141211703.png" alt="image-20240320141211703"  />
</p>
<p>Then <code>per-channel</code> is easy to explain:</p>
<p><img loading="lazy" src="../assets/image-20240320141423724.png" alt="image-20240320141423724"  />
</p>
<h1 id="319-quantization-with-code">3.19 Quantization with code<a hidden class="anchor" aria-hidden="true" href="#319-quantization-with-code">#</a></h1>
<h3 id="1-different-methods">1. Different methods<a hidden class="anchor" aria-hidden="true" href="#1-different-methods">#</a></h3>
<ul>
<li>
<p><strong>Dynamic Quantization</strong>:
The easiest method of quantization PyTorch supports is called <strong>dynamic quantization</strong>. This involves not just converting the weights to int8 - as happens in all quantization variants - but also converting the activations to int8 on the fly, just before doing the computation (hence “dynamic”).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.quantization
</span></span><span style="display:flex;"><span>quantized_model <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>quantize_dynamic(model, {torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear}, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>qint8)
</span></span></code></pre></div></li>
</ul>
<p>Additional adjustment may be involved, like replace the original <code>add</code> and <code>concat</code> operation with <code>nn.quantized.FloatFunctional</code></p>
<ul>
<li>
<p><strong>Post-Training Static Quantization</strong>:</p>
</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<p>Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting “observer” modules at different points that record these distributions). This information is used to determine how specifically the different activations should be quantized at inference time</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># set quantization config for server (x86)</span>
</span></span><span style="display:flex;"><span>deploymentmyModel<span style="color:#f92672">.</span>qconfig <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>get_default_config(<span style="color:#e6db74">&#39;fbgemm&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># insert observers</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare(myModel, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calibrate the model and collect statistics</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># convert to quantized version</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(myModel, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Quantization Aware Training</strong>:</p>
</li>
<li>
<!-- raw HTML omitted -->
</li>
<li>
<p>Quantization-aware training(QAT) is the third method, and the one that typically results in highest accuracy of these three. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># specify quantization config for QAT</span>
</span></span><span style="display:flex;"><span>qat_model<span style="color:#f92672">.</span>qconfig<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>get_default_qat_qconfig(<span style="color:#e6db74">&#39;fbgemm&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># prepare QAT</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare_qat(qat_model, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># convert to quantized version, removing dropout, to check for accuracy on each</span>
</span></span><span style="display:flex;"><span>epochquantized_model<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(qat_model<span style="color:#f92672">.</span>eval(), inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><p>During the re-training process, we&rsquo;ll forward the loss of the mimic int8 values and the true values to the optimizer, so that this quantization loss will be optimized during re-training.</p>
<ul>
<li>Forward process: use one of the methods from MinMax, Histogram, and Entropy to mimic the quantization</li>
<li>Backward process: use smooth derivative method so that we can get the derivarive <strong>TO BE CONTINUED</strong>&hellip;</li>
</ul>
</li>
</ul>
<h3 id="2-different-frameworks">2. Different frameworks<a hidden class="anchor" aria-hidden="true" href="#2-different-frameworks">#</a></h3>
<h4 id="pytorch-quantization-from-nvidia-vs-torchaoquantization-from-native-pytorch"><code>pytorch-quantization</code> from Nvidia <strong>V.S.</strong> <code>torch.ao.quantization</code> from native Pytorch<a hidden class="anchor" aria-hidden="true" href="#pytorch-quantization-from-nvidia-vs-torchaoquantization-from-native-pytorch">#</a></h4>
<ul>
<li><code>torch.ao.quantization</code> aims primarily on CPU while <code>pytorch-quantization</code> focuses on Nvidia platform deployment.</li>
<li>Overall same procedure.</li>
<li>More people using the first one.</li>
<li><a href="https://discuss.pytorch.org/t/what-is-the-recommended-way-to-use-pytorch-naive-quantization-when-deploying-to-int8-tensorrt/197455">No one answers this question</a>.</li>
</ul>
<h1 id="things-that-i-plan-to-do-next-week">Things that I plan to do next week<a hidden class="anchor" aria-hidden="true" href="#things-that-i-plan-to-do-next-week">#</a></h1>
<h3 id="1-more-understanding-in-quantization">1. More understanding in quantization<a hidden class="anchor" aria-hidden="true" href="#1-more-understanding-in-quantization">#</a></h3>
<ul>
<li>Theoretical part:
<ul>
<li><a href="https://arxiv.org/abs/2106.08295">A White Paper on Neural Network Quantization</a></li>
<li><a href="https://arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></li>
</ul>
</li>
<li>Code Practice:
<ul>
<li>If <code>torch.ao.quantization</code>:
<ul>
<li><a href="https://pytorch.org/docs/stable/quantization.html">Official Docs</a></li>
<li><a href="https://pytorch.org/tutorials/search.html?q=quantization&amp;check_keywords=yes&amp;area=default">Official Tutorials</a></li>
</ul>
</li>
<li>else:
<ul>
<li><code>pytorch-quantization</code> module docs and tutorial.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-span-on-more-code-of-3d-object-detection-model">2. Span on more code of 3D object detection model<a hidden class="anchor" aria-hidden="true" href="#2-span-on-more-code-of-3d-object-detection-model">#</a></h3>
<ul>
<li>Complete the review of code for PointPillars&rsquo; predict and visualization process (link above).</li>
<li><a href="https://arxiv.org/abs/2203.17270">BevFormer</a></li>
<li><a href="https://arxiv.org/abs/1904.08189">CenterNet</a> and <a href="https://arxiv.org/abs/2006.11275">CenterPoint</a></li>
<li>and more&hellip;</li>
</ul>
<h3 id="3-empirical-experiment-on-quantization-on-3d-object-detection-model">3. Empirical experiment on quantization on 3d object detection model<a hidden class="anchor" aria-hidden="true" href="#3-empirical-experiment-on-quantization-on-3d-object-detection-model">#</a></h3>
<p>Highly possible that the performance will not be that good on PointPillars since:</p>
<p><img loading="lazy" src="../assets/image-20240320161318119.png" alt="image-20240320161318119"  />
</p>
<p>by <a href="https://openreview.net/forum?id=0d1gQI114C">LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection</a>.</p>
<p>If bad, I&rsquo;ll go understand the essay to explore why.</p>
<p>Apply all three types of methods on these models, check the performance and sort out why.</p>
<h2 id="by-next-week">By next week<a hidden class="anchor" aria-hidden="true" href="#by-next-week">#</a></h2>
<ul>
<li>Get CenterPoint run on evaluation.</li>
<li>Quantize CenterPoint using the three methods to check the performance as compared to un-quantized one:
<ul>
<li>Latency for each inference.</li>
<li>Use CUDA memory API to calculate total memory usage by the model</li>
<li>Either calculate the activation memory usage by hand or use some other tools</li>
</ul>
</li>
<li>After all these, try apply SmoothQuant to the model and see the result.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://banghao.live/tags/deep-learning/">Deep Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://banghao.live/blog/catseg/">
    <span class="title">Next »</span>
    <br>
    <span>CATseg: A complete walk through of the model architecture</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://banghao.live/">Banghao&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
