<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<script
  src="/blog/js/theme-toggle.min.3de3c47734bc776a4afab25ec9e41354fbca4c6cded1dbf7e15121cfaefe6350.js"
  integrity="sha256-PePEdzS8d2pK&#43;rJeyeQTVPvKTGze0dv34VEhz67&#43;Y1A="
></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://biboyqg.github.io/blog/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://biboyqg.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/" title="Home Page">
                    <span>Home Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Quantization on CenterPoint
    </h1>
    <div class="post-meta"><span title='2024-04-01 16:32:18 -0500 -0500'>April 1, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h3 id="take-mmdetection-as-an-example">Take <code>mmdetection</code> as an example<a hidden class="anchor" aria-hidden="true" href="#take-mmdetection-as-an-example">#</a></h3>
<ol>
<li>First find the <code>Runner</code> class:</li>
</ol>
<p><img loading="lazy" src="https://s2.loli.net/2024/04/03/VZ14NHWtB6afnFp.png" alt="image-20240403083721183"  />
</p>
<p>This is the place where the build of the model is completed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> __init__(<span style="color:#f92672">...</span>):
</span></span><span style="display:flex;"><span>      	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>      	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>build_model(model)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># wrap model</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wrap_model(
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>cfg<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;model_wrapper_cfg&#39;</span>), self<span style="color:#f92672">.</span>model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># get model name from the model class</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> hasattr(self<span style="color:#f92672">.</span>model, <span style="color:#e6db74">&#39;module&#39;</span>):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>_model_name <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>module<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>_model_name <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span></code></pre></div><ol start="2">
<li>Learn about how <code>pytorch-quantization</code> works by diving into its source code:</li>
</ol>
<p><img loading="lazy" src="https://s2.loli.net/2024/04/04/R9mewBVDyoMdsWr.png" alt="image-20240403171605404"  />
</p>
<ol start="3">
<li>Code about the quantization function respect to a specific Pytorch model as input:</li>
</ol>
<p><code>quant_utils.py</code>: a utils file to provide helper functions to <code>ptq.py</code> interfaces.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> yaml
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> collections
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> quant_modules
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization.nn.modules <span style="color:#f92672">import</span> _utils <span style="color:#66d9ef">as</span> quant_nn_utils
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> calib
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> nn <span style="color:#66d9ef">as</span> quant_nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization.tensor_quant <span style="color:#f92672">import</span> QuantDescriptor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> absl <span style="color:#f92672">import</span> logging <span style="color:#66d9ef">as</span> quant_logging
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(config, weight, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># intput QuantDescriptor: Max or Histogram</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calib_method -&gt; [&#34;max&#34;, &#34;histogram&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize</span>(calib_method: str):
</span></span><span style="display:flex;"><span>    quant_desc_input <span style="color:#f92672">=</span> QuantDescriptor(calib_method<span style="color:#f92672">=</span>calib_method)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantConv2d<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantMaxPool2d<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantLinear<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_logging<span style="color:#f92672">.</span>set_verbosity(quant_logging<span style="color:#f92672">.</span>ERROR)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_model</span>(config, weight, device, calib_method):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># quant_modules.initialize() &lt;- the method that automatically quantizes the model</span>
</span></span><span style="display:flex;"><span>    initialize(calib_method)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> load_model(config, weight, device)
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transfer_torch_to_quantization</span>(nn_instance, quant_mudule):
</span></span><span style="display:flex;"><span>    quant_instance <span style="color:#f92672">=</span> quant_mudule<span style="color:#f92672">.</span>__new__(quant_mudule)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, val <span style="color:#f92672">in</span> vars(nn_instance)<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        setattr(quant_instance, k, val)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Return two instances of QuantDescriptor; self.__class__ is the class of quant_instance, E.g.: QuantConv2d</span>
</span></span><span style="display:flex;"><span>        quant_desc_input, quant_desc_weight <span style="color:#f92672">=</span> quant_nn_utils<span style="color:#f92672">.</span>pop_quant_desc_in_kwargs(self<span style="color:#f92672">.</span>__class__)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(self, quant_nn_utils<span style="color:#f92672">.</span>QuantInputMixin):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>init_quantizer(quant_desc_input)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>HistogramCalibrator):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>init_quantizer(quant_desc_input, quant_desc_weight)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>HistogramCalibrator):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_weight_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    __init__(quant_instance)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quant_instance
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">quantization_ignore_match</span>(ignore_layer, path):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> ignore_layer <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> isinstance(ignore_layer, str) <span style="color:#f92672">or</span> isinstance(ignore_layer, list):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(ignore_layer, str):
</span></span><span style="display:flex;"><span>            ignore_layer <span style="color:#f92672">=</span> [ignore_layer]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> path <span style="color:#f92672">in</span> ignore_layer:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> ignore_layer:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> re<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(item, path):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iterative method</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">torch_module_find_quant_module</span>(module, module_dict, ignore_layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> module<span style="color:#f92672">.</span>_modules:
</span></span><span style="display:flex;"><span>        submodule <span style="color:#f92672">=</span> module<span style="color:#f92672">.</span>_modules[name]
</span></span><span style="display:flex;"><span>        path <span style="color:#f92672">=</span> name
</span></span><span style="display:flex;"><span>        torch_module_find_quant_module(submodule, module_dict, ignore_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        submodule_id <span style="color:#f92672">=</span> id(type(submodule))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> submodule_id <span style="color:#f92672">in</span> module_dict:
</span></span><span style="display:flex;"><span>            ignored <span style="color:#f92672">=</span> quantization_ignore_match(ignore_layer, path)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> ignored:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Quantization : </span><span style="color:#e6db74">{</span>path<span style="color:#e6db74">}</span><span style="color:#e6db74"> has ignored.&#34;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># substitute the layer with quantized version</span>
</span></span><span style="display:flex;"><span>            module<span style="color:#f92672">.</span>_modules[name] <span style="color:#f92672">=</span> transfer_torch_to_quantization(submodule, module_dict[submodule_id])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace_to_quantization_model</span>(model, ignore_layer<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    module_dict <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> entry <span style="color:#f92672">in</span> quant_modules<span style="color:#f92672">.</span>_DEFAULT_QUANT_MAP:
</span></span><span style="display:flex;"><span>        module <span style="color:#f92672">=</span> getattr(entry<span style="color:#f92672">.</span>orig_mod, entry<span style="color:#f92672">.</span>mod_name)
</span></span><span style="display:flex;"><span>        module_dict[id(module)] <span style="color:#f92672">=</span> entry<span style="color:#f92672">.</span>replace_mod
</span></span><span style="display:flex;"><span>    torch_module_find_quant_module(model, module_dict, ignore_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_dataloader</span>(dir, batch_size, is_train):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_val_dataset</span>(dir, batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    dataloader <span style="color:#f92672">=</span> create_dataloader(dir, batch_size, is_train)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dataloader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_train_dataset</span>(dir, batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>    dataloader <span style="color:#f92672">=</span> create_dataloader(dir, batch_size, is_train)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dataloader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(save_dir, model, dataloader):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(model, loader, save_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> save_dir <span style="color:#f92672">and</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(save_dir) <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;&#34;</span>:
</span></span><span style="display:flex;"><span>        os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(save_dir), exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> test(save_dir, model, loader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">collect_stats</span>(model, data_loader, device, num_batch<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Enable calibrator</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable_quant()
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable_calib()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># test</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, datas <span style="color:#f92672">in</span> enumerate(data_loader):
</span></span><span style="display:flex;"><span>            data <span style="color:#f92672">=</span> datas[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device)<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>            model(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> num_batch:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Disable calibrator</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable_quant()
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable_calib()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_amax</span>(model, device, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> isinstance(module<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>MaxCalibrator):
</span></span><span style="display:flex;"><span>                    module<span style="color:#f92672">.</span>load_calib_amax()
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                    module<span style="color:#f92672">.</span>load_calib_amax(<span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_amax <span style="color:#f92672">=</span> module<span style="color:#f92672">.</span>_amax<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># method parameter designed for &#34;histogram&#34;, method in [&#39;entropy&#39;, &#39;mse&#39;, &#39;percentile&#39;]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calibrate_model</span>(model, dataloader, device, method):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Collect stats with data flowing</span>
</span></span><span style="display:flex;"><span>    collect_stats(model, dataloader, device)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get dynamic range and compute amax (used in calibration)</span>
</span></span><span style="display:flex;"><span>    compute_amax(model, device, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">export_ptq</span>(model, save_file, device, dynamic_batch<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    input_dummy <span style="color:#f92672">=</span> <span style="color:#f92672">...</span>(device)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>TensorQuantizer<span style="color:#f92672">.</span>use_fb_fake_quant <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(model, input_dummy, save_file, opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">13</span>,
</span></span><span style="display:flex;"><span>                          input_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input&#39;</span>], output_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;output&#39;</span>],
</span></span><span style="display:flex;"><span>                          dynamic_axes<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;input&#39;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#39;batch&#39;</span>}, <span style="color:#e6db74">&#39;output&#39;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#39;batch&#39;</span>}} <span style="color:#66d9ef">if</span> dynamic_batch <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                          )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>TensorQuantizer<span style="color:#f92672">.</span>use_fb_fake_quant <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># To test if there&#39;s any quantized layer</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">have_quantizer</span>(layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> layer<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Disable quantization (to a specific layer)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">disable_quantization</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># init</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Disable quantization</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply</span>(self, disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_disabled <span style="color:#f92672">=</span> disabled
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __enter__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __exit__(self, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Enable quantization (to a specific layer)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">enable_quantization</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply</span>(self, enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_disabled <span style="color:#f92672">=</span> <span style="color:#f92672">not</span> enabled
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __enter__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __exit__(self, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Saving log</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SummaryTool</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, file):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>file <span style="color:#f92672">=</span> file
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">append</span>(self, item):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>append(item)
</span></span><span style="display:flex;"><span>        json<span style="color:#f92672">.</span>dump(self<span style="color:#f92672">.</span>data, open(self<span style="color:#f92672">.</span>file, <span style="color:#e6db74">&#34;w&#34;</span>), indent<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sensitive_analysis</span>(model, loader, summary_file):
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> SummaryTool(summary_file)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># for loop to iterate every layer</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Sensitive analysis by each layer....&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(list(model<span style="color:#f92672">.</span>modules()))):
</span></span><span style="display:flex;"><span>        layer <span style="color:#f92672">=</span> list(model<span style="color:#f92672">.</span>modules())[i]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># tell if layer has quantized layer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> have_quantizer(layer): <span style="color:#75715e"># if so</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># disable the layer</span>
</span></span><span style="display:flex;"><span>            disable_quantization(layer)<span style="color:#f92672">.</span>apply()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># calculate map</span>
</span></span><span style="display:flex;"><span>            ap <span style="color:#f92672">=</span> evaluate(model, loader)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># save ap in json</span>
</span></span><span style="display:flex;"><span>            summary<span style="color:#f92672">.</span>append([ap, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;model.</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>])
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># enable back the layer</span>
</span></span><span style="display:flex;"><span>            enable_quantization(layer)<span style="color:#f92672">.</span>apply()
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;layer </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> ap: </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;ignore model.</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> because it is </span><span style="color:#e6db74">{</span>type(layer)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># after the iteration, print the top 10 worst quantized layer and save to log</span>
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> sorted(summary<span style="color:#f92672">.</span>data, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">0</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Sensitive Summary: &#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> n, (ap, name) <span style="color:#f92672">in</span> enumerate(summary[:<span style="color:#ae81ff">10</span>]):
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">: Using fp16 </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">, ap = </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">:</span><span style="color:#e6db74">.5f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        summary<span style="color:#f92672">.</span>append([name, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">: Using fp16 </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">, ap = </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">:</span><span style="color:#e6db74">.5f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>])
</span></span></code></pre></div><p><code>ptq.py</code>: an interface that utilizes the helper/utils functions</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> quant_utils <span style="color:#66d9ef">as</span> quantize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> argparse
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_SensitiveAnalysis</span>(config, weight, dir, calib_method, hist_method, device):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Model ....&#34;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_model(config, weight, device, calib_method)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>replace_to_quantization_model(model)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare dataset</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Dataset ....&#34;</span>)
</span></span><span style="display:flex;"><span>    train_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_train_dataset(dir, args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    val_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_val_dataset(dir, args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calibration model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Begining Calibration ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>calibrate_model(model, train_dataloader, device, hist_method)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sensitive analysis</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Begining Sensitive Analysis ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>sensitive_analysis(model, val_dataloader, args<span style="color:#f92672">.</span>sensitive_summary)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_PTQ</span>(args, device):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Model ....&#34;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_model(args<span style="color:#f92672">.</span>config, args<span style="color:#f92672">.</span>weights, device, args<span style="color:#f92672">.</span>calib_method)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>replace_to_quantization_model(model, args<span style="color:#f92672">.</span>ignore_layers)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare dataset</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Dataset ....&#34;</span>)
</span></span><span style="display:flex;"><span>    val_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_val_dataset(args<span style="color:#f92672">.</span>cocodir, batch_size<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    train_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_train_dataset(args<span style="color:#f92672">.</span>cocodir, batch_size<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calibrate model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Beginning Calibration ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>calibrate_model(model, train_dataloader, device, args<span style="color:#f92672">.</span>hist_method)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>SummaryTool(args<span style="color:#f92672">.</span>ptq_summary)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>eval_origin:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Evaluate Origin...&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> quantize<span style="color:#f92672">.</span>disable_quantization(model):
</span></span><span style="display:flex;"><span>            ap <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>evaluate(model, val_dataloader)
</span></span><span style="display:flex;"><span>            summary<span style="color:#f92672">.</span>append([<span style="color:#e6db74">&#34;Origin&#34;</span>, ap])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>eval_ptq:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Evaluate PTQ...&#34;</span>)
</span></span><span style="display:flex;"><span>        ap <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>evaluate(model, val_dataloader)
</span></span><span style="display:flex;"><span>        summary<span style="color:#f92672">.</span>append([<span style="color:#e6db74">&#34;PTQ&#34;</span>, ap])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>save_ptq:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Export PTQ...&#34;</span>)
</span></span><span style="display:flex;"><span>        quantize<span style="color:#f92672">.</span>export_ptq(model, args<span style="color:#f92672">.</span>ptq, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    parser <span style="color:#f92672">=</span> argparse<span style="color:#f92672">.</span>ArgumentParser()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--config&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;configs/...&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;model config file&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--weights&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;initial weights path&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--dir&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dataset/...&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dataset directory&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--batch_size&#39;</span>, type<span style="color:#f92672">=</span>int, default<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;batch size for data loader&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--device&#39;</span>, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;0&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda device, i.e. 0 or 0,1,2,3 or cpu&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--hist_method&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>, choices<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;entropy&#39;</span>, <span style="color:#e6db74">&#39;mse&#39;</span>, <span style="color:#e6db74">&#39;percentile&#39;</span>], help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;histogram calibration methods used in histogram, one of [&#34;entropy&#34;, &#34;mse&#34;, &#34;percentile&#34;]&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--calib_method&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;histogram&#39;</span>, choices<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;max&#34;</span>, <span style="color:#e6db74">&#34;histogram&#34;</span>], help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;calibration methods used in histogram, one of [&#34;max&#34;, &#34;histogram&#34;]&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--sensitive&#39;</span>, type<span style="color:#f92672">=</span>bool, default<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;use sensitive analysis or not before ptq&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--sensitive_summary&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sensitive-summary.json&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;summary save file&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ignore_layers&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;model\.105\.m\.(.*)&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;regx&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--save_ptq&#34;</span>, type<span style="color:#f92672">=</span>bool, default<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;file&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ptq&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ptq_centerpoint.onnx&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;file&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--eval_origin&#34;</span>, action<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;store_true&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;do eval for origin model&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--eval_ptq&#34;</span>, action<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;store_true&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;do eval for ptq model&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ptq_summary&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ptq_summary.json&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;summary save file&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    args <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse_args()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    is_cuda <span style="color:#f92672">=</span> (args<span style="color:#f92672">.</span>device <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;cpu&#39;</span>) <span style="color:#f92672">and</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available()
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> is_cuda <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sensitive analysis</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>sensitive:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Sensitive Analysis....&#34;</span>)
</span></span><span style="display:flex;"><span>        run_SensitiveAnalysis(args<span style="color:#f92672">.</span>config, args<span style="color:#f92672">.</span>weights, args<span style="color:#f92672">.</span>dir, args<span style="color:#f92672">.</span>calib_method, args<span style="color:#f92672">.</span>hist_method, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># PTQ</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ignore_layers= [&#34;model\.105\.m\.(.*)&#34;, model\.99\.m\.(.*)]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># args.ignore_layer = ignore_layers</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Beginning PTQ.....&#34;</span>)
</span></span><span style="display:flex;"><span>    run_PTQ(args, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;PTQ Quantization Has Finished....&#34;</span>)
</span></span></code></pre></div><p>It can be noted that three functions in the <code>quant_utils.py</code> are not implemented:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(config, weight, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_dataloader</span>(dir, batch_size, is_train):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(save_dir, model, dataloader):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
</ul>
<p>This is because <code>MMDetection3D</code> is well-encapsulated:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;runner_type&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> cfg:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># build the default runner</span>
</span></span><span style="display:flex;"><span>        runner <span style="color:#f92672">=</span> Runner<span style="color:#f92672">.</span>from_cfg(cfg)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># build customized runner from the registry</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if &#39;runner_type&#39; is set in the cfg</span>
</span></span><span style="display:flex;"><span>        runner <span style="color:#f92672">=</span> RUNNERS<span style="color:#f92672">.</span>build(cfg)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># start testing</span>
</span></span><span style="display:flex;"><span>    runner<span style="color:#f92672">.</span>test()
</span></span></code></pre></div><p>And in the <code>test</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(self) <span style="color:#f92672">-&gt;</span> dict:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Launch test.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            dict: A dict of metrics on testing set.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_test_loop <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">RuntimeError</span>(
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;`self._test_loop` should not be None when calling test &#39;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;method. Please provide `test_dataloader`, `test_cfg` and &#39;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;`test_evaluator` arguments when initializing runner.&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_test_loop <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>build_test_loop(self<span style="color:#f92672">.</span>_test_loop)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>call_hook(<span style="color:#e6db74">&#39;before_run&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># make sure checkpoint-related hooks are triggered after `before_run`</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>load_or_resume()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        metrics <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>test_loop<span style="color:#f92672">.</span>run()  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>call_hook(<span style="color:#e6db74">&#39;after_run&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> metrics
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>In <code>self.build_test_loop(self._test_loop)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_test_loop</span>(self, loop: Union[BaseLoop, Dict]) <span style="color:#f92672">-&gt;</span> BaseLoop:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Build test loop.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Examples of ``loop``::
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            # `TestLoop` will be used
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop = dict()
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            # custom test loop
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop = dict(type=&#39;CustomTestLoop&#39;)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop (BaseLoop or dict): A test loop or a dict to build test loop.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                If ``loop`` is a test loop object, just returns itself.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            :obj:`BaseLoop`: Test loop object build from ``loop_cfg``.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(loop, BaseLoop):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> loop
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> <span style="color:#f92672">not</span> isinstance(loop, dict):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">TypeError</span>(
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;test_loop should be a Loop object or dict, but got </span><span style="color:#e6db74">{</span>loop<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loop_cfg <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(loop)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;type&#39;</span> <span style="color:#f92672">in</span> loop_cfg:
</span></span><span style="display:flex;"><span>            loop <span style="color:#f92672">=</span> LOOPS<span style="color:#f92672">.</span>build(
</span></span><span style="display:flex;"><span>                loop_cfg,
</span></span><span style="display:flex;"><span>                default_args<span style="color:#f92672">=</span>dict(
</span></span><span style="display:flex;"><span>                    runner<span style="color:#f92672">=</span>self,
</span></span><span style="display:flex;"><span>                    dataloader<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_dataloader,
</span></span><span style="display:flex;"><span>                    evaluator<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_evaluator))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            loop <span style="color:#f92672">=</span> TestLoop(
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">**</span>loop_cfg,
</span></span><span style="display:flex;"><span>                runner<span style="color:#f92672">=</span>self,
</span></span><span style="display:flex;"><span>                dataloader<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_dataloader,
</span></span><span style="display:flex;"><span>                evaluator<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_evaluator)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loop  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>&hellip;&hellip; so since this week&rsquo;s focus is on writing the <code>PTQ</code> API (above),</p>
<h3 id="by-next-week">By next week:<a hidden class="anchor" aria-hidden="true" href="#by-next-week">#</a></h3>
<ul>
<li>get model, dataloader definition (just need some more time).</li>
<li>or switch to <code>OpenPCDet</code> for simplicity.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://biboyqg.github.io/blog/tags/quantization/">Quantization</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/3d-object-detection/">3D Object Detection</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/centerpoint/">CenterPoint</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://biboyqg.github.io/blog/blog/camera/">
    <span class="title">« Prev</span>
    <br>
    <span>IoT-Enabled Home Security Camera</span>
  </a>
  <a class="next" href="https://biboyqg.github.io/blog/blog/log/">
    <span class="title">Next »</span>
    <br>
    <span>Daily Log</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    <footer class="footer">
  <span
    >&copy; 2025
    <a href="https://biboyqg.github.io/blog/">Banghao&#39;s Blog</a></span
  >
</footer>
</body>

</html>
