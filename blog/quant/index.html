<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Quantization on CenterPoint | Banghao&#39;s Blog</title>
<meta name="keywords" content="Quantization, 3D Object Detection, CenterPoint">
<meta name="description" content="Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:
class Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get(&#39;model_wrapper_cfg&#39;), self.model) # get model name from the model class if hasattr(self.model, &#39;module&#39;): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils.">
<meta name="author" content="Banghao Chi">
<link rel="canonical" href="https://biboyqg.github.io/blog/blog/quant/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.7ece0b63e4dea9482286a19834da3b9806c50613b62e3908ed11bcc65688c436.css" integrity="sha256-fs4LY&#43;TeqUgihqGYNNo7mAbFBhO2LjkI7RG8xlaIxDY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://biboyqg.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://biboyqg.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://biboyqg.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://biboyqg.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://biboyqg.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://biboyqg.github.io/blog/blog/quant/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Quantization on CenterPoint" />
<meta property="og:description" content="Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:
class Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get(&#39;model_wrapper_cfg&#39;), self.model) # get model name from the model class if hasattr(self.model, &#39;module&#39;): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://biboyqg.github.io/blog/blog/quant/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-04-01T16:32:18-05:00" />
<meta property="article:modified_time" content="2024-04-01T16:32:18-05:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Quantization on CenterPoint"/>
<meta name="twitter:description" content="Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:
class Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get(&#39;model_wrapper_cfg&#39;), self.model) # get model name from the model class if hasattr(self.model, &#39;module&#39;): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://biboyqg.github.io/blog/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Quantization on CenterPoint",
      "item": "https://biboyqg.github.io/blog/blog/quant/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Quantization on CenterPoint",
  "name": "Quantization on CenterPoint",
  "description": "Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:\nclass Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get(\u0026#39;model_wrapper_cfg\u0026#39;), self.model) # get model name from the model class if hasattr(self.model, \u0026#39;module\u0026#39;): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils.",
  "keywords": [
    "Quantization", "3D Object Detection", "CenterPoint"
  ],
  "articleBody": "Take mmdetection as an example First find the Runner class: This is the place where the build of the model is completed:\nclass Runner: def __init__(...): ... ... self.model = self.build_model(model) # wrap model self.model = self.wrap_model( self.cfg.get('model_wrapper_cfg'), self.model) # get model name from the model class if hasattr(self.model, 'module'): self._model_name = self.model.module.__class__.__name__ else: self._model_name = self.model.__class__.__name__ ... ... Learn about how pytorch-quantization works by diving into its source code: Code about the quantization function respect to a specific Pytorch model as input: quant_utils.py: a utils file to provide helper functions to ptq.py interfaces.\nimport torch import re import yaml import json import os import collections from pathlib import Path from pytorch_quantization import quant_modules from pytorch_quantization.nn.modules import _utils as quant_nn_utils from pytorch_quantization import calib from pytorch_quantization import nn as quant_nn from pytorch_quantization.tensor_quant import QuantDescriptor from absl import logging as quant_logging def load_model(config, weight, device='cpu'): pass # intput QuantDescriptor: Max or Histogram # calib_method -\u003e [\"max\", \"histogram\"] def initialize(calib_method: str): quant_desc_input = QuantDescriptor(calib_method=calib_method) quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input) quant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input) quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input) quant_logging.set_verbosity(quant_logging.ERROR) def prepare_model(config, weight, device, calib_method): # quant_modules.initialize() \u003c- the method that automatically quantizes the model initialize(calib_method) model = load_model(config, weight, device) model.float() model.eval() return model def transfer_torch_to_quantization(nn_instance, quant_mudule): quant_instance = quant_mudule.__new__(quant_mudule) for k, val in vars(nn_instance).items(): setattr(quant_instance, k, val) def __init__(self): # Return two instances of QuantDescriptor; self.__class__ is the class of quant_instance, E.g.: QuantConv2d quant_desc_input, quant_desc_weight = quant_nn_utils.pop_quant_desc_in_kwargs(self.__class__) if isinstance(self, quant_nn_utils.QuantInputMixin): self.init_quantizer(quant_desc_input) if isinstance(self._input_quantizer._calibrator, calib.HistogramCalibrator): self._input_quantizer._calibrator._torch_hist = True else: self.init_quantizer(quant_desc_input, quant_desc_weight) if isinstance(self._input_quantizer._calibrator, calib.HistogramCalibrator): self._input_quantizer._calibrator._torch_hist = True self._weight_quantizer._calibrator._torch_hist = True __init__(quant_instance) return quant_instance def quantization_ignore_match(ignore_layer, path): if ignore_layer is None: return False if isinstance(ignore_layer, str) or isinstance(ignore_layer, list): if isinstance(ignore_layer, str): ignore_layer = [ignore_layer] if path in ignore_layer: return True for item in ignore_layer: if re.match(item, path): return True return False # iterative method def torch_module_find_quant_module(module, module_dict, ignore_layer): for name in module._modules: submodule = module._modules[name] path = name torch_module_find_quant_module(submodule, module_dict, ignore_layer) submodule_id = id(type(submodule)) if submodule_id in module_dict: ignored = quantization_ignore_match(ignore_layer, path) if ignored: print(f\"Quantization : {path} has ignored.\") continue # substitute the layer with quantized version module._modules[name] = transfer_torch_to_quantization(submodule, module_dict[submodule_id]) def replace_to_quantization_model(model, ignore_layer=None): module_dict = {} for entry in quant_modules._DEFAULT_QUANT_MAP: module = getattr(entry.orig_mod, entry.mod_name) module_dict[id(module)] = entry.replace_mod torch_module_find_quant_module(model, module_dict, ignore_layer) def create_dataloader(dir, batch_size, is_train): pass def prepare_val_dataset(dir, batch_size, is_train=False): dataloader = create_dataloader(dir, batch_size, is_train) return dataloader def prepare_train_dataset(dir, batch_size, is_train=True): dataloader = create_dataloader(dir, batch_size, is_train) return dataloader def test(save_dir, model, dataloader): pass def evaluate(model, loader, save_dir='.'): if save_dir and os.path.dirname(save_dir) != \"\": os.makedirs(os.path.dirname(save_dir), exist_ok=True) return test(save_dir, model, loader) def collect_stats(model, data_loader, device, num_batch=200): model.eval() # Enable calibrator for name, module in model.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if module._calibrator is not None: module.disable_quant() module.enable_calib() else: module.disable() # test with torch.no_grad(): for i, datas in enumerate(data_loader): data = datas[0].to(device).float() model(data) if i \u003e= num_batch: break # Disable calibrator for name, module in model.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if module._calibrator is not None: module.enable_quant() module.disable_calib() else: module.enable() def compute_amax(model, device, **kwargs): for name, module in model.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if module._calibrator is not None: if isinstance(module._calibrator, calib.MaxCalibrator): module.load_calib_amax() else: module.load_calib_amax(**kwargs) module._amax = module._amax.to(device) # method parameter designed for \"histogram\", method in ['entropy', 'mse', 'percentile'] def calibrate_model(model, dataloader, device, method): # Collect stats with data flowing collect_stats(model, dataloader, device) # Get dynamic range and compute amax (used in calibration) compute_amax(model, device, method='mse') def export_ptq(model, save_file, device, dynamic_batch=False): input_dummy = ...(device) quant_nn.TensorQuantizer.use_fb_fake_quant = True model.eval() with torch.no_grad(): torch.onnx.export(model, input_dummy, save_file, opset_version=13, input_names=['input'], output_names=['output'], dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}} if dynamic_batch else None, ) quant_nn.TensorQuantizer.use_fb_fake_quant = False # To test if there's any quantized layer def have_quantizer(layer): for name, module in layer.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): return True # Disable quantization (to a specific layer) class disable_quantization: # init def __init__(self, model): self.model = model # Disable quantization def apply(self, disabled=True): for name, module in self.model.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): module._disabled = disabled def __enter__(self): self.apply(disabled=True) def __exit__(self, *args, **kwargs): self.apply(disabled=False) # Enable quantization (to a specific layer) class enable_quantization: def __init__(self, model): self.model = model def apply(self, enabled=True): for name, module in self.model.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): module._disabled = not enabled def __enter__(self): self.apply(enabled=True) return self def __exit__(self, *args, **kwargs): self.apply(enabled=False) # Saving log class SummaryTool: def __init__(self, file): self.file = file self.data = [] def append(self, item): self.data.append(item) json.dump(self.data, open(self.file, \"w\"), indent=4) def sensitive_analysis(model, loader, summary_file): summary = SummaryTool(summary_file) # for loop to iterate every layer print(\"Sensitive analysis by each layer....\") for i in range(0, len(list(model.modules()))): layer = list(model.modules())[i] # tell if layer has quantized layer if have_quantizer(layer): # if so # disable the layer disable_quantization(layer).apply() # calculate map ap = evaluate(model, loader) # save ap in json summary.append([ap, f\"model.{i}\"]) # enable back the layer enable_quantization(layer).apply() print(f\"layer {i} ap: {ap}\") else: print(f\"ignore model.{i} because it is {type(layer)}\") # after the iteration, print the top 10 worst quantized layer and save to log summary = sorted(summary.data, key=lambda x: x[0], reverse=True) print(\"Sensitive Summary: \") for n, (ap, name) in enumerate(summary[:10]): print(f\"Top{n}: Using fp16 {name}, ap = {ap:.5f}\") summary.append([name, f\"Top{n}: Using fp16 {name}, ap = {ap:.5f}\"]) ptq.py: an interface that utilizes the helper/utils functions\nimport torch import quant_utils as quantize import argparse def run_SensitiveAnalysis(config, weight, dir, calib_method, hist_method, device): # prepare model print(\"Prepare Model ....\") model = quantize.prepare_model(config, weight, device, calib_method) quantize.replace_to_quantization_model(model) # prepare dataset print(\"Prepare Dataset ....\") train_dataloader = quantize.prepare_train_dataset(dir, args.batch_size, is_train=True) val_dataloader = quantize.prepare_val_dataset(dir, args.batch_size, is_train=False) # calibration model print(\"Begining Calibration ....\") quantize.calibrate_model(model, train_dataloader, device, hist_method) # sensitive analysis print(\"Begining Sensitive Analysis ....\") quantize.sensitive_analysis(model, val_dataloader, args.sensitive_summary) def run_PTQ(args, device): # prepare model print(\"Prepare Model ....\") model = quantize.prepare_model(args.config, args.weights, device, args.calib_method) quantize.replace_to_quantization_model(model, args.ignore_layers) # prepare dataset print(\"Prepare Dataset ....\") val_dataloader = quantize.prepare_val_dataset(args.cocodir, batch_size=args.batch_size, is_train=False) train_dataloader = quantize.prepare_train_dataset(args.cocodir, batch_size=args.batch_size, is_train=True) # calibrate model print(\"Beginning Calibration ....\") quantize.calibrate_model(model, train_dataloader, device, args.hist_method) summary = quantize.SummaryTool(args.ptq_summary) if args.eval_origin: print(\"Evaluate Origin...\") with quantize.disable_quantization(model): ap = quantize.evaluate(model, val_dataloader) summary.append([\"Origin\", ap]) if args.eval_ptq: print(\"Evaluate PTQ...\") ap = quantize.evaluate(model, val_dataloader) summary.append([\"PTQ\", ap]) if args.save_ptq: print(\"Export PTQ...\") quantize.export_ptq(model, args.ptq, device) if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument('--config', type=str, default='configs/...', help='model config file') parser.add_argument('--weights', type=str, default='', help='initial weights path') parser.add_argument('--dir', type=str, default=\"dataset/...\", help=\"dataset directory\") parser.add_argument('--batch_size', type=int, default=8, help=\"batch size for data loader\") parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu') parser.add_argument('--hist_method', type=str, default='mse', choices=['entropy', 'mse', 'percentile'], help='histogram calibration methods used in histogram, one of [\"entropy\", \"mse\", \"percentile\"]') parser.add_argument('--calib_method', type=str, default='histogram', choices=[\"max\", \"histogram\"], help='calibration methods used in histogram, one of [\"max\", \"histogram\"]') parser.add_argument('--sensitive', type=bool, default=True, help=\"use sensitive analysis or not before ptq\") parser.add_argument(\"--sensitive_summary\", type=str, default=\"sensitive-summary.json\", help=\"summary save file\") parser.add_argument(\"--ignore_layers\", type=str, default=\"model\\.105\\.m\\.(.*)\", help=\"regx\") parser.add_argument(\"--save_ptq\", type=bool, default=False, help=\"file\") parser.add_argument(\"--ptq\", type=str, default=\"ptq_centerpoint.onnx\", help=\"file\") parser.add_argument(\"--eval_origin\", action=\"store_true\", help=\"do eval for origin model\") parser.add_argument(\"--eval_ptq\", action=\"store_true\", help=\"do eval for ptq model\") parser.add_argument(\"--ptq_summary\", type=str, default=\"ptq_summary.json\", help=\"summary save file\") args = parser.parse_args() is_cuda = (args.device != 'cpu') and torch.cuda.is_available() device = torch.device(\"cuda:0\" if is_cuda else \"cpu\") # sensitive analysis if args.sensitive: print(\"Sensitive Analysis....\") run_SensitiveAnalysis(args.config, args.weights, args.dir, args.calib_method, args.hist_method, device) # PTQ # ignore_layers= [\"model\\.105\\.m\\.(.*)\", model\\.99\\.m\\.(.*)] # args.ignore_layer = ignore_layers print(\"Beginning PTQ.....\") run_PTQ(args, device) print(\"PTQ Quantization Has Finished....\") It can be noted that three functions in the quant_utils.py are not implemented:\ndef load_model(config, weight, device='cpu'): pass def create_dataloader(dir, batch_size, is_train): pass def test(save_dir, model, dataloader): pass This is because MMDetection3D is well-encapsulated:\ndef main(): ... ... if 'runner_type' not in cfg: # build the default runner runner = Runner.from_cfg(cfg) else: # build customized runner from the registry # if 'runner_type' is set in the cfg runner = RUNNERS.build(cfg) # start testing runner.test() And in the test function:\nclass Runner: ... ... def test(self) -\u003e dict: \"\"\"Launch test. Returns: dict: A dict of metrics on testing set. \"\"\" if self._test_loop is None: raise RuntimeError( '`self._test_loop` should not be None when calling test ' 'method. Please provide `test_dataloader`, `test_cfg` and ' '`test_evaluator` arguments when initializing runner.') self._test_loop = self.build_test_loop(self._test_loop) # type: ignore self.call_hook('before_run') # make sure checkpoint-related hooks are triggered after `before_run` self.load_or_resume() metrics = self.test_loop.run() # type: ignore self.call_hook('after_run') return metrics ... ... In self.build_test_loop(self._test_loop):\nclass Runner: ... ... def build_test_loop(self, loop: Union[BaseLoop, Dict]) -\u003e BaseLoop: \"\"\"Build test loop. Examples of ``loop``:: # `TestLoop` will be used loop = dict() # custom test loop loop = dict(type='CustomTestLoop') Args: loop (BaseLoop or dict): A test loop or a dict to build test loop. If ``loop`` is a test loop object, just returns itself. Returns: :obj:`BaseLoop`: Test loop object build from ``loop_cfg``. \"\"\" if isinstance(loop, BaseLoop): return loop elif not isinstance(loop, dict): raise TypeError( f'test_loop should be a Loop object or dict, but got {loop}') loop_cfg = copy.deepcopy(loop) # type: ignore if 'type' in loop_cfg: loop = LOOPS.build( loop_cfg, default_args=dict( runner=self, dataloader=self._test_dataloader, evaluator=self._test_evaluator)) else: loop = TestLoop( **loop_cfg, runner=self, dataloader=self._test_dataloader, evaluator=self._test_evaluator) # type: ignore return loop # type: ignore ... ... …… so since this week’s focus is on writing the PTQ API (above),\nBy next week: get model, dataloader definition (just need some more time). or switch to OpenPCDet for simplicity. ",
  "wordCount" : "1440",
  "inLanguage": "en",
  "datePublished": "2024-04-01T16:32:18-05:00",
  "dateModified": "2024-04-01T16:32:18-05:00",
  "author":{
    "@type": "Person",
    "name": "Banghao Chi"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://biboyqg.github.io/blog/blog/quant/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Banghao's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://biboyqg.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://biboyqg.github.io/blog/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://biboyqg.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/" title="Home Page">
                    <span>Home Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Quantization on CenterPoint
    </h1>
    <div class="post-meta"><span title='2024-04-01 16:32:18 -0500 -0500'>April 1, 2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><h3 id="take-mmdetection-as-an-example">Take <code>mmdetection</code> as an example<a hidden class="anchor" aria-hidden="true" href="#take-mmdetection-as-an-example">#</a></h3>
<ol>
<li>First find the <code>Runner</code> class:</li>
</ol>
<p><img loading="lazy" src="https://s2.loli.net/2024/04/03/VZ14NHWtB6afnFp.png" alt="image-20240403083721183"  />
</p>
<p>This is the place where the build of the model is completed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> __init__(<span style="color:#f92672">...</span>):
</span></span><span style="display:flex;"><span>      	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>      	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>build_model(model)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># wrap model</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>wrap_model(
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>cfg<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;model_wrapper_cfg&#39;</span>), self<span style="color:#f92672">.</span>model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># get model name from the model class</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> hasattr(self<span style="color:#f92672">.</span>model, <span style="color:#e6db74">&#39;module&#39;</span>):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>_model_name <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>module<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>_model_name <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span>
</span></span></code></pre></div><ol start="2">
<li>Learn about how <code>pytorch-quantization</code> works by diving into its source code:</li>
</ol>
<p><img loading="lazy" src="https://s2.loli.net/2024/04/04/R9mewBVDyoMdsWr.png" alt="image-20240403171605404"  />
</p>
<ol start="3">
<li>Code about the quantization function respect to a specific Pytorch model as input:</li>
</ol>
<p><code>quant_utils.py</code>: a utils file to provide helper functions to <code>ptq.py</code> interfaces.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> re
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> yaml
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> json
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> collections
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pathlib <span style="color:#f92672">import</span> Path
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> quant_modules
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization.nn.modules <span style="color:#f92672">import</span> _utils <span style="color:#66d9ef">as</span> quant_nn_utils
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> calib
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization <span style="color:#f92672">import</span> nn <span style="color:#66d9ef">as</span> quant_nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pytorch_quantization.tensor_quant <span style="color:#f92672">import</span> QuantDescriptor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> absl <span style="color:#f92672">import</span> logging <span style="color:#66d9ef">as</span> quant_logging
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(config, weight, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># intput QuantDescriptor: Max or Histogram</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># calib_method -&gt; [&#34;max&#34;, &#34;histogram&#34;]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize</span>(calib_method: str):
</span></span><span style="display:flex;"><span>    quant_desc_input <span style="color:#f92672">=</span> QuantDescriptor(calib_method<span style="color:#f92672">=</span>calib_method)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantConv2d<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantMaxPool2d<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>QuantLinear<span style="color:#f92672">.</span>set_default_quant_desc_input(quant_desc_input)
</span></span><span style="display:flex;"><span>    quant_logging<span style="color:#f92672">.</span>set_verbosity(quant_logging<span style="color:#f92672">.</span>ERROR)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_model</span>(config, weight, device, calib_method):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># quant_modules.initialize() &lt;- the method that automatically quantizes the model</span>
</span></span><span style="display:flex;"><span>    initialize(calib_method)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> load_model(config, weight, device)
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transfer_torch_to_quantization</span>(nn_instance, quant_mudule):
</span></span><span style="display:flex;"><span>    quant_instance <span style="color:#f92672">=</span> quant_mudule<span style="color:#f92672">.</span>__new__(quant_mudule)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, val <span style="color:#f92672">in</span> vars(nn_instance)<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        setattr(quant_instance, k, val)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Return two instances of QuantDescriptor; self.__class__ is the class of quant_instance, E.g.: QuantConv2d</span>
</span></span><span style="display:flex;"><span>        quant_desc_input, quant_desc_weight <span style="color:#f92672">=</span> quant_nn_utils<span style="color:#f92672">.</span>pop_quant_desc_in_kwargs(self<span style="color:#f92672">.</span>__class__)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(self, quant_nn_utils<span style="color:#f92672">.</span>QuantInputMixin):
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>init_quantizer(quant_desc_input)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>HistogramCalibrator):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>init_quantizer(quant_desc_input, quant_desc_weight)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>HistogramCalibrator):
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_input_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>_weight_quantizer<span style="color:#f92672">.</span>_calibrator<span style="color:#f92672">.</span>_torch_hist <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    __init__(quant_instance)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quant_instance
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">quantization_ignore_match</span>(ignore_layer, path):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> ignore_layer <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> isinstance(ignore_layer, str) <span style="color:#f92672">or</span> isinstance(ignore_layer, list):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(ignore_layer, str):
</span></span><span style="display:flex;"><span>            ignore_layer <span style="color:#f92672">=</span> [ignore_layer]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> path <span style="color:#f92672">in</span> ignore_layer:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> ignore_layer:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> re<span style="color:#f92672">.</span><span style="color:#66d9ef">match</span>(item, path):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># iterative method</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">torch_module_find_quant_module</span>(module, module_dict, ignore_layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> module<span style="color:#f92672">.</span>_modules:
</span></span><span style="display:flex;"><span>        submodule <span style="color:#f92672">=</span> module<span style="color:#f92672">.</span>_modules[name]
</span></span><span style="display:flex;"><span>        path <span style="color:#f92672">=</span> name
</span></span><span style="display:flex;"><span>        torch_module_find_quant_module(submodule, module_dict, ignore_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        submodule_id <span style="color:#f92672">=</span> id(type(submodule))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> submodule_id <span style="color:#f92672">in</span> module_dict:
</span></span><span style="display:flex;"><span>            ignored <span style="color:#f92672">=</span> quantization_ignore_match(ignore_layer, path)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> ignored:
</span></span><span style="display:flex;"><span>                print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Quantization : </span><span style="color:#e6db74">{</span>path<span style="color:#e6db74">}</span><span style="color:#e6db74"> has ignored.&#34;</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># substitute the layer with quantized version</span>
</span></span><span style="display:flex;"><span>            module<span style="color:#f92672">.</span>_modules[name] <span style="color:#f92672">=</span> transfer_torch_to_quantization(submodule, module_dict[submodule_id])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">replace_to_quantization_model</span>(model, ignore_layer<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    module_dict <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> entry <span style="color:#f92672">in</span> quant_modules<span style="color:#f92672">.</span>_DEFAULT_QUANT_MAP:
</span></span><span style="display:flex;"><span>        module <span style="color:#f92672">=</span> getattr(entry<span style="color:#f92672">.</span>orig_mod, entry<span style="color:#f92672">.</span>mod_name)
</span></span><span style="display:flex;"><span>        module_dict[id(module)] <span style="color:#f92672">=</span> entry<span style="color:#f92672">.</span>replace_mod
</span></span><span style="display:flex;"><span>    torch_module_find_quant_module(model, module_dict, ignore_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_dataloader</span>(dir, batch_size, is_train):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_val_dataset</span>(dir, batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    dataloader <span style="color:#f92672">=</span> create_dataloader(dir, batch_size, is_train)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dataloader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">prepare_train_dataset</span>(dir, batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>    dataloader <span style="color:#f92672">=</span> create_dataloader(dir, batch_size, is_train)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> dataloader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(save_dir, model, dataloader):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">evaluate</span>(model, loader, save_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> save_dir <span style="color:#f92672">and</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(save_dir) <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#34;&#34;</span>:
</span></span><span style="display:flex;"><span>        os<span style="color:#f92672">.</span>makedirs(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>dirname(save_dir), exist_ok<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> test(save_dir, model, loader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">collect_stats</span>(model, data_loader, device, num_batch<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Enable calibrator</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable_quant()
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable_calib()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># test</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i, datas <span style="color:#f92672">in</span> enumerate(data_loader):
</span></span><span style="display:flex;"><span>            data <span style="color:#f92672">=</span> datas[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>to(device)<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>            model(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;=</span> num_batch:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Disable calibrator</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable_quant()
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>disable_calib()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>enable()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_amax</span>(model, device, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> module<span style="color:#f92672">.</span>_calibrator <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> isinstance(module<span style="color:#f92672">.</span>_calibrator, calib<span style="color:#f92672">.</span>MaxCalibrator):
</span></span><span style="display:flex;"><span>                    module<span style="color:#f92672">.</span>load_calib_amax()
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                    module<span style="color:#f92672">.</span>load_calib_amax(<span style="color:#f92672">**</span>kwargs)
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_amax <span style="color:#f92672">=</span> module<span style="color:#f92672">.</span>_amax<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># method parameter designed for &#34;histogram&#34;, method in [&#39;entropy&#39;, &#39;mse&#39;, &#39;percentile&#39;]</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calibrate_model</span>(model, dataloader, device, method):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Collect stats with data flowing</span>
</span></span><span style="display:flex;"><span>    collect_stats(model, dataloader, device)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get dynamic range and compute amax (used in calibration)</span>
</span></span><span style="display:flex;"><span>    compute_amax(model, device, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">export_ptq</span>(model, save_file, device, dynamic_batch<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    input_dummy <span style="color:#f92672">=</span> <span style="color:#f92672">...</span>(device)
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>TensorQuantizer<span style="color:#f92672">.</span>use_fb_fake_quant <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(model, input_dummy, save_file, opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">13</span>,
</span></span><span style="display:flex;"><span>                          input_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input&#39;</span>], output_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;output&#39;</span>],
</span></span><span style="display:flex;"><span>                          dynamic_axes<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#39;input&#39;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#39;batch&#39;</span>}, <span style="color:#e6db74">&#39;output&#39;</span>: {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#39;batch&#39;</span>}} <span style="color:#66d9ef">if</span> dynamic_batch <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                          )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    quant_nn<span style="color:#f92672">.</span>TensorQuantizer<span style="color:#f92672">.</span>use_fb_fake_quant <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># To test if there&#39;s any quantized layer</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">have_quantizer</span>(layer):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> layer<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Disable quantization (to a specific layer)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">disable_quantization</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># init</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Disable quantization</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply</span>(self, disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_disabled <span style="color:#f92672">=</span> disabled
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __enter__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __exit__(self, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(disabled<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Enable quantization (to a specific layer)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">enable_quantization</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply</span>(self, enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(module, quant_nn<span style="color:#f92672">.</span>TensorQuantizer):
</span></span><span style="display:flex;"><span>                module<span style="color:#f92672">.</span>_disabled <span style="color:#f92672">=</span> <span style="color:#f92672">not</span> enabled
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __enter__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __exit__(self, <span style="color:#f92672">*</span>args, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>apply(enabled<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Saving log</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SummaryTool</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, file):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>file <span style="color:#f92672">=</span> file
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">append</span>(self, item):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>append(item)
</span></span><span style="display:flex;"><span>        json<span style="color:#f92672">.</span>dump(self<span style="color:#f92672">.</span>data, open(self<span style="color:#f92672">.</span>file, <span style="color:#e6db74">&#34;w&#34;</span>), indent<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sensitive_analysis</span>(model, loader, summary_file):
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> SummaryTool(summary_file)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># for loop to iterate every layer</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Sensitive analysis by each layer....&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, len(list(model<span style="color:#f92672">.</span>modules()))):
</span></span><span style="display:flex;"><span>        layer <span style="color:#f92672">=</span> list(model<span style="color:#f92672">.</span>modules())[i]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># tell if layer has quantized layer</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> have_quantizer(layer): <span style="color:#75715e"># if so</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># disable the layer</span>
</span></span><span style="display:flex;"><span>            disable_quantization(layer)<span style="color:#f92672">.</span>apply()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># calculate map</span>
</span></span><span style="display:flex;"><span>            ap <span style="color:#f92672">=</span> evaluate(model, loader)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># save ap in json</span>
</span></span><span style="display:flex;"><span>            summary<span style="color:#f92672">.</span>append([ap, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;model.</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>])
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># enable back the layer</span>
</span></span><span style="display:flex;"><span>            enable_quantization(layer)<span style="color:#f92672">.</span>apply()
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;layer </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> ap: </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;ignore model.</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74"> because it is </span><span style="color:#e6db74">{</span>type(layer)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># after the iteration, print the top 10 worst quantized layer and save to log</span>
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> sorted(summary<span style="color:#f92672">.</span>data, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">0</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Sensitive Summary: &#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> n, (ap, name) <span style="color:#f92672">in</span> enumerate(summary[:<span style="color:#ae81ff">10</span>]):
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">: Using fp16 </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">, ap = </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">:</span><span style="color:#e6db74">.5f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>        summary<span style="color:#f92672">.</span>append([name, <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Top</span><span style="color:#e6db74">{</span>n<span style="color:#e6db74">}</span><span style="color:#e6db74">: Using fp16 </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">, ap = </span><span style="color:#e6db74">{</span>ap<span style="color:#e6db74">:</span><span style="color:#e6db74">.5f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>])
</span></span></code></pre></div><p><code>ptq.py</code>: an interface that utilizes the helper/utils functions</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> quant_utils <span style="color:#66d9ef">as</span> quantize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> argparse
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_SensitiveAnalysis</span>(config, weight, dir, calib_method, hist_method, device):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Model ....&#34;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_model(config, weight, device, calib_method)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>replace_to_quantization_model(model)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare dataset</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Dataset ....&#34;</span>)
</span></span><span style="display:flex;"><span>    train_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_train_dataset(dir, args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    val_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_val_dataset(dir, args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calibration model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Begining Calibration ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>calibrate_model(model, train_dataloader, device, hist_method)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sensitive analysis</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Begining Sensitive Analysis ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>sensitive_analysis(model, val_dataloader, args<span style="color:#f92672">.</span>sensitive_summary)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_PTQ</span>(args, device):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Model ....&#34;</span>)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_model(args<span style="color:#f92672">.</span>config, args<span style="color:#f92672">.</span>weights, device, args<span style="color:#f92672">.</span>calib_method)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>replace_to_quantization_model(model, args<span style="color:#f92672">.</span>ignore_layers)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># prepare dataset</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Prepare Dataset ....&#34;</span>)
</span></span><span style="display:flex;"><span>    val_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_val_dataset(args<span style="color:#f92672">.</span>cocodir, batch_size<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    train_dataloader <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>prepare_train_dataset(args<span style="color:#f92672">.</span>cocodir, batch_size<span style="color:#f92672">=</span>args<span style="color:#f92672">.</span>batch_size, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># calibrate model</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Beginning Calibration ....&#34;</span>)
</span></span><span style="display:flex;"><span>    quantize<span style="color:#f92672">.</span>calibrate_model(model, train_dataloader, device, args<span style="color:#f92672">.</span>hist_method)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    summary <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>SummaryTool(args<span style="color:#f92672">.</span>ptq_summary)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>eval_origin:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Evaluate Origin...&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> quantize<span style="color:#f92672">.</span>disable_quantization(model):
</span></span><span style="display:flex;"><span>            ap <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>evaluate(model, val_dataloader)
</span></span><span style="display:flex;"><span>            summary<span style="color:#f92672">.</span>append([<span style="color:#e6db74">&#34;Origin&#34;</span>, ap])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>eval_ptq:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Evaluate PTQ...&#34;</span>)
</span></span><span style="display:flex;"><span>        ap <span style="color:#f92672">=</span> quantize<span style="color:#f92672">.</span>evaluate(model, val_dataloader)
</span></span><span style="display:flex;"><span>        summary<span style="color:#f92672">.</span>append([<span style="color:#e6db74">&#34;PTQ&#34;</span>, ap])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>save_ptq:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Export PTQ...&#34;</span>)
</span></span><span style="display:flex;"><span>        quantize<span style="color:#f92672">.</span>export_ptq(model, args<span style="color:#f92672">.</span>ptq, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    parser <span style="color:#f92672">=</span> argparse<span style="color:#f92672">.</span>ArgumentParser()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--config&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;configs/...&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;model config file&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--weights&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;initial weights path&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--dir&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dataset/...&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;dataset directory&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--batch_size&#39;</span>, type<span style="color:#f92672">=</span>int, default<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;batch size for data loader&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--device&#39;</span>, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;0&#39;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda device, i.e. 0 or 0,1,2,3 or cpu&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--hist_method&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>, choices<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;entropy&#39;</span>, <span style="color:#e6db74">&#39;mse&#39;</span>, <span style="color:#e6db74">&#39;percentile&#39;</span>], help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;histogram calibration methods used in histogram, one of [&#34;entropy&#34;, &#34;mse&#34;, &#34;percentile&#34;]&#39;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--calib_method&#39;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;histogram&#39;</span>, choices<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;max&#34;</span>, <span style="color:#e6db74">&#34;histogram&#34;</span>], help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;calibration methods used in histogram, one of [&#34;max&#34;, &#34;histogram&#34;]&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--sensitive&#39;</span>, type<span style="color:#f92672">=</span>bool, default<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;use sensitive analysis or not before ptq&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--sensitive_summary&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sensitive-summary.json&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;summary save file&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ignore_layers&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;model\.105\.m\.(.*)&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;regx&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--save_ptq&#34;</span>, type<span style="color:#f92672">=</span>bool, default<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;file&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ptq&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ptq_centerpoint.onnx&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;file&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--eval_origin&#34;</span>, action<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;store_true&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;do eval for origin model&#34;</span>)
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--eval_ptq&#34;</span>, action<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;store_true&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;do eval for ptq model&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;--ptq_summary&#34;</span>, type<span style="color:#f92672">=</span>str, default<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ptq_summary.json&#34;</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;summary save file&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    args <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse_args()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    is_cuda <span style="color:#f92672">=</span> (args<span style="color:#f92672">.</span>device <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;cpu&#39;</span>) <span style="color:#f92672">and</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available()
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda:0&#34;</span> <span style="color:#66d9ef">if</span> is_cuda <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sensitive analysis</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> args<span style="color:#f92672">.</span>sensitive:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;Sensitive Analysis....&#34;</span>)
</span></span><span style="display:flex;"><span>        run_SensitiveAnalysis(args<span style="color:#f92672">.</span>config, args<span style="color:#f92672">.</span>weights, args<span style="color:#f92672">.</span>dir, args<span style="color:#f92672">.</span>calib_method, args<span style="color:#f92672">.</span>hist_method, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># PTQ</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ignore_layers= [&#34;model\.105\.m\.(.*)&#34;, model\.99\.m\.(.*)]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># args.ignore_layer = ignore_layers</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Beginning PTQ.....&#34;</span>)
</span></span><span style="display:flex;"><span>    run_PTQ(args, device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;PTQ Quantization Has Finished....&#34;</span>)
</span></span></code></pre></div><p>It can be noted that three functions in the <code>quant_utils.py</code> are not implemented:</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_model</span>(config, weight, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_dataloader</span>(dir, batch_size, is_train):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(save_dir, model, dataloader):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">pass</span>
</span></span></code></pre></div></li>
</ul>
<p>This is because <code>MMDetection3D</code> is well-encapsulated:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">main</span>():
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;runner_type&#39;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> cfg:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># build the default runner</span>
</span></span><span style="display:flex;"><span>        runner <span style="color:#f92672">=</span> Runner<span style="color:#f92672">.</span>from_cfg(cfg)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># build customized runner from the registry</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if &#39;runner_type&#39; is set in the cfg</span>
</span></span><span style="display:flex;"><span>        runner <span style="color:#f92672">=</span> RUNNERS<span style="color:#f92672">.</span>build(cfg)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># start testing</span>
</span></span><span style="display:flex;"><span>    runner<span style="color:#f92672">.</span>test()
</span></span></code></pre></div><p>And in the <code>test</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test</span>(self) <span style="color:#f92672">-&gt;</span> dict:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Launch test.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            dict: A dict of metrics on testing set.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>_test_loop <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">RuntimeError</span>(
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;`self._test_loop` should not be None when calling test &#39;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;method. Please provide `test_dataloader`, `test_cfg` and &#39;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">&#39;`test_evaluator` arguments when initializing runner.&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_test_loop <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>build_test_loop(self<span style="color:#f92672">.</span>_test_loop)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>call_hook(<span style="color:#e6db74">&#39;before_run&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># make sure checkpoint-related hooks are triggered after `before_run`</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>load_or_resume()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        metrics <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>test_loop<span style="color:#f92672">.</span>run()  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>call_hook(<span style="color:#e6db74">&#39;after_run&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> metrics
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>In <code>self.build_test_loop(self._test_loop)</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Runner</span>:
</span></span><span style="display:flex;"><span>  	<span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_test_loop</span>(self, loop: Union[BaseLoop, Dict]) <span style="color:#f92672">-&gt;</span> BaseLoop:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Build test loop.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Examples of ``loop``::
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            # `TestLoop` will be used
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop = dict()
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            # custom test loop
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop = dict(type=&#39;CustomTestLoop&#39;)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            loop (BaseLoop or dict): A test loop or a dict to build test loop.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                If ``loop`` is a test loop object, just returns itself.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            :obj:`BaseLoop`: Test loop object build from ``loop_cfg``.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(loop, BaseLoop):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> loop
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> <span style="color:#f92672">not</span> isinstance(loop, dict):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">TypeError</span>(
</span></span><span style="display:flex;"><span>                <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;test_loop should be a Loop object or dict, but got </span><span style="color:#e6db74">{</span>loop<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loop_cfg <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy(loop)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;type&#39;</span> <span style="color:#f92672">in</span> loop_cfg:
</span></span><span style="display:flex;"><span>            loop <span style="color:#f92672">=</span> LOOPS<span style="color:#f92672">.</span>build(
</span></span><span style="display:flex;"><span>                loop_cfg,
</span></span><span style="display:flex;"><span>                default_args<span style="color:#f92672">=</span>dict(
</span></span><span style="display:flex;"><span>                    runner<span style="color:#f92672">=</span>self,
</span></span><span style="display:flex;"><span>                    dataloader<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_dataloader,
</span></span><span style="display:flex;"><span>                    evaluator<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_evaluator))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            loop <span style="color:#f92672">=</span> TestLoop(
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">**</span>loop_cfg,
</span></span><span style="display:flex;"><span>                runner<span style="color:#f92672">=</span>self,
</span></span><span style="display:flex;"><span>                dataloader<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_dataloader,
</span></span><span style="display:flex;"><span>                evaluator<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>_test_evaluator)  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loop  <span style="color:#75715e"># type: ignore</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">...</span>
</span></span></code></pre></div><p>&hellip;&hellip; so since this week&rsquo;s focus is on writing the <code>PTQ</code> API (above),</p>
<h3 id="by-next-week">By next week:<a hidden class="anchor" aria-hidden="true" href="#by-next-week">#</a></h3>
<ul>
<li>get model, dataloader definition (just need some more time).</li>
<li>or switch to <code>OpenPCDet</code> for simplicity.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://biboyqg.github.io/blog/tags/quantization/">Quantization</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/3d-object-detection/">3D Object Detection</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/centerpoint/">CenterPoint</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://biboyqg.github.io/blog/blog/camera/">
    <span class="title">« Prev</span>
    <br>
    <span>IoT-Enabled Home Security Camera</span>
  </a>
  <a class="next" href="https://biboyqg.github.io/blog/blog/log/">
    <span class="title">Next »</span>
    <br>
    <span>Daily Log</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    <footer class="footer">
  <span
    >&copy; 2025
    <a href="https://biboyqg.github.io/blog/">Banghao&#39;s Blog</a></span
  >
</footer>
</body>

</html>
