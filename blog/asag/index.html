<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
<script
  src="/blog/js/theme-toggle.min.4c7848f7880c1b4e8e1195754fdbd0ca3d5ae9aac7d5282162029954a67c277c.js"
  integrity="sha256-THhI94gMG06OEZV1T9vQyj1a6arH1SghYgKZVKZ8J3w="
></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://biboyqg.github.io/blog/" accesskey="h" title="Banghao&#39;s Blog (Alt + H)">Banghao&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://biboyqg.github.io/blog/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/blog/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://biboyqg.github.io/" title="Home Page">
                    <span>Home Page</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LLMarking
    </h1>
    <div class="post-meta"><span title='2024-06-12 19:24:12 +0800 +0800'>June 12, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Banghao Chi

</div>
  </header> 
  <div class="post-content"><p><a href="https://github.com/BiboyQG/LLMarking">This</a> is the official repo for <strong>Automatic Short Answer Grading (ASAG)</strong> project, <strong>named LLMarking</strong>, from <strong>Xi&rsquo;an Jiaotong Liverpool University (XJTLU)</strong>.</p>
<p>Using <a href="https://github.com/vllm-project/vllm">vLLM</a> as the Large Language Model (LLM) inference framework and <a href="https://github.com/tiangolo/fastapi">FastAPI</a> as the HTTP service framework, this project can achieve high throughput of both LLM tokens delivered and request handling.</p>
<h2 id="feature">Feature<a hidden class="anchor" aria-hidden="true" href="#feature">#</a></h2>
<p>This project aims to achieve high concurrency automatic short answer grading (ASAG) system and implement the construction of service.</p>
<ul>
<li>Prompt-learning enables LLMs to handle downstream tasks even without fintuning. We implemented zero-shot, one-shot, and few-shot to test the performances of different LLMs.</li>
<li>LoRA/QLoRA enables us to finetune to model with less GPU resources such as memory and computation capacity. This can be happening when continuous poor performances was witnessed even after trials of various prompt-learning.</li>
<li>vLLM supports Continuous batching of incoming requests, using an extra thread for inferencing.</li>
<li>vLLM provides abstracts of asyncio, using asyncio http framework after abstracts of uvicorn+FastAPI to achieve http api privision.</li>
</ul>
<h2 id="supported-models">Supported models<a hidden class="anchor" aria-hidden="true" href="#supported-models">#</a></h2>
<ul>
<li><code>Qwen/Qwen1.5-14B-Chat-GPTQ-Int4</code></li>
<li><code>Qwen/Qwen1.5-32B-Chat-AWQ</code></li>
<li><code>internlm/internlm2-chat-7b</code></li>
<li><code>01-ai/Yi-1.5-9B-Chat</code></li>
<li><code>modelscope/Yi-1.5-34B-Chat-AWQ</code></li>
<li><code>CohereForAI/aya-23-8B</code></li>
<li><code>meta-llama/Meta-Llama-3-8B-Instruct</code></li>
<li><code>THUDM/glm-4-9b-chat</code></li>
<li><code>Qwen/Qwen2-7B-Instruct</code></li>
<li><code>google/gemma-1.1-7b-it</code></li>
<li><code>mistralai/Mistral-7B-Instruct-v0.3</code></li>
<li><code>microsoft/Phi-3-small-8k-instruct</code></li>
<li><code>openbmb/MiniCPM-2B-dpo-bf16</code></li>
</ul>
<h2 id="getting-started">Getting Started<a hidden class="anchor" aria-hidden="true" href="#getting-started">#</a></h2>
<h3 id="requirements">Requirements<a hidden class="anchor" aria-hidden="true" href="#requirements">#</a></h3>
<blockquote>
<p><strong>IMPORTANT</strong>:</p>
<p>The requirement below is mandatory. And we&rsquo;ve only tested our project on the following platform.</p>
</blockquote>
<table>
<thead>
<tr>
<th>Mandatory</th>
<th>Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python</td>
<td>3.8</td>
</tr>
<tr>
<td>CUDA</td>
<td>12.1</td>
</tr>
<tr>
<td>torch</td>
<td>2.1</td>
</tr>
<tr>
<td>einops</td>
<td>0.8.0</td>
</tr>
<tr>
<td>transformers</td>
<td>4.41.0</td>
</tr>
<tr>
<td>accelerate</td>
<td>0.30.1</td>
</tr>
<tr>
<td>vLLM</td>
<td>0.4.3</td>
</tr>
<tr>
<td>tiktoken</td>
<td>0.6.0</td>
</tr>
<tr>
<td>sentencepiece</td>
<td>0.2.0</td>
</tr>
<tr>
<td>scipy</td>
<td>1.13.0</td>
</tr>
<tr>
<td>FastAPI</td>
<td>0.111.0</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>TIP</strong>:</p>
<p>Use <code>pip install -r requirement.txt</code> to install all the requirement if you want to create a new environment on your own or stick with existing environment.</p>
</blockquote>
<h3 id="quickstart">Quickstart<a hidden class="anchor" aria-hidden="true" href="#quickstart">#</a></h3>
<h4 id="repo-download">Repo Download<a hidden class="anchor" aria-hidden="true" href="#repo-download">#</a></h4>
<p>We first clone the whole project by git clone this repo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone git@github.com:BiboyQG/ASAG.git <span style="color:#f92672">&amp;&amp;</span> cd ASAG
</span></span></code></pre></div><h4 id="environment-setup">Environment Setup<a hidden class="anchor" aria-hidden="true" href="#environment-setup">#</a></h4>
<p>Then, it is necessary for us to setup a virtual environmrnt in order to run the project.</p>
<p>Currently, we don&rsquo;t provide docker image or dockerfile, but instead we offer conda(Anaconda/Miniconda) environment config file inside <code>env</code> folder.</p>
<p>Therefore, you can simply copy and run the following in your terminal to quickly setup the environment:</p>
<blockquote>
<p><strong>NOTE</strong>:</p>
<p>You can rename <code>my_new_env</code> to any name you want.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>conda env create -n my_new_env -f env/environment.yml <span style="color:#f92672">&amp;&amp;</span> conda activate my_new_env
</span></span></code></pre></div><h4 id="launch-server">Launch Server<a hidden class="anchor" aria-hidden="true" href="#launch-server">#</a></h4>
<p>Then we need to setup server-side to provide the service to the clients. To launch our HTTP server, simply:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>python vllm_server.py -m [index of the model in the above list]
</span></span></code></pre></div><p>If you launch the server with the specific model you specify for the first time, the server would automatically download the model and save the files to <code>.cache/huggingface/hub</code>.</p>
<blockquote>
<p><strong>NOTE</strong>:</p>
<p>Some users may find it difficult to download model files from Huggingface due to internet issues. Hence, we provide the following solution.</p>
</blockquote>
<p>For users that don&rsquo;t have access to Huggingface, you need to do the following things:</p>
<ul>
<li>Import <code>snapshot_download</code> from <code>modelscope</code> instead of from <code>huggingface_hub</code>:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> modelscope <span style="color:#f92672">import</span> snapshot_download
</span></span></code></pre></div><ul>
<li>Enable the use of <code>modelscope</code> by uncommenting this line of code within <code>vllm_server.py</code>:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;VLLM_USE_MODELSCOPE&#39;</span>]<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;True&#39;</span>
</span></span></code></pre></div><h4 id="request-and-response">Request and Response<a hidden class="anchor" aria-hidden="true" href="#request-and-response">#</a></h4>
<p>After that, we can either start the student entry or client side to pass our inputs to the server:</p>
<ul>
<li><strong>For student entry:</strong></li>
</ul>
<blockquote>
<p><strong>NOTE</strong>:</p>
<p><code>0</code> stands for using zero-shot prompt, while <code>1</code> for one-shot, and <code>2</code> for few-shot.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python student_entry.py -n <span style="color:#f92672">[</span>0, 1, 2<span style="color:#f92672">]</span>
</span></span></code></pre></div><ul>
<li><strong>For casual client:</strong></li>
</ul>
<blockquote>
<p><strong>NOTE</strong>:</p>
<p><code>-s</code> stands for get response in a streaming way, which is optional.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python student_entry.py <span style="color:#f92672">[</span>-s<span style="color:#f92672">]</span>
</span></span></code></pre></div><h4 id="webui">WebUI<a hidden class="anchor" aria-hidden="true" href="#webui">#</a></h4>
<p>After launching vllm_server, you can also run gradio_webui.py which is a webui based on gradio. This can achieve a chat-liked format like ChatGPT, which is more user-friendly.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python gradio_webui.py
</span></span></code></pre></div><p><img loading="lazy" src="https://s2.loli.net/2024/06/11/duwy9Q4j7JM1PVp.png" alt="webui"  />
</p>
<h2 id="data-and-results">Data and Results<a hidden class="anchor" aria-hidden="true" href="#data-and-results">#</a></h2>
<ul>
<li>
<p>Example data: <a href="https://github.com/BiboyQG/ASAG/blob/master/data/example.json">example.json</a></p>
</li>
<li>
<p>Zero-shot pormpt template:</p>
</li>
</ul>
<p><img loading="lazy" src="https://s2.loli.net/2024/06/11/8UOoJBshVgtKS1l.png" alt="image-20240611154055447"  />
</p>
<ul>
<li>Test on different LLMs with prompt template and example data: <a href="https://github.com/BiboyQG/ASAG/tree/master/results">results</a>.</li>
</ul>
<h2 id="acknowledgement">Acknowledgement<a hidden class="anchor" aria-hidden="true" href="#acknowledgement">#</a></h2>
<ul>
<li><a href="https://github.com/BiboyQG/ASAG/blob/master/vllm_server.py">vllm_server.py</a> referenced from <a href="https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/api_server.py">vLLM official implementation - server</a>.</li>
<li><a href="https://github.com/BiboyQG/ASAG/blob/master/vllm_client.py">vllm_client.py</a> referenced from <a href="https://github.com/vllm-project/vllm/blob/main/examples/api_client.py">vLLM official implementation - client</a>.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://biboyqg.github.io/blog/tags/llm/">LLM</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/nlp/">NLP</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/research/">Research</a></li>
      <li><a href="https://biboyqg.github.io/blog/tags/project/">Project</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://biboyqg.github.io/blog/blog/token/">
    <span class="title">Next »</span>
    <br>
    <span>Let&#39;s build GPT from scratch with BPE!</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    <footer class="footer">
  <span
    >&copy; 2025
    <a href="https://biboyqg.github.io/blog/">Banghao&#39;s Blog</a></span
  >
</footer>
</body>

</html>
